{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniGrid - Navegaci√≥n con Observaci√≥n Parcial\n",
    "\n",
    "**Proyecto de Nivel 4 - Reinforcement Learning**\n",
    "\n",
    "Este notebook explora MiniGrid, un entorno donde el agente tiene **observaci√≥n parcial** - solo ve una porci√≥n del mundo. Esto introduce desaf√≠os √∫nicos que no existen en entornos con observaci√≥n completa.\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "\n",
    "- Entender la diferencia entre observaci√≥n completa y parcial\n",
    "- Analizar la arquitectura CNN personalizada para grids\n",
    "- Explorar el rol del entropy coefficient en exploraci√≥n\n",
    "- Implementar curriculum learning (f√°cil ‚Üí dif√≠cil)\n",
    "- Comparar MLP vs CNN para observaciones estructuradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup y Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n (descomentar si es necesario)\n",
    "# !pip install minigrid stable-baselines3 gymnasium matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gymnasium as gym\n",
    "import minigrid\n",
    "from minigrid.wrappers import ImgObsWrapper, RGBImgObsWrapper, FullyObsWrapper\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "print(f\"MiniGrid: {minigrid.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. An√°lisis de la Arquitectura\n",
    "\n",
    "### 2.1 ¬øQu√© es la Observaci√≥n Parcial?\n",
    "\n",
    "En MiniGrid, el agente **NO ve todo el entorno**. Solo tiene una vista de 7√ó7 celdas en la direcci√≥n que est√° mirando.\n",
    "\n",
    "```\n",
    "Observaci√≥n Completa (FullyObs)     Observaci√≥n Parcial (Por defecto)\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚îÇ               ‚îÇ       ?????       ‚îÇ\n",
    "‚îÇ ‚ñ° . . . . . . . ‚ñ° ‚îÇ               ‚îÇ       ?????       ‚îÇ\n",
    "‚îÇ ‚ñ° . . . . . . . ‚ñ° ‚îÇ               ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ ‚ñ° . . . . . . . ‚ñ° ‚îÇ               ‚îÇ   ‚îÇ . . . . ‚îÇ     ‚îÇ\n",
    "‚îÇ ‚ñ° . . A‚Üí. . . . ‚ñ° ‚îÇ               ‚îÇ   ‚îÇ . . . . ‚îÇ     ‚îÇ\n",
    "‚îÇ ‚ñ° . . . . . . . ‚ñ° ‚îÇ               ‚îÇ   ‚îÇ . A‚Üí. . ‚îÇ     ‚îÇ\n",
    "‚îÇ ‚ñ° . . . . . . . ‚ñ° ‚îÇ               ‚îÇ   ‚îÇ . . . . ‚îÇ     ‚îÇ\n",
    "‚îÇ ‚ñ° . . . . . . G ‚ñ° ‚îÇ               ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚îÇ               ‚îÇ    7√ó7 visible    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "   Ve TODO                              Ve SOLO al frente\n",
    "```\n",
    "\n",
    "Esto crea el problema de **aliasing perceptual**: estados diferentes pueden verse id√©nticos desde la perspectiva del agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostrar la diferencia entre observaci√≥n parcial y completa\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARACI√ìN DE OBSERVACIONES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear entorno con observaci√≥n parcial (default)\n",
    "env_partial = gym.make(\"MiniGrid-Empty-8x8-v0\")\n",
    "obs_partial, _ = env_partial.reset()\n",
    "\n",
    "# Crear entorno con observaci√≥n completa\n",
    "env_full = gym.make(\"MiniGrid-Empty-8x8-v0\")\n",
    "env_full = FullyObsWrapper(env_full)\n",
    "obs_full, _ = env_full.reset()\n",
    "\n",
    "print(f\"\\nObservaci√≥n PARCIAL:\")\n",
    "print(f\"  Tipo: {type(obs_partial)}\")\n",
    "print(f\"  Imagen shape: {obs_partial['image'].shape}\")\n",
    "print(f\"  Descripci√≥n: Vista 7√ó7√ó3 (solo ve al frente)\")\n",
    "\n",
    "print(f\"\\nObservaci√≥n COMPLETA:\")\n",
    "print(f\"  Tipo: {type(obs_full)}\")\n",
    "print(f\"  Imagen shape: {obs_full['image'].shape}\")\n",
    "print(f\"  Descripci√≥n: Ve todo el grid\")\n",
    "\n",
    "env_partial.close()\n",
    "env_full.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Estructura de la Observaci√≥n (7√ó7√ó3)\n",
    "\n",
    "Cada celda tiene 3 canales de informaci√≥n:\n",
    "\n",
    "| Canal | Significado | Valores |\n",
    "|-------|------------|----------|\n",
    "| 0 | Tipo de objeto | 0=vac√≠o, 1=muro, 2=puerta, 4=llave, 5=pelota, 8=meta |\n",
    "| 1 | Color | 0-5 (rojo, verde, azul, p√∫rpura, amarillo, gris) |\n",
    "| 2 | Estado | 0=abierto, 1=cerrado, 2=bloqueado (para puertas) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar los canales de la observaci√≥n\n",
    "env = gym.make(\"MiniGrid-DoorKey-5x5-v0\")\n",
    "obs, _ = env.reset(seed=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "\n",
    "# Canal 0: Tipo de objeto\n",
    "im0 = axes[0].imshow(obs['image'][:, :, 0], cmap='viridis')\n",
    "axes[0].set_title('Canal 0: Tipo de Objeto')\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "# Canal 1: Color\n",
    "im1 = axes[1].imshow(obs['image'][:, :, 1], cmap='tab10')\n",
    "axes[1].set_title('Canal 1: Color')\n",
    "plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "# Canal 2: Estado\n",
    "im2 = axes[2].imshow(obs['image'][:, :, 2], cmap='coolwarm')\n",
    "axes[2].set_title('Canal 2: Estado')\n",
    "plt.colorbar(im2, ax=axes[2])\n",
    "\n",
    "# Combinado como RGB\n",
    "axes[3].imshow(obs['image'].astype(np.uint8) * 30)  # Escalar para visualizar\n",
    "axes[3].set_title('Combinado (escalado)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks(range(7))\n",
    "    ax.set_yticks(range(7))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Espacio de Acciones\n",
    "\n",
    "MiniGrid tiene 7 acciones discretas:\n",
    "\n",
    "| Acci√≥n | ID | Descripci√≥n |\n",
    "|--------|----|--------------|\n",
    "| Girar izquierda | 0 | Rotar 90¬∞ a la izquierda |\n",
    "| Girar derecha | 1 | Rotar 90¬∞ a la derecha |\n",
    "| Avanzar | 2 | Moverse una celda adelante |\n",
    "| Recoger | 3 | Tomar objeto de la celda frontal |\n",
    "| Soltar | 4 | Dejar objeto en celda frontal |\n",
    "| Toggle | 5 | Interactuar (abrir/cerrar puerta) |\n",
    "| Done | 6 | Terminar episodio (raramente usado) |\n",
    "\n",
    "**Nota**: La mayor√≠a de entornos solo requieren acciones 0-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostrar las acciones\n",
    "from minigrid.core.constants import OBJECT_TO_IDX, COLOR_TO_IDX, STATE_TO_IDX\n",
    "\n",
    "print(\"ACCIONES DISPONIBLES\")\n",
    "print(\"=\"*40)\n",
    "acciones = [\n",
    "    (0, \"Girar izquierda\", \"Rota 90¬∞ a la izquierda\"),\n",
    "    (1, \"Girar derecha\", \"Rota 90¬∞ a la derecha\"),\n",
    "    (2, \"Avanzar\", \"Mueve una celda adelante\"),\n",
    "    (3, \"Recoger\", \"Toma objeto delante\"),\n",
    "    (4, \"Soltar\", \"Deja objeto delante\"),\n",
    "    (5, \"Toggle\", \"Abre/cierra puerta\"),\n",
    "    (6, \"Done\", \"Termina episodio\"),\n",
    "]\n",
    "for id, nombre, desc in acciones:\n",
    "    print(f\"  {id}: {nombre:18} - {desc}\")\n",
    "\n",
    "print(\"\\nOBJETOS EN EL GRID\")\n",
    "print(\"=\"*40)\n",
    "for obj, idx in OBJECT_TO_IDX.items():\n",
    "    print(f\"  {idx}: {obj}\")\n",
    "\n",
    "print(\"\\nCOLORES\")\n",
    "print(\"=\"*40)\n",
    "for color, idx in COLOR_TO_IDX.items():\n",
    "    print(f\"  {idx}: {color}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Funci√≥n de Recompensa\n",
    "\n",
    "MiniGrid usa una recompensa **sparse** (escasa):\n",
    "\n",
    "```python\n",
    "# Recompensa en MiniGrid\n",
    "if reached_goal:\n",
    "    reward = 1 - 0.9 * (step_count / max_steps)\n",
    "else:\n",
    "    reward = 0\n",
    "```\n",
    "\n",
    "- **Solo recibe recompensa al llegar a la meta**\n",
    "- Recompensa m√°xima: ~1.0 (si llega muy r√°pido)\n",
    "- Recompensa m√≠nima: ~0.1 (si llega al l√≠mite de pasos)\n",
    "- 0 si no llega\n",
    "\n",
    "Esto hace el problema **dif√≠cil** - el agente debe explorar sin feedback hasta encontrar la meta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Arquitectura CNN Personalizada\n",
    "\n",
    "El c√≥digo usa una CNN espec√≠fica para procesar observaciones de MiniGrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la arquitectura MinigridCNN del c√≥digo fuente\n",
    "class MinigridCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    CNN para procesar las observaciones de MiniGrid.\n",
    "    \n",
    "    Arquitectura:\n",
    "        Input: (7, 7, 3) - Vista parcial del agente\n",
    "        Conv2d(3 ‚Üí 16, kernel=2) ‚Üí ReLU ‚Üí (6, 6, 16)\n",
    "        Conv2d(16 ‚Üí 32, kernel=2) ‚Üí ReLU ‚Üí (5, 5, 32)\n",
    "        Conv2d(32 ‚Üí 64, kernel=2) ‚Üí ReLU ‚Üí (4, 4, 64)\n",
    "        Flatten ‚Üí 1024\n",
    "        Linear(1024 ‚Üí 64) ‚Üí ReLU\n",
    "        Output: 64 features\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space, features_dim=64):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        n_input_channels = observation_space.shape[2]  # 3 canales\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 16, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        # Calcular tama√±o de salida\n",
    "        with torch.no_grad():\n",
    "            sample = torch.zeros(1, n_input_channels, *observation_space.shape[:2])\n",
    "            n_flatten = self.cnn(sample).shape[1]\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        # Cambiar de (B, H, W, C) a (B, C, H, W)\n",
    "        x = observations.permute(0, 3, 1, 2)\n",
    "        return self.linear(self.cnn(x))\n",
    "\n",
    "# Contar par√°metros\n",
    "dummy_space = gym.spaces.Box(low=0, high=1, shape=(7, 7, 3), dtype=np.float32)\n",
    "cnn = MinigridCNN(dummy_space, features_dim=64)\n",
    "total_params = sum(p.numel() for p in cnn.parameters())\n",
    "\n",
    "print(\"ARQUITECTURA MinigridCNN\")\n",
    "print(\"=\"*50)\n",
    "print(cnn)\n",
    "print(f\"\\nTotal par√°metros: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 ¬øPor qu√© CNN y no MLP?\n",
    "\n",
    "| Enfoque | Entrada | Ventajas | Desventajas |\n",
    "|---------|---------|----------|-------------|\n",
    "| **MLP** | Vector aplanado (147D) | M√°s r√°pido, simple | Pierde estructura espacial |\n",
    "| **CNN** | Imagen (7√ó7√ó3) | Preserva relaciones espaciales | M√°s par√°metros |\n",
    "\n",
    "La CNN es preferible porque:\n",
    "1. Detecta patrones locales (muro adelante, puerta a la derecha)\n",
    "2. Es invariante a traslaciones parciales\n",
    "3. Comparte pesos entre posiciones del campo visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entornos Disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar entornos disponibles organizados por dificultad\n",
    "ENTORNOS = {\n",
    "    # Nivel 1: B√°sicos\n",
    "    \"Empty\": (\"MiniGrid-Empty-5x5-v0\", \"Vac√≠o 5√ó5, ir a meta\", \"‚≠ê\"),\n",
    "    \"Empty8\": (\"MiniGrid-Empty-8x8-v0\", \"Vac√≠o 8√ó8, m√°s exploraci√≥n\", \"‚≠ê\"),\n",
    "    \n",
    "    # Nivel 2: Con obst√°culos\n",
    "    \"FourRooms\": (\"MiniGrid-FourRooms-v0\", \"4 habitaciones, pasar entre ellas\", \"‚≠ê‚≠ê\"),\n",
    "    \"SimpleCrossing\": (\"MiniGrid-SimpleCrossingS9N1-v0\", \"Cruzar obst√°culos\", \"‚≠ê‚≠ê\"),\n",
    "    \n",
    "    # Nivel 3: Con objetos\n",
    "    \"DoorKey\": (\"MiniGrid-DoorKey-5x5-v0\", \"Encontrar llave, abrir puerta\", \"‚≠ê‚≠ê‚≠ê\"),\n",
    "    \"DoorKey8\": (\"MiniGrid-DoorKey-8x8-v0\", \"DoorKey en grid m√°s grande\", \"‚≠ê‚≠ê‚≠ê\"),\n",
    "    \"Unlock\": (\"MiniGrid-Unlock-v0\", \"Solo encontrar y usar llave\", \"‚≠ê‚≠ê‚≠ê\"),\n",
    "    \n",
    "    # Nivel 4: Avanzados\n",
    "    \"LavaCrossing\": (\"MiniGrid-LavaCrossingS9N1-v0\", \"Evitar lava mortal\", \"‚≠ê‚≠ê‚≠ê‚≠ê\"),\n",
    "    \"DistShift\": (\"MiniGrid-DistShift1-v0\", \"Distribuci√≥n cambiante\", \"‚≠ê‚≠ê‚≠ê‚≠ê\"),\n",
    "}\n",
    "\n",
    "print(\"ENTORNOS DE MINIGRID\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Nombre':<15} {'Dificultad':<10} {'Descripci√≥n'}\")\n",
    "print(\"-\"*70)\n",
    "for name, (env_id, desc, diff) in ENTORNOS.items():\n",
    "    print(f\"{name:<15} {diff:<10} {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar algunos entornos\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "entornos_viz = [\"Empty\", \"FourRooms\", \"DoorKey\", \"SimpleCrossing\", \"LavaCrossing\", \"DoorKey8\"]\n",
    "\n",
    "for idx, nombre in enumerate(entornos_viz):\n",
    "    if nombre in ENTORNOS:\n",
    "        env_id, desc, diff = ENTORNOS[nombre]\n",
    "        try:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env.reset(seed=42)\n",
    "            img = env.render()\n",
    "            axes[idx].imshow(img)\n",
    "            axes[idx].set_title(f\"{nombre} {diff}\")\n",
    "            axes[idx].axis('off')\n",
    "            env.close()\n",
    "        except Exception as e:\n",
    "            axes[idx].text(0.5, 0.5, f\"Error: {str(e)[:30]}\", ha='center', va='center')\n",
    "            axes[idx].set_title(nombre)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. C√≥digo Base para Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrappers del c√≥digo fuente\n",
    "class FlatObsWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"Aplana la observaci√≥n para usar con MLP.\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = env.observation_space['image'].shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255,\n",
    "            shape=(np.prod(obs_shape),),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        return obs['image'].flatten().astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "class SimpleObsWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"Normaliza la observaci√≥n de imagen para CNN.\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=env.observation_space['image'].shape,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        return obs['image'].astype(np.float32) / 10.0\n",
    "\n",
    "\n",
    "class MinigridCallback(BaseCallback):\n",
    "    \"\"\"Callback para registrar m√©tricas.\"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.successes = []\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        for info in self.locals.get('infos', []):\n",
    "            if 'episode' in info:\n",
    "                self.episode_rewards.append(info['episode']['r'])\n",
    "                self.episode_lengths.append(info['episode']['l'])\n",
    "                self.successes.append(1 if info['episode']['r'] > 0 else 0)\n",
    "        return True\n",
    "\n",
    "\n",
    "def crear_entorno(nombre=\"Empty\", wrapper=\"cnn\", render=False):\n",
    "    \"\"\"Crea entorno con el wrapper apropiado.\"\"\"\n",
    "    env_id = ENTORNOS.get(nombre, (nombre, \"\", \"\"))[0] if nombre in ENTORNOS else nombre\n",
    "    render_mode = \"human\" if render else None\n",
    "    \n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    \n",
    "    if wrapper == \"flat\":\n",
    "        env = FlatObsWrapper(env)\n",
    "    else:\n",
    "        env = SimpleObsWrapper(env)\n",
    "    \n",
    "    return env\n",
    "\n",
    "\n",
    "def plot_training(callbacks, labels, title=\"Entrenamiento\"):\n",
    "    \"\"\"Grafica m√∫ltiples entrenamientos.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(callbacks)))\n",
    "    \n",
    "    for callback, label, color in zip(callbacks, labels, colors):\n",
    "        if not callback.episode_rewards:\n",
    "            continue\n",
    "        \n",
    "        rewards = callback.episode_rewards\n",
    "        lengths = callback.episode_lengths\n",
    "        successes = callback.successes\n",
    "        window = min(50, len(rewards) // 4) if len(rewards) > 4 else 1\n",
    "        \n",
    "        # Recompensas\n",
    "        axes[0].plot(rewards, alpha=0.2, color=color)\n",
    "        if window > 1:\n",
    "            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            axes[0].plot(range(window-1, len(rewards)), smoothed, color=color, label=label, linewidth=2)\n",
    "        \n",
    "        # Longitud\n",
    "        axes[1].plot(lengths, alpha=0.2, color=color)\n",
    "        if window > 1:\n",
    "            smoothed = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "            axes[1].plot(range(window-1, len(lengths)), smoothed, color=color, label=label, linewidth=2)\n",
    "        \n",
    "        # Tasa de √©xito\n",
    "        if successes:\n",
    "            cumsum = np.cumsum(successes)\n",
    "            rate = cumsum / (np.arange(len(successes)) + 1)\n",
    "            axes[2].plot(rate, color=color, label=label, linewidth=2)\n",
    "    \n",
    "    axes[0].set_xlabel('Episodio')\n",
    "    axes[0].set_ylabel('Recompensa')\n",
    "    axes[0].set_title('Recompensa por Episodio')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_xlabel('Episodio')\n",
    "    axes[1].set_ylabel('Pasos')\n",
    "    axes[1].set_title('Longitud de Episodio')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].set_xlabel('Episodio')\n",
    "    axes[2].set_ylabel('Tasa')\n",
    "    axes[2].set_title('Tasa de √âxito Acumulada')\n",
    "    axes[2].set_ylim(0, 1)\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Funciones de entrenamiento cargadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar agente b√°sico en Empty-5x5\n",
    "print(\"=\"*60)\n",
    "print(\"  ENTRENAMIENTO DEMO - Empty 5x5\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = crear_entorno(\"Empty\", wrapper=\"cnn\")\n",
    "\n",
    "policy_kwargs = {\n",
    "    \"features_extractor_class\": MinigridCNN,\n",
    "    \"features_extractor_kwargs\": {\"features_dim\": 64}\n",
    "}\n",
    "\n",
    "model = PPO(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=0,\n",
    "    learning_rate=0.0003,\n",
    "    n_steps=128,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.99,\n",
    "    ent_coef=0.01,\n",
    ")\n",
    "\n",
    "callback = MinigridCallback()\n",
    "\n",
    "TIMESTEPS = 30000  # Reducido para demo\n",
    "model.learn(total_timesteps=TIMESTEPS, callback=callback, progress_bar=True)\n",
    "\n",
    "print(f\"\\nEpisodios completados: {len(callback.episode_rewards)}\")\n",
    "if callback.episode_rewards:\n",
    "    print(f\"Recompensa promedio (√∫ltimos 20): {np.mean(callback.episode_rewards[-20:]):.3f}\")\n",
    "    print(f\"Tasa de √©xito: {100*sum(callback.successes)/len(callback.successes):.1f}%\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar entrenamiento\n",
    "plot_training([callback], [\"PPO Empty-5x5\"], \"Entrenamiento Demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Variantes Implementadas\n",
    "\n",
    "### Variante A: MLP vs CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_mlp_vs_cnn(env_name=\"Empty8\", timesteps=30000):\n",
    "    \"\"\"\n",
    "    Compara MLP (observaci√≥n aplanada) vs CNN (imagen).\n",
    "    \n",
    "    MLP: Trata la observaci√≥n como vector de 147 elementos\n",
    "    CNN: Procesa la imagen 7√ó7√ó3 con convoluciones\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  VARIANTE A: MLP vs CNN en {env_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    # MLP\n",
    "    print(\"\\n[1/2] Entrenando MLP...\")\n",
    "    env_mlp = crear_entorno(env_name, wrapper=\"flat\")\n",
    "    \n",
    "    model_mlp = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env_mlp,\n",
    "        verbose=0,\n",
    "        learning_rate=0.0003,\n",
    "        n_steps=128,\n",
    "        batch_size=64,\n",
    "        ent_coef=0.01,\n",
    "    )\n",
    "    \n",
    "    callback_mlp = MinigridCallback()\n",
    "    model_mlp.learn(total_timesteps=timesteps, callback=callback_mlp, progress_bar=True)\n",
    "    resultados['MLP'] = callback_mlp\n",
    "    env_mlp.close()\n",
    "    \n",
    "    # CNN\n",
    "    print(\"\\n[2/2] Entrenando CNN...\")\n",
    "    env_cnn = crear_entorno(env_name, wrapper=\"cnn\")\n",
    "    \n",
    "    policy_kwargs = {\n",
    "        \"features_extractor_class\": MinigridCNN,\n",
    "        \"features_extractor_kwargs\": {\"features_dim\": 64}\n",
    "    }\n",
    "    \n",
    "    model_cnn = PPO(\n",
    "        \"CnnPolicy\",\n",
    "        env_cnn,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=0,\n",
    "        learning_rate=0.0003,\n",
    "        n_steps=128,\n",
    "        batch_size=64,\n",
    "        ent_coef=0.01,\n",
    "    )\n",
    "    \n",
    "    callback_cnn = MinigridCallback()\n",
    "    model_cnn.learn(total_timesteps=timesteps, callback=callback_cnn, progress_bar=True)\n",
    "    resultados['CNN'] = callback_cnn\n",
    "    env_cnn.close()\n",
    "    \n",
    "    # Resultados\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS\")\n",
    "    print(\"=\"*60)\n",
    "    for nombre, cb in resultados.items():\n",
    "        if cb.episode_rewards:\n",
    "            mean_r = np.mean(cb.episode_rewards[-20:])\n",
    "            success = 100 * sum(cb.successes) / len(cb.successes)\n",
    "            print(f\"{nombre}: Recompensa={mean_r:.3f}, √âxito={success:.1f}%\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Ejecutar comparaci√≥n\n",
    "resultados_mlp_cnn = entrenar_mlp_vs_cnn(\"Empty8\", timesteps=30000)\n",
    "plot_training(\n",
    "    [resultados_mlp_cnn['MLP'], resultados_mlp_cnn['CNN']],\n",
    "    ['MLP (147D)', 'CNN (7√ó7√ó3)'],\n",
    "    \"Variante A: MLP vs CNN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variante B: Curriculum Learning\n",
    "\n",
    "Entrenar primero en entornos f√°ciles y transferir a dif√≠ciles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curriculum_learning(timesteps_per_level=20000):\n",
    "    \"\"\"\n",
    "    Curriculum Learning: Entrenar progresivamente en niveles de dificultad.\n",
    "    \n",
    "    Secuencia: Empty5x5 ‚Üí Empty8x8 ‚Üí FourRooms ‚Üí DoorKey5x5\n",
    "    \n",
    "    El modelo mantiene sus pesos entre niveles, permitiendo\n",
    "    que el conocimiento de niveles f√°ciles ayude en los dif√≠ciles.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"  VARIANTE B: CURRICULUM LEARNING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    curriculum = [\n",
    "        (\"Empty\", \"Nivel 1: Empty 5√ó5\"),\n",
    "        (\"Empty8\", \"Nivel 2: Empty 8√ó8\"),\n",
    "        (\"FourRooms\", \"Nivel 3: FourRooms\"),\n",
    "        (\"DoorKey\", \"Nivel 4: DoorKey 5√ó5\"),\n",
    "    ]\n",
    "    \n",
    "    all_callbacks = []\n",
    "    model = None\n",
    "    \n",
    "    for idx, (env_name, desc) in enumerate(curriculum):\n",
    "        print(f\"\\n[{idx+1}/{len(curriculum)}] {desc}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        env = crear_entorno(env_name, wrapper=\"cnn\")\n",
    "        \n",
    "        if model is None:\n",
    "            # Crear modelo nuevo\n",
    "            policy_kwargs = {\n",
    "                \"features_extractor_class\": MinigridCNN,\n",
    "                \"features_extractor_kwargs\": {\"features_dim\": 64}\n",
    "            }\n",
    "            model = PPO(\n",
    "                \"CnnPolicy\", env,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                verbose=0,\n",
    "                learning_rate=0.0003,\n",
    "                ent_coef=0.01,\n",
    "            )\n",
    "        else:\n",
    "            # Transferir al nuevo entorno\n",
    "            model.set_env(env)\n",
    "        \n",
    "        callback = MinigridCallback()\n",
    "        model.learn(total_timesteps=timesteps_per_level, callback=callback, progress_bar=True)\n",
    "        all_callbacks.append((env_name, callback))\n",
    "        \n",
    "        if callback.successes:\n",
    "            success_rate = 100 * sum(callback.successes) / len(callback.successes)\n",
    "            print(f\"  ‚Üí Tasa de √©xito: {success_rate:.1f}%\")\n",
    "        \n",
    "        env.close()\n",
    "    \n",
    "    return model, all_callbacks\n",
    "\n",
    "# Comparar: Curriculum vs Entrenar directo en DoorKey\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  COMPARACI√ìN: CURRICULUM vs DIRECTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Curriculum\n",
    "model_curr, callbacks_curr = curriculum_learning(timesteps_per_level=15000)\n",
    "\n",
    "# Directo (solo DoorKey)\n",
    "print(\"\\n[DIRECTO] Entrenando solo en DoorKey...\")\n",
    "env_direct = crear_entorno(\"DoorKey\", wrapper=\"cnn\")\n",
    "policy_kwargs = {\n",
    "    \"features_extractor_class\": MinigridCNN,\n",
    "    \"features_extractor_kwargs\": {\"features_dim\": 64}\n",
    "}\n",
    "model_direct = PPO(\"CnnPolicy\", env_direct, policy_kwargs=policy_kwargs, verbose=0, ent_coef=0.01)\n",
    "callback_direct = MinigridCallback()\n",
    "model_direct.learn(total_timesteps=60000, callback=callback_direct, progress_bar=True)  # Mismo total\n",
    "env_direct.close()\n",
    "\n",
    "# Comparar en DoorKey\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTADOS EN DOORKEY\")\n",
    "print(\"=\"*60)\n",
    "curr_doorkey = callbacks_curr[-1][1]  # √öltimo nivel del curriculum\n",
    "print(f\"Curriculum: √âxito={100*sum(curr_doorkey.successes)/len(curr_doorkey.successes):.1f}%\")\n",
    "print(f\"Directo:    √âxito={100*sum(callback_direct.successes)/len(callback_direct.successes):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variante C: Entropy Coefficient Study\n",
    "\n",
    "El entropy coefficient controla la exploraci√≥n del agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_study(env_name=\"FourRooms\", timesteps=30000):\n",
    "    \"\"\"\n",
    "    Estudia el efecto del entropy coefficient en la exploraci√≥n.\n",
    "    \n",
    "    ent_coef alto (0.1): M√°s exploraci√≥n, acciones m√°s aleatorias\n",
    "    ent_coef bajo (0.0): Menos exploraci√≥n, acciones m√°s deterministas\n",
    "    \n",
    "    En entornos con recompensa sparse (como MiniGrid), \n",
    "    necesitamos suficiente exploraci√≥n para encontrar la meta.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  VARIANTE C: ENTROPY COEFFICIENT STUDY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    ent_coefs = [0.0, 0.01, 0.05, 0.1]\n",
    "    resultados = {}\n",
    "    \n",
    "    for ent_coef in ent_coefs:\n",
    "        print(f\"\\nEntrenando con ent_coef={ent_coef}...\")\n",
    "        \n",
    "        env = crear_entorno(env_name, wrapper=\"cnn\")\n",
    "        \n",
    "        policy_kwargs = {\n",
    "            \"features_extractor_class\": MinigridCNN,\n",
    "            \"features_extractor_kwargs\": {\"features_dim\": 64}\n",
    "        }\n",
    "        \n",
    "        model = PPO(\n",
    "            \"CnnPolicy\", env,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=0,\n",
    "            learning_rate=0.0003,\n",
    "            ent_coef=ent_coef,\n",
    "        )\n",
    "        \n",
    "        callback = MinigridCallback()\n",
    "        model.learn(total_timesteps=timesteps, callback=callback, progress_bar=True)\n",
    "        \n",
    "        resultados[f\"ent={ent_coef}\"] = callback\n",
    "        env.close()\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Ejecutar estudio\n",
    "resultados_entropy = entropy_study(\"FourRooms\", timesteps=25000)\n",
    "\n",
    "# Graficar\n",
    "callbacks = list(resultados_entropy.values())\n",
    "labels = list(resultados_entropy.keys())\n",
    "plot_training(callbacks, labels, \"Variante C: Efecto del Entropy Coefficient\")\n",
    "\n",
    "# Tabla de resultados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Entropy':<12} {'√âxito':<10} {'Recompensa (√∫ltimos 20)'}\")\n",
    "print(\"-\"*50)\n",
    "for label, cb in resultados_entropy.items():\n",
    "    if cb.successes:\n",
    "        success = 100 * sum(cb.successes) / len(cb.successes)\n",
    "        mean_r = np.mean(cb.episode_rewards[-20:]) if len(cb.episode_rewards) >= 20 else np.mean(cb.episode_rewards)\n",
    "        print(f\"{label:<12} {success:>6.1f}%    {mean_r:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variante D: Multi-Entorno (Generalizaci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_env_training(timesteps=40000):\n",
    "    \"\"\"\n",
    "    Entrena en m√∫ltiples entornos alternando entre ellos.\n",
    "    \n",
    "    Objetivo: Crear un agente que generalice a nuevos entornos.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"  VARIANTE D: MULTI-ENTORNO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Entornos de entrenamiento\n",
    "    train_envs = [\"Empty\", \"Empty8\", \"FourRooms\"]\n",
    "    # Entorno de test (no visto durante entrenamiento)\n",
    "    test_env = \"SimpleCrossing\"\n",
    "    \n",
    "    print(f\"Entrenamiento en: {train_envs}\")\n",
    "    print(f\"Evaluaci√≥n en: {test_env} (no visto)\")\n",
    "    \n",
    "    # Crear entornos\n",
    "    def make_env(name):\n",
    "        def _init():\n",
    "            return crear_entorno(name, wrapper=\"cnn\")\n",
    "        return _init\n",
    "    \n",
    "    # Entrenar rotando entre entornos\n",
    "    all_callbacks = []\n",
    "    model = None\n",
    "    \n",
    "    timesteps_per_env = timesteps // len(train_envs)\n",
    "    \n",
    "    for env_name in train_envs:\n",
    "        print(f\"\\nEntrenando en {env_name}...\")\n",
    "        env = crear_entorno(env_name, wrapper=\"cnn\")\n",
    "        \n",
    "        if model is None:\n",
    "            policy_kwargs = {\n",
    "                \"features_extractor_class\": MinigridCNN,\n",
    "                \"features_extractor_kwargs\": {\"features_dim\": 64}\n",
    "            }\n",
    "            model = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=0, ent_coef=0.01)\n",
    "        else:\n",
    "            model.set_env(env)\n",
    "        \n",
    "        callback = MinigridCallback()\n",
    "        model.learn(total_timesteps=timesteps_per_env, callback=callback, progress_bar=True)\n",
    "        all_callbacks.append((env_name, callback))\n",
    "        env.close()\n",
    "    \n",
    "    # Evaluar en entorno no visto\n",
    "    print(f\"\\nEvaluando en {test_env}...\")\n",
    "    env_test = crear_entorno(test_env, wrapper=\"cnn\")\n",
    "    \n",
    "    rewards = []\n",
    "    successes = []\n",
    "    for _ in range(50):\n",
    "        obs, _ = env_test.reset()\n",
    "        done = False\n",
    "        total_r = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, r, term, trunc, _ = env_test.step(action)\n",
    "            total_r += r\n",
    "            done = term or trunc\n",
    "        rewards.append(total_r)\n",
    "        successes.append(1 if total_r > 0 else 0)\n",
    "    \n",
    "    env_test.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS DE GENERALIZACI√ìN\")\n",
    "    print(\"=\"*60)\n",
    "    for name, cb in all_callbacks:\n",
    "        if cb.successes:\n",
    "            print(f\"{name}: {100*sum(cb.successes)/len(cb.successes):.1f}% √©xito\")\n",
    "    print(f\"\\n{test_env} (NO VISTO): {100*sum(successes)/len(successes):.1f}% √©xito\")\n",
    "    print(f\"Recompensa promedio en test: {np.mean(rewards):.3f}\")\n",
    "    \n",
    "    return model, all_callbacks, (rewards, successes)\n",
    "\n",
    "# Ejecutar\n",
    "model_multi, callbacks_multi, test_results = multi_env_training(timesteps=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variante E: Memoria (LSTM) - Conceptual\n",
    "\n",
    "Para tareas que requieren memoria (recordar d√≥nde est√° la llave), se necesita arquitectura recurrente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: RecurrentPPO requiere sb3-contrib\n",
    "# !pip install sb3-contrib\n",
    "\n",
    "try:\n",
    "    from sb3_contrib import RecurrentPPO\n",
    "    SB3_CONTRIB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SB3_CONTRIB_AVAILABLE = False\n",
    "    print(\"sb3-contrib no instalado. Instalar con: pip install sb3-contrib\")\n",
    "\n",
    "def entrenar_con_memoria(env_name=\"DoorKey\", timesteps=50000):\n",
    "    \"\"\"\n",
    "    Entrena con LSTM para tareas que requieren memoria.\n",
    "    \n",
    "    En DoorKey, el agente debe:\n",
    "    1. Encontrar la llave\n",
    "    2. Recordar d√≥nde est√° la puerta\n",
    "    3. Volver y abrirla\n",
    "    \n",
    "    Sin memoria, el agente \"olvida\" d√≥nde vio la puerta.\n",
    "    \"\"\"\n",
    "    if not SB3_CONTRIB_AVAILABLE:\n",
    "        print(\"Requiere sb3-contrib. Ejemplo conceptual:\")\n",
    "        print(\"\"\"\n",
    "        from sb3_contrib import RecurrentPPO\n",
    "        \n",
    "        model = RecurrentPPO(\n",
    "            \"MlpLstmPolicy\",  # Pol√≠tica con LSTM\n",
    "            env,\n",
    "            verbose=1,\n",
    "            n_steps=128,\n",
    "            batch_size=64,\n",
    "            n_epochs=4,\n",
    "            gamma=0.99,\n",
    "            ent_coef=0.01,\n",
    "            # LSTM espec√≠fico\n",
    "            policy_kwargs=dict(\n",
    "                lstm_hidden_size=64,\n",
    "                n_lstm_layers=1,\n",
    "            )\n",
    "        )\n",
    "        \"\"\")\n",
    "        return None\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"  VARIANTE E: PPO CON MEMORIA (LSTM)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    env = crear_entorno(env_name, wrapper=\"flat\")  # LSTM funciona mejor con MLP\n",
    "    \n",
    "    model = RecurrentPPO(\n",
    "        \"MlpLstmPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        n_steps=128,\n",
    "        batch_size=64,\n",
    "        n_epochs=4,\n",
    "        ent_coef=0.01,\n",
    "    )\n",
    "    \n",
    "    callback = MinigridCallback()\n",
    "    model.learn(total_timesteps=timesteps, callback=callback, progress_bar=True)\n",
    "    \n",
    "    env.close()\n",
    "    return model, callback\n",
    "\n",
    "# Intentar entrenar con memoria\n",
    "result_lstm = entrenar_con_memoria(\"DoorKey\", timesteps=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluaci√≥n Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_visual(model, env_name=\"Empty\", n_episodios=3):\n",
    "    \"\"\"Eval√∫a y visualiza el agente.\"\"\"\n",
    "    env_id = ENTORNOS.get(env_name, (env_name, \"\", \"\"))[0] if env_name in ENTORNOS else env_name\n",
    "    env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "    env_wrapped = SimpleObsWrapper(env)\n",
    "    \n",
    "    for ep in range(n_episodios):\n",
    "        obs, _ = env_wrapped.reset()\n",
    "        frames = [env.render()]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 100:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, term, trunc, _ = env_wrapped.step(action)\n",
    "            frames.append(env.render())\n",
    "            total_reward += reward\n",
    "            done = term or trunc\n",
    "            steps += 1\n",
    "        \n",
    "        # Mostrar algunos frames\n",
    "        n_show = min(6, len(frames))\n",
    "        indices = np.linspace(0, len(frames)-1, n_show, dtype=int)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, n_show, figsize=(3*n_show, 3))\n",
    "        for idx, ax in zip(indices, axes):\n",
    "            ax.imshow(frames[idx])\n",
    "            ax.set_title(f\"Step {idx}\")\n",
    "            ax.axis('off')\n",
    "        \n",
    "        resultado = \"META\" if total_reward > 0 else \"Fallo\"\n",
    "        plt.suptitle(f\"Episodio {ep+1}: {resultado} ({steps} pasos, R={total_reward:.3f})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# Evaluar el modelo entrenado\n",
    "print(\"Evaluaci√≥n del modelo entrenado:\")\n",
    "evaluar_visual(model, \"Empty\", n_episodios=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resumen y Conclusiones\n",
    "\n",
    "### Preguntas Respondidas\n",
    "\n",
    "**¬øPor qu√© el agente solo ve 7√ó7?**\n",
    "- MiniGrid implementa **observaci√≥n parcial** para simular visi√≥n limitada\n",
    "- Esto crea desaf√≠os de exploraci√≥n y memoria que no existen con observaci√≥n completa\n",
    "- Es m√°s realista para aplicaciones rob√≥ticas\n",
    "\n",
    "**¬øMLP o CNN?**\n",
    "- **CNN** preserva estructura espacial y detecta patrones locales\n",
    "- **MLP** es m√°s r√°pido pero pierde relaciones espaciales\n",
    "- Para observaciones 7√ó7√ó3, CNN tiene ventaja significativa\n",
    "\n",
    "**¬øC√≥mo resolver tareas que requieren memoria?**\n",
    "- Usar arquitecturas recurrentes (LSTM/GRU)\n",
    "- RecurrentPPO de sb3-contrib\n",
    "- Alternativamente, agregar historial de observaciones (frame stacking)\n",
    "\n",
    "### Lecciones Aprendidas\n",
    "\n",
    "| Concepto | Hallazgo |\n",
    "|----------|----------|\n",
    "| Entropy coefficient | 0.01-0.05 √≥ptimo para recompensa sparse |\n",
    "| Curriculum Learning | Ayuda significativamente en tareas dif√≠ciles |\n",
    "| CNN vs MLP | CNN mejor para observaciones estructuradas |\n",
    "| Generalizaci√≥n | Entrenar en m√∫ltiples entornos mejora transfer |\n",
    "\n",
    "### Siguientes Pasos\n",
    "\n",
    "1. Probar entornos m√°s dif√≠ciles (LavaCrossing, DistShift)\n",
    "2. Implementar observation stacking para pseudo-memoria\n",
    "3. Explorar attention mechanisms para observaci√≥n parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Referencias\n",
    "\n",
    "- [MiniGrid Documentation](https://minigrid.farama.org/)\n",
    "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)\n",
    "- [PPO Paper](https://arxiv.org/abs/1707.06347)\n",
    "- [Curriculum Learning Survey](https://arxiv.org/abs/2003.04960)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## üó∫Ô∏è Variantes de Entrenamiento ‚Äî MiniGrid\n\nLas variantes en MiniGrid exploran c√≥mo **representar la observaci√≥n**:\nla misma imagen 7√ó7√ó3 puede procesarse de formas muy distintas.\n\n| Variante | Red | Observaci√≥n | Estructura espacial |\n|----------|-----|-------------|---------------------|\n| A | MLP | Vector aplanado (147D) | No aprovecha |\n| B | CNN *(actual)* | Imagen 7√ó7√ó3 | S√≠ aprovecha |\n| C | CNN + Curriculum | M√∫ltiples entornos | S√≠ + progresi√≥n |\n\n**Observaci√≥n MiniGrid**: el agente ve una ventana parcial 7√ó7 centrada en √©l.\nCada celda tiene 3 valores: tipo de objeto, color, estado.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Variante A ‚Äî MLP + Observaci√≥n Aplanada\n\n```python\npython minigrid_navegacion.py --variant flat\n```\n\nLa imagen 7√ó7√ó3 se aplana a un vector de **147 valores** y se pasa directamente a una red densa (MLP).\n\n**Desventaja fundamental**: el MLP no sabe que el valor en la posici√≥n [3,4] est√° *al lado* del valor en [3,5]. Pierde la informaci√≥n espacial.\n\n```\nImagen 7√ó7√ó3:              Vector 147D:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            [0.0, 1.0, 2.0, 0.0, 1.0, ...]\n‚îÇ . . . . . . ‚îÇ  flatten   ‚îÇ                              ‚îÇ\n‚îÇ . A . . . . ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ 147 valores sin estructura   ‚îÇ\n‚îÇ . . . G . . ‚îÇ            ‚îÇ espacial                     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Cu√°ndo funciona**: entornos peque√±os y simples donde la posici√≥n relativa importa poco.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante A: MLP + Observaci√≥n Aplanada\n# from minigrid_navegacion import entrenar_flat\n# model, callback = entrenar_flat(env_name=\"Empty\", timesteps=30000, algorithm=\"PPO\")\n\nprint(\"Variante A: MLP + Observaci√≥n Aplanada\")\nprint()\nflat_code = \"\"\"\n# FlatObsWrapper: aplana la imagen a vector\nclass FlatObsWrapper(gym.ObservationWrapper):\n    def __init__(self, env):\n        obs_shape = env.observation_space['image'].shape  # (7, 7, 3)\n        self.observation_space = gym.spaces.Box(\n            low=0, high=255,\n            shape=(np.prod(obs_shape),),   # 147\n            dtype=np.float32\n        )\n    \n    def observation(self, obs):\n        return obs['image'].flatten().astype(np.float32) / 255.0\n\n# Red: MLP simple\nPPO(\n    \"MlpPolicy\",                    # Red densa\n    env,\n    policy_kwargs={\"net_arch\": [256, 256]},\n)\n\"\"\"\nprint(flat_code)\nprint(\"Tama√±o de entrada: 7√ó7√ó3 = 147 valores\")\nprint(\"Red: [147 ‚Üí 256 ‚Üí 256 ‚Üí 6 acciones]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Variante B ‚Äî CNN Personalizada *(implementaci√≥n actual)*\n\n```python\npython minigrid_navegacion.py --variant cnn\n```\n\nLa imagen 7√ó7√ó3 se procesa con **convoluciones** que respetan la estructura espacial. Cada filtro convolucional \"mira\" una zona de la imagen.\n\n**Arquitectura MinigridCNN**:\n```\nEntrada: 7√ó7√ó3\n  ‚Üí Conv2d(3‚Üí16, kernel=2)   ‚Üí 6√ó6√ó16\n  ‚Üí Conv2d(16‚Üí32, kernel=2)  ‚Üí 5√ó5√ó32\n  ‚Üí Conv2d(32‚Üí64, kernel=2)  ‚Üí 4√ó4√ó64\n  ‚Üí Flatten                  ‚Üí 1024\n  ‚Üí Linear(64)               ‚Üí 64 features\n  ‚Üí pol√≠tica PPO/DQN\n```\n\n**Ventaja vs MLP**: los filtros convolucionales aprenden a detectar patrones visuales locales (paredes, puertas, la posici√≥n relativa del agente).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante B: CNN personalizada (default)\n# from minigrid_navegacion import entrenar_minigrid\n# model, callback = entrenar_minigrid(env_name=\"DoorKey\", timesteps=50000, algorithm=\"PPO\")\n\nprint(\"Variante B: CNN personalizada (MinigridCNN)\")\nprint()\ncnn_code = \"\"\"\nclass MinigridCNN(BaseFeaturesExtractor):\n    def __init__(self, observation_space, features_dim=64):\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=2),   # 7√ó7√ó3 ‚Üí 6√ó6√ó16\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=2),  # 6√ó6√ó16 ‚Üí 5√ó5√ó32\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=2),  # 5√ó5√ó32 ‚Üí 4√ó4√ó64\n            nn.ReLU(),\n            nn.Flatten(),                       # ‚Üí 1024\n        )\n        self.linear = nn.Sequential(\n            nn.Linear(1024, features_dim),      # ‚Üí 64\n            nn.ReLU()\n        )\n    \n    def forward(self, x):\n        x = x.permute(0, 3, 1, 2)  # (B,H,W,C) ‚Üí (B,C,H,W)\n        return self.linear(self.cnn(x))\n\n# Usar en PPO:\npolicy_kwargs = {\n    \"features_extractor_class\": MinigridCNN,\n    \"features_extractor_kwargs\": {\"features_dim\": 64}\n}\nPPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, ...)\n\"\"\"\nprint(cnn_code)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Variante C ‚Äî Curriculum Learning\n\n```python\npython minigrid_navegacion.py --curriculum\n```\n\nEl agente aprende habilidades de navegaci√≥n progresivamente:\n\n| Nivel | Entorno | Habilidad nueva |\n|-------|---------|----------------|\n| 1 | Empty-5x5 | Ir a la meta (sin obst√°culos) |\n| 2 | Empty-8x8 | Planificar rutas m√°s largas |\n| 3 | FourRooms | Navegar entre habitaciones |\n| 4 | DoorKey | Recoger llave, abrir puerta, ir a meta |\n| 5 | LavaCrossing | Evitar obst√°culos de lava |\n\n**Concepto clave**: DoorKey es muy dif√≠cil de aprender desde cero porque el agente debe:\n1. Encontrar la llave\n2. Recogerla\n3. Encontrar la puerta\n4. Abrirla\n5. Llegar a la meta\n\nCon curriculum, el agente ya sabe navegar (nivel 1-3) y solo necesita aprender la secuencia llave-puerta.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante C: Curriculum Learning\n# from minigrid_navegacion import curriculum_minigrid\n# model, historial = curriculum_minigrid(timesteps_por_nivel=20000, algorithm=\"PPO\")\n# Genera: minigrid_curriculum.png\n\nprint(\"Variante C: Curriculum Learning (5 niveles)\")\nprint()\ncurriculum_code = \"\"\"\ncurriculum = [\n    (\"Empty\",        \"Navegar a meta ‚Äî trivial\"),\n    (\"Empty8\",       \"Meta m√°s lejos\"),\n    (\"FourRooms\",    \"Navegar entre habitaciones\"),\n    (\"DoorKey\",      \"Llave ‚Üí puerta ‚Üí meta\"),\n    (\"LavaCrossing\", \"Evitar lava\"),\n]\n\n# Transferencia entre niveles:\nif nivel == 0:\n    model = PPO(\"CnnPolicy\", env, policy_kwargs=cnn_kwargs, ...)\nelse:\n    model.set_env(env)  # Mismo modelo, entorno m√°s complejo\n    # Los pesos CNN se mantienen (ya sabe 'ver')\n    # Solo necesita aprender nuevos comportamientos\n\nmodel.learn(timesteps_por_nivel, reset_num_timesteps=(nivel==0))\n\"\"\"\nprint(curriculum_code)\nprint(\"M√©tricas de seguimiento:\")\nprint(\"  - Recompensa por episodio (progresi√≥n)\")\nprint(\"  - Tasa de √©xito acumulada (% episodios que llega a la meta)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Comparativa: MLP vs CNN vs Curriculum\n\n| Aspecto | A: MLP+Flat | B: CNN | C: CNN+Curriculum |\n|---------|-------------|--------|-------------------|\n| Par√°metros | Menos | M√°s | M√°s |\n| Entiende espacial | No | S√≠ | S√≠ |\n| Tasa de √©xito (Easy) | Media | Alta | Alta |\n| Tasa de √©xito (DoorKey) | Baja | Media | Alta |\n| Timesteps necesarios | Menos | M√°s | Muchos |\n\n**Lecci√≥n principal**: en entornos con estructura espacial (laberintos, cuadr√≠culas), las CNN son casi siempre superiores a los MLP. El curriculum multiplica ese efecto en entornos dif√≠ciles.\n\n**Experimento recomendado**: entrenar Variante A y B en DoorKey con los mismos timesteps y comparar la tasa de √©xito final.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}