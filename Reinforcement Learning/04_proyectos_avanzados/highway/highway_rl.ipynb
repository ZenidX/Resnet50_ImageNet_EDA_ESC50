{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highway-Env: Conducci√≥n Aut√≥noma con RL\n",
    "\n",
    "## Objetivos de este Notebook\n",
    "\n",
    "1. **Entender la observaci√≥n kinematics**: ¬øC√≥mo ve el agente a otros veh√≠culos?\n",
    "2. **Estudiar gamma bajo**: ¬øPor qu√© Œ≥=0.8 funciona mejor que Œ≥=0.99?\n",
    "3. **Transfer learning**: ¬øGeneraliza de highway a merge?\n",
    "4. **Reward shaping**: Velocidad vs seguridad\n",
    "5. **M√∫ltiples entornos**: Highway, Parking, Intersection\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisitos\n",
    "\n",
    "```bash\n",
    "pip install highway-env stable-baselines3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "try:\n",
    "    import highway_env\n",
    "    HIGHWAY_AVAILABLE = True\n",
    "    print(\"Highway-Env disponible\")\n",
    "except ImportError:\n",
    "    HIGHWAY_AVAILABLE = False\n",
    "    print(\"Instalar con: pip install highway-env\")\n",
    "\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Entornos Disponibles\n",
    "\n",
    "| Entorno | ID | Descripci√≥n |\n",
    "|---------|-----|-------------|\n",
    "| Highway | `highway-v0` | Adelantar coches en autopista |\n",
    "| Merge | `merge-v0` | Incorporarse a autopista |\n",
    "| Roundabout | `roundabout-v0` | Navegar rotonda |\n",
    "| Parking | `parking-v0` | Aparcar en plaza |\n",
    "| Intersection | `intersection-v0` | Cruzar intersecci√≥n |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HIGHWAY_AVAILABLE:\n",
    "    env = gym.make(\"highway-v0\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ENTORNO: Highway-v0\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nObservaci√≥n: {env.observation_space}\")\n",
    "    print(f\"Acciones: {env.action_space}\")\n",
    "    \n",
    "    # Ver observaci√≥n\n",
    "    obs, info = env.reset()\n",
    "    print(f\"\\nObservaci√≥n shape: {obs.shape}\")\n",
    "    print(f\"Ejemplo: {obs}\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. An√°lisis de la Observaci√≥n Kinematics\n",
    "\n",
    "## ¬øC√≥mo ve el agente a otros veh√≠culos?\n",
    "\n",
    "La observaci√≥n es una **matriz 5√ó5** donde:\n",
    "- Fila 0: Ego-vehicle (tu coche)\n",
    "- Filas 1-4: Veh√≠culos cercanos\n",
    "\n",
    "Cada fila tiene 5 features:\n",
    "\n",
    "| Columna | Feature | Descripci√≥n |\n",
    "|---------|---------|-------------|\n",
    "| 0 | presence | ¬øHay veh√≠culo? (0/1) |\n",
    "| 1 | x | Posici√≥n X relativa |\n",
    "| 2 | y | Posici√≥n Y relativa |\n",
    "| 3 | vx | Velocidad X relativa |\n",
    "| 4 | vy | Velocidad Y relativa |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HIGHWAY_AVAILABLE:\n",
    "    env = gym.make(\"highway-v0\")\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"OBSERVACI√ìN KINEMATICS (5 veh√≠culos √ó 5 features)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    features = [\"presence\", \"x\", \"y\", \"vx\", \"vy\"]\n",
    "    vehiculos = [\"Ego (t√∫)\", \"Veh√≠culo 1\", \"Veh√≠culo 2\", \"Veh√≠culo 3\", \"Veh√≠culo 4\"]\n",
    "    \n",
    "    print(f\"\\n{'Veh√≠culo':<15}\", end=\"\")\n",
    "    for f in features:\n",
    "        print(f\"{f:>10}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for i, (nombre, fila) in enumerate(zip(vehiculos, obs)):\n",
    "        print(f\"{nombre:<15}\", end=\"\")\n",
    "        for val in fila:\n",
    "            print(f\"{val:>10.3f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones Disponibles\n",
    "\n",
    "| Acci√≥n | Descripci√≥n |\n",
    "|--------|-------------|\n",
    "| 0 | LANE_LEFT (cambiar carril izquierda) |\n",
    "| 1 | IDLE (mantener) |\n",
    "| 2 | LANE_RIGHT (cambiar carril derecha) |\n",
    "| 3 | FASTER (acelerar) |\n",
    "| 4 | SLOWER (frenar) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funci√≥n de Recompensa\n",
    "\n",
    "| Evento | Recompensa |\n",
    "|--------|------------|\n",
    "| Colisi√≥n | -1 |\n",
    "| Velocidad alta | +0.4 (proporcional) |\n",
    "| Carril derecho | +0.1 |\n",
    "| Cambio de carril | -0.1 (peque√±a penalizaci√≥n) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. C√≥digo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayCallback(BaseCallback):\n",
    "    \"\"\"Callback para highway-env.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.crashes = []\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        for info in self.locals.get('infos', []):\n",
    "            if 'episode' in info:\n",
    "                self.episode_rewards.append(info['episode']['r'])\n",
    "            if 'crashed' in info:\n",
    "                self.crashes.append(info['crashed'])\n",
    "        return True\n",
    "\n",
    "\n",
    "def entrenar_highway(env_id=\"highway-v0\", timesteps=30000, algoritmo=\"DQN\", **kwargs):\n",
    "    \"\"\"Entrena en un entorno de highway-env.\"\"\"\n",
    "    env = gym.make(env_id)\n",
    "    env = Monitor(env)\n",
    "    \n",
    "    config = {\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"gamma\": 0.8,  # Horizonte corto para conducci√≥n\n",
    "        \"buffer_size\": 50000,\n",
    "    }\n",
    "    config.update(kwargs)\n",
    "    \n",
    "    if algoritmo == \"DQN\":\n",
    "        model = DQN(\"MlpPolicy\", env, verbose=0, **config)\n",
    "    else:\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=0, \n",
    "                    learning_rate=config['learning_rate'],\n",
    "                    gamma=config['gamma'])\n",
    "    \n",
    "    callback = HighwayCallback()\n",
    "    model.learn(total_timesteps=timesteps, callback=callback, progress_bar=True)\n",
    "    \n",
    "    env.close()\n",
    "    return model, callback\n",
    "\n",
    "print(\"Funciones cargadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. VARIANTE A: Transfer Learning (Highway ‚Üí Merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estudiar_transfer_learning(timesteps=20000):\n",
    "    \"\"\"\n",
    "    Estudia si un modelo entrenado en highway generaliza a merge.\n",
    "    \"\"\"\n",
    "    if not HIGHWAY_AVAILABLE:\n",
    "        return {}\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"VARIANTE A: Transfer Learning\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    # 1. Entrenar en highway\n",
    "    print(\"\\n1. Entrenando en Highway...\")\n",
    "    model_highway, cb_highway = entrenar_highway(\"highway-v0\", timesteps)\n",
    "    \n",
    "    # Evaluar en highway\n",
    "    env_eval = gym.make(\"highway-v0\")\n",
    "    mean_hw, std_hw = evaluate_policy(model_highway, env_eval, n_eval_episodes=10)\n",
    "    env_eval.close()\n",
    "    print(f\"   En Highway: {mean_hw:.1f} ¬± {std_hw:.1f}\")\n",
    "    \n",
    "    # 2. Evaluar en merge (sin reentrenar)\n",
    "    print(\"\\n2. Evaluando en Merge (sin reentrenar)...\")\n",
    "    env_merge = gym.make(\"merge-v0\")\n",
    "    mean_merge_transfer, std_merge = evaluate_policy(model_highway, env_merge, n_eval_episodes=10)\n",
    "    env_merge.close()\n",
    "    print(f\"   Transfer a Merge: {mean_merge_transfer:.1f} ¬± {std_merge:.1f}\")\n",
    "    \n",
    "    # 3. Entrenar desde cero en merge\n",
    "    print(\"\\n3. Entrenando desde cero en Merge...\")\n",
    "    model_merge, cb_merge = entrenar_highway(\"merge-v0\", timesteps)\n",
    "    \n",
    "    env_merge2 = gym.make(\"merge-v0\")\n",
    "    mean_merge_scratch, _ = evaluate_policy(model_merge, env_merge2, n_eval_episodes=10)\n",
    "    env_merge2.close()\n",
    "    print(f\"   Desde cero en Merge: {mean_merge_scratch:.1f}\")\n",
    "    \n",
    "    resultados = {\n",
    "        'highway_rewards': cb_highway.episode_rewards,\n",
    "        'highway_eval': mean_hw,\n",
    "        'transfer_to_merge': mean_merge_transfer,\n",
    "        'merge_from_scratch': mean_merge_scratch\n",
    "    }\n",
    "    \n",
    "    # Conclusi√≥n\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"CONCLUSI√ìN:\")\n",
    "    if mean_merge_transfer > mean_merge_scratch * 0.5:\n",
    "        print(\"  El modelo S√ç transfiere conocimiento a merge\")\n",
    "    else:\n",
    "        print(\"  El modelo NO transfiere bien a merge\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "if HIGHWAY_AVAILABLE:\n",
    "    resultados_a = estudiar_transfer_learning(timesteps=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. VARIANTE B: Estudio de Gamma\n",
    "\n",
    "### ¬øPor qu√© gamma bajo para conducci√≥n?\n",
    "\n",
    "En conducci√≥n aut√≥noma:\n",
    "- Las decisiones deben ser **r√°pidas y locales**\n",
    "- El futuro lejano es **muy incierto** (otros conductores)\n",
    "- Œ≥=0.8 funciona mejor que Œ≥=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estudiar_gamma_highway(timesteps=15000):\n",
    "    \"\"\"\n",
    "    Compara diferentes valores de gamma en highway.\n",
    "    \"\"\"\n",
    "    if not HIGHWAY_AVAILABLE:\n",
    "        return {}\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"VARIANTE B: Estudio de Gamma en Highway\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    gammas = [0.8, 0.9, 0.95, 0.99]\n",
    "    resultados = {}\n",
    "    \n",
    "    for gamma in gammas:\n",
    "        print(f\"\\nEntrenando con gamma={gamma}...\")\n",
    "        model, callback = entrenar_highway(\"highway-v0\", timesteps, gamma=gamma)\n",
    "        \n",
    "        env_eval = gym.make(\"highway-v0\")\n",
    "        mean_reward, std_reward = evaluate_policy(model, env_eval, n_eval_episodes=10)\n",
    "        env_eval.close()\n",
    "        \n",
    "        resultados[gamma] = {\n",
    "            'rewards': callback.episode_rewards,\n",
    "            'mean': mean_reward,\n",
    "            'std': std_reward\n",
    "        }\n",
    "        print(f\"  gamma={gamma}: {mean_reward:.1f} ¬± {std_reward:.1f}\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "if HIGHWAY_AVAILABLE:\n",
    "    resultados_b = estudiar_gamma_highway(timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar\n",
    "if HIGHWAY_AVAILABLE and 'resultados_b' in dir() and resultados_b:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    for gamma, data in resultados_b.items():\n",
    "        rewards = data['rewards']\n",
    "        if len(rewards) > 5:\n",
    "            smoothed = np.convolve(rewards, np.ones(5)/5, mode='valid')\n",
    "            axes[0].plot(smoothed, label=f'Œ≥={gamma}')\n",
    "    axes[0].set_xlabel('Episodio')\n",
    "    axes[0].set_ylabel('Recompensa')\n",
    "    axes[0].set_title('Efecto de Gamma en Highway')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    gammas = list(resultados_b.keys())\n",
    "    means = [resultados_b[g]['mean'] for g in gammas]\n",
    "    axes[1].bar([str(g) for g in gammas], means)\n",
    "    axes[1].set_xlabel('Gamma')\n",
    "    axes[1].set_ylabel('Recompensa Final')\n",
    "    axes[1].set_title('Rendimiento por Gamma')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. VARIANTE C: Reward Shaping (Velocidad vs Seguridad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_entorno_custom(reward_speed_range=[0.2, 0.5], collision_reward=-1.0):\n",
    "    \"\"\"\n",
    "    Crea entorno con recompensa personalizada.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"highway-v0\")\n",
    "    \n",
    "    # Configurar recompensa\n",
    "    env.unwrapped.config[\"reward_speed_range\"] = reward_speed_range\n",
    "    env.unwrapped.config[\"collision_reward\"] = collision_reward\n",
    "    \n",
    "    return Monitor(env)\n",
    "\n",
    "\n",
    "def estudiar_reward_shaping(timesteps=15000):\n",
    "    \"\"\"\n",
    "    Compara diferentes configuraciones de recompensa.\n",
    "    \"\"\"\n",
    "    if not HIGHWAY_AVAILABLE:\n",
    "        return {}\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"VARIANTE C: Reward Shaping\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    configs = {\n",
    "        \"Balanceado\": {\"reward_speed_range\": [0.2, 0.5], \"collision_reward\": -1.0},\n",
    "        \"Solo Velocidad\": {\"reward_speed_range\": [0.5, 1.0], \"collision_reward\": -0.5},\n",
    "        \"Solo Seguridad\": {\"reward_speed_range\": [0.0, 0.2], \"collision_reward\": -5.0},\n",
    "    }\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    for nombre, config in configs.items():\n",
    "        print(f\"\\nEntrenando '{nombre}'...\")\n",
    "        print(f\"  speed_range={config['reward_speed_range']}, collision={config['collision_reward']}\")\n",
    "        \n",
    "        env = crear_entorno_custom(**config)\n",
    "        \n",
    "        model = DQN(\"MlpPolicy\", env, verbose=0,\n",
    "                    learning_rate=5e-4, gamma=0.8, buffer_size=50000)\n",
    "        \n",
    "        callback = HighwayCallback()\n",
    "        model.learn(total_timesteps=timesteps, callback=callback, progress_bar=True)\n",
    "        \n",
    "        # Evaluar\n",
    "        env_eval = gym.make(\"highway-v0\")\n",
    "        mean_reward, std_reward = evaluate_policy(model, env_eval, n_eval_episodes=10)\n",
    "        \n",
    "        resultados[nombre] = {\n",
    "            'rewards': callback.episode_rewards,\n",
    "            'crashes': callback.crashes,\n",
    "            'mean': mean_reward,\n",
    "            'std': std_reward\n",
    "        }\n",
    "        \n",
    "        crash_rate = np.mean(callback.crashes) if callback.crashes else 0\n",
    "        print(f\"  Resultado: {mean_reward:.1f} (crash rate: {crash_rate:.2%})\")\n",
    "        \n",
    "        env.close()\n",
    "        env_eval.close()\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "if HIGHWAY_AVAILABLE:\n",
    "    resultados_c = estudiar_reward_shaping(timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. VARIANTE D: DQN vs PPO en Parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_en_parking(timesteps=20000):\n",
    "    \"\"\"\n",
    "    Compara DQN y PPO en el entorno de parking.\n",
    "    \"\"\"\n",
    "    if not HIGHWAY_AVAILABLE:\n",
    "        return {}\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"VARIANTE D: DQN vs PPO en Parking\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    for algo in [\"DQN\", \"PPO\"]:\n",
    "        print(f\"\\nEntrenando {algo} en parking...\")\n",
    "        \n",
    "        env = gym.make(\"parking-v0\")\n",
    "        env = Monitor(env)\n",
    "        \n",
    "        if algo == \"DQN\":\n",
    "            model = DQN(\"MlpPolicy\", env, verbose=0,\n",
    "                       learning_rate=5e-4, gamma=0.8, buffer_size=50000)\n",
    "        else:\n",
    "            model = PPO(\"MlpPolicy\", env, verbose=0,\n",
    "                       learning_rate=5e-4, gamma=0.8)\n",
    "        \n",
    "        callback = HighwayCallback()\n",
    "        model.learn(total_timesteps=timesteps, callback=callback, progress_bar=True)\n",
    "        \n",
    "        env_eval = gym.make(\"parking-v0\")\n",
    "        mean_reward, std_reward = evaluate_policy(model, env_eval, n_eval_episodes=10)\n",
    "        env_eval.close()\n",
    "        \n",
    "        resultados[algo] = {\n",
    "            'rewards': callback.episode_rewards,\n",
    "            'mean': mean_reward,\n",
    "            'std': std_reward\n",
    "        }\n",
    "        \n",
    "        print(f\"  {algo}: {mean_reward:.1f} ¬± {std_reward:.1f}\")\n",
    "        env.close()\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "if HIGHWAY_AVAILABLE:\n",
    "    resultados_d = comparar_en_parking(timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Conclusiones\n",
    "\n",
    "## ¬øQu√© aprendimos?\n",
    "\n",
    "1. **Observaci√≥n kinematics**: Matriz 5√ó5 con informaci√≥n relativa de veh√≠culos cercanos\n",
    "\n",
    "2. **Gamma bajo funciona mejor**: Œ≥=0.8 es mejor que Œ≥=0.99 para conducci√≥n porque:\n",
    "   - Las decisiones son locales\n",
    "   - El futuro es muy incierto\n",
    "\n",
    "3. **Transfer learning**: El modelo puede generalizar parcialmente entre entornos similares\n",
    "\n",
    "4. **Reward shaping**: El balance velocidad/seguridad es crucial\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [Highway-Env Documentation](https://highway-env.readthedocs.io/)\n",
    "- [RL for Autonomous Driving Survey](https://arxiv.org/abs/2002.00444)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## üöó Variantes de Entrenamiento ‚Äî Highway\n\nLas variantes en Highway-Env exploran una pregunta diferente: **¬øc√≥mo transferir conocimiento entre entornos?**\n\nCon 7 entornos disponibles (autopista, rotonda, intersecci√≥n...), podemos entrenar un agente en uno y ver c√≥mo se adapta a otros.\n\n| Variante | Estrategia | Entornos | Concepto |\n|----------|-----------|---------|---------|\n| A | Entorno √∫nico *(actual)* | 1 entorno | Baseline |\n| B | Transfer Learning | 2 entornos | Conocimiento reutilizable |\n| C | Curriculum Learning | 5 entornos prog. | Aprender gradualmente |\n\n**Entornos disponibles** (orden de dificultad aproximado):\n`highway-fast` ‚Üí `highway` ‚Üí `merge` ‚Üí `roundabout` ‚Üí `intersection`",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Variante A ‚Äî Entrenamiento en Entorno √önico *(actual)*\n\n```python\npython highway_conduccion.py --env highway --algorithm DQN\npython highway_conduccion.py --env intersection --algorithm DQN\n```\n\nEntrena DQN o PPO en un entorno espec√≠fico. Es el baseline para comparar con las otras variantes.\n\n**Observaci√≥n**: matriz 5√ó5 de cinem√°tica (5 veh√≠culos √ó 5 caracter√≠sticas: presencia, x, y, vx, vy)\n**Acciones**: LANE_LEFT, IDLE, LANE_RIGHT, FASTER, SLOWER\n**Reward**: velocidad alta (+0.4), carril derecho (+0.1), colisi√≥n (-1.0)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante A: Entrenamiento en un solo entorno\n# from highway_conduccion import entrenar_highway\n# model, callback = entrenar_highway(env_name=\"highway\", timesteps=50000, algorithm=\"DQN\")\n\nprint(\"Variante A: Entrenamiento en entorno √∫nico\")\nprint()\nprint(\"Entornos disponibles:\")\nentornos = {\n    \"highway\":      \"Autopista: adelantar coches, mantener velocidad\",\n    \"highway-fast\": \"Autopista simplificada (m√°s r√°pida)\",\n    \"merge\":        \"Incorporarse a autopista\",\n    \"roundabout\":   \"Rotonda\",\n    \"parking\":      \"Aparcar en plaza (acciones continuas)\",\n    \"intersection\": \"Cruzar intersecci√≥n\",\n    \"racetrack\":    \"Circuito de carreras\",\n}\nfor name, desc in entornos.items():\n    print(f\"  {name:<15}: {desc}\")\nprint()\nprint(\"Para entrenar en cualquier entorno:\")\nprint(\"  entrenar_highway(env_name='intersection', timesteps=50000, algorithm='DQN')\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Variante B ‚Äî Transfer Learning entre Entornos\n\n```python\npython highway_conduccion.py --transfer\npython highway_conduccion.py --transfer --source highway --target intersection\n```\n\n**Idea**: las habilidades aprendidas en autopista (mantener velocidad, no chocar) son parcialmente √∫tiles al cruzar una intersecci√≥n, aunque el escenario sea diferente.\n\n**Flujo**:\n1. **Fase 1**: Entrenar en entorno *fuente* (m√°s sencillo, ej. highway) hasta convergencia\n2. **Fase 2**: Transferir pesos al entorno *objetivo* (m√°s complejo, ej. intersection) y hacer fine-tuning\n\n```\nhighway (simple) ‚îÄ‚îÄ‚ñ∫ entrenar ‚îÄ‚îÄ‚ñ∫ pesos red\n                                       ‚îÇ\n                                  set_env(intersection)\n                                       ‚îÇ\nintersection (complejo) ‚îÄ‚îÄ‚ñ∫ fine-tune ‚îÄ‚îÄ‚ñ∫ pol√≠tica final\n```\n\n**Hiperpar√°metro cr√≠tico**: learning rate del fine-tuning. Debe ser menor que el entrenamiento base para no \"olvidar\" lo aprendido (*catastrophic forgetting*).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante B: Transfer Learning\n# from highway_conduccion import transfer_learning\n# model, cb_src, cb_tgt = transfer_learning(\n#     source_env=\"highway\",\n#     target_env=\"intersection\",\n#     source_timesteps=80000,\n#     target_timesteps=40000,\n#     algorithm=\"DQN\"\n# )\n# Genera: highway_transfer_learning.png\n\nprint(\"Variante B: Transfer Learning highway ‚Üí intersection\")\nprint()\ntransfer_code = \"\"\"\n# Fase 1: entrenar en fuente\nmodel, cb_source = entrenar_highway(\"highway\", timesteps=80000, algorithm=\"DQN\")\n\n# Fase 2: transferir al objetivo\nenv_target = crear_entorno(\"intersection\")\nmodel.set_env(env_target)            # Mismo modelo, nuevo entorno\nmodel.learning_rate = 1e-4           # LR reducido: fine-tuning suave\n\nmodel.learn(\n    total_timesteps=40000,\n    reset_num_timesteps=False,       # No reiniciar el contador\n)\n\"\"\"\nprint(transfer_code)\nprint(\"La funci√≥n tambi√©n entrena un modelo desde cero en el objetivo\")\nprint(\"para comparar 'Con transfer' vs 'Sin transfer'.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Variante C ‚Äî Curriculum Learning Progresivo\n\n```python\npython highway_conduccion.py --curriculum\n```\n\nEl agente aprende conducci√≥n de forma gradual, como un estudiante humano:\n\n| Nivel | Entorno | Habilidad a√±adida |\n|-------|---------|-----------------|\n| 1 | highway-fast | Flujo b√°sico, no chocar |\n| 2 | highway | Adelantamientos, cambios de carril |\n| 3 | merge | Incorporaci√≥n, gesti√≥n de huecos |\n| 4 | roundabout | Decisiones de giro, prioridad |\n| 5 | intersection | Coordinaci√≥n compleja |\n\n**Por qu√© funciona**: cada nivel a√±ade una habilidad nueva sobre las anteriores. El agente no tiene que aprender todo desde cero al cambiar de entorno.\n\n**Comparaci√≥n con Variante A** (entrenamiento directo en intersection):\n- Sin curriculum: el agente ve colisiones constantemente al principio ‚Üí se√±al de reward muy escasa\n- Con curriculum: el agente ya sabe conducir cuando llega a intersection ‚Üí aprende m√°s r√°pido",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante C: Curriculum Learning\n# from highway_conduccion import curriculum_learning\n# model, historial = curriculum_learning(timesteps_per_env=20000, algorithm=\"DQN\")\n# Genera: highway_curriculum.png\n\nprint(\"Variante C: Curriculum Learning (5 niveles)\")\nprint()\ncurriculum_code = \"\"\"\ncurriculum = [\n    (\"highway-fast\",  \"Autopista simple\"),     # Nivel 1\n    (\"highway\",       \"Autopista completa\"),   # Nivel 2\n    (\"merge\",         \"Incorporaci√≥n\"),         # Nivel 3\n    (\"roundabout\",    \"Rotonda\"),               # Nivel 4\n    (\"intersection\",  \"Intersecci√≥n\"),          # Nivel 5\n]\n\n# En cada nivel:\nif nivel == 0:\n    model = DQN(\"MlpPolicy\", env, ...)  # Crear modelo nuevo\nelse:\n    model.set_env(env)                   # Transferir al siguiente entorno\n    model.learning_rate *= 0.8           # Reducir LR progresivamente\n\nmodel.learn(timesteps_per_env, reset_num_timesteps=(nivel==0))\nmodel.save(f\"nivel_{nivel}_{env_name}.zip\")  # Checkpoint por nivel\n\"\"\"\nprint(curriculum_code)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Comparativa de Variantes\n\n| Aspecto | A: √önico | B: Transfer | C: Curriculum |\n|---------|----------|-------------|---------------|\n| Entornos | 1 | 2 | 5 |\n| Complejidad impl. | Baja | Media | Media |\n| Rendimiento en entorno f√°cil | Alto | Alto | Alto |\n| Rendimiento en entorno dif√≠cil | Bajo | Medio | Alto |\n| Tiempo total | Corto | Medio | Largo |\n\n**Cu√°ndo usar cada variante**:\n- **A** ‚Äî Baseline o cuando solo interesa un entorno\n- **B** ‚Äî Cuando tienes un entorno fuente y quieres adaptar a otro\n- **C** ‚Äî Cuando quieres el mejor rendimiento posible en el entorno m√°s dif√≠cil\n\n**Lecci√≥n**: el curriculum learning es una de las t√©cnicas m√°s potentes para entornos dif√≠ciles donde el reward es escaso al principio.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}