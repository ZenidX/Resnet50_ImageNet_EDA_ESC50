{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LunarLander: Comparaci√≥n de Algoritmos de RL\n",
    "\n",
    "## Objetivos de este Notebook\n",
    "\n",
    "1. **Comparar PPO vs DQN vs A2C**: ¬øCu√°l funciona mejor?\n",
    "2. **Entender qu√© significa \"resolver\" un entorno**\n",
    "3. **Estudiar el efecto de gamma** (horizonte temporal)\n",
    "4. **Experimentar con learning rate**\n",
    "5. **Probar versi√≥n continua con SAC**\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisitos\n",
    "\n",
    "```bash\n",
    "pip install stable-baselines3 gymnasium[box2d]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Stable-Baselines3\n",
    "from stable_baselines3 import PPO, DQN, A2C, SAC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Verificar Box2D\n",
    "try:\n",
    "    env_test = gym.make(\"LunarLander-v3\")\n",
    "    env_test.close()\n",
    "    LUNAR_AVAILABLE = True\n",
    "    print(\"LunarLander disponible\")\n",
    "except:\n",
    "    LUNAR_AVAILABLE = False\n",
    "    print(\"Instalar con: pip install gymnasium[box2d]\")\n",
    "\n",
    "print(f\"Gymnasium disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Descripci√≥n del Entorno\n",
    "\n",
    "## LunarLander-v3\n",
    "\n",
    "Aterrizar un m√≥dulo lunar de forma segura:\n",
    "\n",
    "```\n",
    "         üöÄ ‚Üê M√≥dulo lunar\n",
    "        / | \\\n",
    "       /  |  \\ ‚Üê Propulsores\n",
    "      ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "     /         \\\n",
    "    /    üéØ     \\ ‚Üê Zona de aterrizaje\n",
    "   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "```\n",
    "\n",
    "### Criterio de \"Resolver\"\n",
    "\n",
    "El entorno se considera **resuelto** cuando el agente obtiene una recompensa promedio de **+200** o m√°s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LUNAR_AVAILABLE:\n",
    "    env = gym.make(\"LunarLander-v3\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ENTORNO: LunarLander-v3\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nEspacio de observaci√≥n: {env.observation_space}\")\n",
    "    print(f\"  - 8 dimensiones continuas\")\n",
    "    print(f\"\\nEspacio de acciones: {env.action_space}\")\n",
    "    print(f\"  - 4 acciones discretas\")\n",
    "    \n",
    "    # Detalles\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"OBSERVACI√ìN (8D):\")\n",
    "    print(\"-\"*60)\n",
    "    obs_desc = [\n",
    "        \"Posici√≥n X\",\n",
    "        \"Posici√≥n Y\", \n",
    "        \"Velocidad X\",\n",
    "        \"Velocidad Y\",\n",
    "        \"√Ångulo\",\n",
    "        \"Velocidad angular\",\n",
    "        \"Pierna izquierda en contacto (0/1)\",\n",
    "        \"Pierna derecha en contacto (0/1)\"\n",
    "    ]\n",
    "    for i, desc in enumerate(obs_desc):\n",
    "        print(f\"  [{i}] {desc}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"ACCIONES (4):\")\n",
    "    print(\"-\"*60)\n",
    "    acciones = [\n",
    "        \"No hacer nada\",\n",
    "        \"Motor izquierdo\",\n",
    "        \"Motor principal (abajo)\",\n",
    "        \"Motor derecho\"\n",
    "    ]\n",
    "    for i, desc in enumerate(acciones):\n",
    "        print(f\"  [{i}] {desc}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"RECOMPENSAS:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"  Aterrizar en zona: +100 a +140\")\n",
    "    print(\"  Cada pierna en contacto: +10\")\n",
    "    print(\"  Motor principal encendido: -0.3/frame\")\n",
    "    print(\"  Motor lateral encendido: -0.03/frame\")\n",
    "    print(\"  Crash: -100\")\n",
    "    print(\"  Salir de pantalla: -100\")\n",
    "    print(\"\\n  RESUELTO: recompensa promedio >= 200\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. An√°lisis de Algoritmos\n",
    "\n",
    "## PPO vs DQN vs A2C\n",
    "\n",
    "| Aspecto | PPO | DQN | A2C |\n",
    "|---------|-----|-----|-----|\n",
    "| **Tipo** | Policy Gradient | Value-based | Policy Gradient |\n",
    "| **On/Off-policy** | On-policy | Off-policy | On-policy |\n",
    "| **Replay Buffer** | No | S√≠ | No |\n",
    "| **Sample Efficiency** | Media | Alta | Baja |\n",
    "| **Estabilidad** | Alta | Media | Media |\n",
    "| **Acciones Continuas** | S√≠ | No | S√≠ |\n",
    "\n",
    "### ¬øCu√°ndo usar cada uno?\n",
    "\n",
    "- **PPO**: Algoritmo por defecto, robusto y estable\n",
    "- **DQN**: Cuando tienes acciones discretas y quieres sample efficiency\n",
    "- **A2C**: Similar a PPO pero m√°s simple, √∫til para debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. C√≥digo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCallback(BaseCallback):\n",
    "    \"\"\"Callback para registrar recompensas.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        for info in self.locals.get('infos', []):\n",
    "            if 'episode' in info:\n",
    "                self.episode_rewards.append(info['episode']['r'])\n",
    "                self.episode_lengths.append(info['episode']['l'])\n",
    "        return True\n",
    "\n",
    "\n",
    "def entrenar(algoritmo, timesteps=50000, **kwargs):\n",
    "    \"\"\"\n",
    "    Entrena un algoritmo en LunarLander.\n",
    "    \n",
    "    Args:\n",
    "        algoritmo: \"PPO\", \"DQN\", \"A2C\" o \"SAC\"\n",
    "        timesteps: Pasos de entrenamiento\n",
    "        **kwargs: Hiperpar√°metros adicionales\n",
    "    \"\"\"\n",
    "    env = gym.make(\"LunarLander-v3\")\n",
    "    env = Monitor(env)\n",
    "    \n",
    "    # Configuraci√≥n por defecto\n",
    "    config = {\n",
    "        \"PPO\": {\"learning_rate\": 0.0003, \"n_steps\": 2048, \"batch_size\": 64, \"gamma\": 0.99},\n",
    "        \"DQN\": {\"learning_rate\": 0.0001, \"buffer_size\": 100000, \"gamma\": 0.99},\n",
    "        \"A2C\": {\"learning_rate\": 0.0007, \"n_steps\": 5, \"gamma\": 0.99},\n",
    "    }\n",
    "    \n",
    "    params = config.get(algoritmo, {})\n",
    "    params.update(kwargs)\n",
    "    \n",
    "    # Crear modelo\n",
    "    if algoritmo == \"PPO\":\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=0, **params)\n",
    "    elif algoritmo == \"DQN\":\n",
    "        model = DQN(\"MlpPolicy\", env, verbose=0, **params)\n",
    "    elif algoritmo == \"A2C\":\n",
    "        model = A2C(\"MlpPolicy\", env, verbose=0, **params)\n",
    "    else:\n",
    "        raise ValueError(f\"Algoritmo no soportado: {algoritmo}\")\n",
    "    \n",
    "    callback = RewardCallback()\n",
    "    model.learn(total_timesteps=timesteps, callback=callback, progress_bar=True)\n",
    "    \n",
    "    env.close()\n",
    "    return model, callback\n",
    "\n",
    "print(\"Funciones de entrenamiento cargadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. VARIANTE A: Comparaci√≥n PPO vs DQN vs A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_algoritmos(timesteps=50000):\n",
    "    \"\"\"\n",
    "    Compara PPO, DQN y A2C en LunarLander.\n",
    "    \"\"\"\n",
    "    if not LUNAR_AVAILABLE:\n",
    "        print(\"LunarLander no disponible\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"VARIANTE A: Comparaci√≥n PPO vs DQN vs A2C\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    for algo in [\"PPO\", \"DQN\", \"A2C\"]:\n",
    "        print(f\"\\nEntrenando {algo}...\")\n",
    "        t0 = time.time()\n",
    "        model, callback = entrenar(algo, timesteps)\n",
    "        tiempo = time.time() - t0\n",
    "        \n",
    "        # Evaluar\n",
    "        env_eval = gym.make(\"LunarLander-v3\")\n",
    "        mean_reward, std_reward = evaluate_policy(model, env_eval, n_eval_episodes=10)\n",
    "        env_eval.close()\n",
    "        \n",
    "        resultados[algo] = {\n",
    "            'rewards': callback.episode_rewards,\n",
    "            'mean': mean_reward,\n",
    "            'std': std_reward,\n",
    "            'time': tiempo,\n",
    "            'solved': mean_reward >= 200\n",
    "        }\n",
    "        \n",
    "        status = \"RESUELTO\" if mean_reward >= 200 else \"No resuelto\"\n",
    "        print(f\"  {algo}: {mean_reward:.1f} ¬± {std_reward:.1f} ({status})\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "if LUNAR_AVAILABLE:\n",
    "    resultados_a = comparar_algoritmos(timesteps=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparaci√≥n\n",
    "if LUNAR_AVAILABLE and resultados_a:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Curvas de aprendizaje\n",
    "    for algo, data in resultados_a.items():\n",
    "        rewards = data['rewards']\n",
    "        if len(rewards) > 20:\n",
    "            smoothed = np.convolve(rewards, np.ones(20)/20, mode='valid')\n",
    "            axes[0].plot(smoothed, label=algo)\n",
    "    axes[0].axhline(y=200, color='g', linestyle='--', label='Resuelto (200)')\n",
    "    axes[0].set_xlabel('Episodio')\n",
    "    axes[0].set_ylabel('Recompensa')\n",
    "    axes[0].set_title('Curvas de Aprendizaje')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Recompensa final\n",
    "    algos = list(resultados_a.keys())\n",
    "    means = [resultados_a[a]['mean'] for a in algos]\n",
    "    stds = [resultados_a[a]['std'] for a in algos]\n",
    "    colors = ['green' if resultados_a[a]['solved'] else 'red' for a in algos]\n",
    "    axes[1].bar(algos, means, yerr=stds, capsize=5, color=colors, alpha=0.7)\n",
    "    axes[1].axhline(y=200, color='g', linestyle='--')\n",
    "    axes[1].set_ylabel('Recompensa Media')\n",
    "    axes[1].set_title('Rendimiento Final')\n",
    "    \n",
    "    # Tiempo de entrenamiento\n",
    "    times = [resultados_a[a]['time'] for a in algos]\n",
    "    axes[2].bar(algos, times, color='skyblue')\n",
    "    axes[2].set_ylabel('Tiempo (s)')\n",
    "    axes[2].set_title('Tiempo de Entrenamiento')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabla resumen\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESUMEN\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n{'Algoritmo':<10} {'Recompensa':<20} {'Tiempo (s)':<12} {'Estado'}\")\n",
    "    print(\"-\" * 55)\n",
    "    for algo in algos:\n",
    "        d = resultados_a[algo]\n",
    "        status = \"RESUELTO\" if d['solved'] else \"No resuelto\"\n",
    "        print(f\"{algo:<10} {d['mean']:.1f} ¬± {d['std']:.1f}{'':8} {d['time']:<12.1f} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. VARIANTE B: LunarLander Continuo con SAC\n",
    "\n",
    "### Diferencia: Acciones Continuas\n",
    "\n",
    "| Discreto | Continuo |\n",
    "|----------|----------|\n",
    "| 4 acciones (0,1,2,3) | 2 valores continuos [-1,1] |\n",
    "| Motor encendido/apagado | Control fino de potencia |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_continuo(timesteps=50000):\n",
    "    \"\"\"\n",
    "    Entrena SAC en LunarLanderContinuous.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"VARIANTE B: LunarLander Continuo con SAC\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        env = gym.make(\"LunarLanderContinuous-v3\")\n",
    "        env = Monitor(env)\n",
    "        \n",
    "        print(f\"\\nEspacio de acciones: {env.action_space}\")\n",
    "        print(\"  [0]: Motor principal (-1 a 1)\")\n",
    "        print(\"  [1]: Motor lateral (-1 a 1)\")\n",
    "        \n",
    "        print(\"\\nEntrenando SAC...\")\n",
    "        \n",
    "        model = SAC(\n",
    "            \"MlpPolicy\", env, verbose=0,\n",
    "            learning_rate=0.0003,\n",
    "            buffer_size=100000,\n",
    "            batch_size=256,\n",
    "            gamma=0.99,\n",
    "            tau=0.005\n",
    "        )\n",
    "        \n",
    "        callback = RewardCallback()\n",
    "        model.learn(total_timesteps=timesteps, callback=callback, progress_bar=True)\n",
    "        \n",
    "        # Evaluar\n",
    "        env_eval = gym.make(\"LunarLanderContinuous-v3\")\n",
    "        mean_reward, std_reward = evaluate_policy(model, env_eval, n_eval_episodes=10)\n",
    "        env_eval.close()\n",
    "        \n",
    "        status = \"RESUELTO\" if mean_reward >= 200 else \"No resuelto\"\n",
    "        print(f\"\\nResultado SAC: {mean_reward:.1f} ¬± {std_reward:.1f} ({status})\")\n",
    "        \n",
    "        env.close()\n",
    "        return model, callback\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "if LUNAR_AVAILABLE:\n",
    "    model_sac, cb_sac = entrenar_continuo(timesteps=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. VARIANTE C: Estudio de Gamma (Horizonte Temporal)\n",
    "\n",
    "### ¬øQu√© es gamma (Œ≥)?\n",
    "\n",
    "El factor de descuento determina cu√°nto valora el agente las recompensas futuras:\n",
    "\n",
    "- **Œ≥ = 0.9**: Horizonte corto, prefiere recompensas inmediatas\n",
    "- **Œ≥ = 0.99**: Horizonte largo, planifica a futuro\n",
    "- **Œ≥ = 0.999**: Horizonte muy largo\n",
    "\n",
    "Return = r‚ÇÄ + Œ≥r‚ÇÅ + Œ≥¬≤r‚ÇÇ + Œ≥¬≥r‚ÇÉ + ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estudiar_gamma(timesteps=30000):\n",
    "    \"\"\"\n",
    "    Estudia el efecto de diferentes valores de gamma.\n",
    "    \"\"\"\n",
    "    if not LUNAR_AVAILABLE:\n",
    "        return {}\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"VARIANTE C: Estudio de Gamma\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    gammas = [0.9, 0.95, 0.99, 0.999]\n",
    "    resultados = {}\n",
    "    \n",
    "    for gamma in gammas:\n",
    "        print(f\"\\nEntrenando PPO con gamma={gamma}...\")\n",
    "        model, callback = entrenar(\"PPO\", timesteps, gamma=gamma)\n",
    "        \n",
    "        env_eval = gym.make(\"LunarLander-v3\")\n",
    "        mean_reward, std_reward = evaluate_policy(model, env_eval, n_eval_episodes=10)\n",
    "        env_eval.close()\n",
    "        \n",
    "        resultados[gamma] = {\n",
    "            'rewards': callback.episode_rewards,\n",
    "            'mean': mean_reward,\n",
    "            'std': std_reward\n",
    "        }\n",
    "        print(f\"  gamma={gamma}: {mean_reward:.1f} ¬± {std_reward:.1f}\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "if LUNAR_AVAILABLE:\n",
    "    resultados_c = estudiar_gamma(timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar efecto de gamma\n",
    "if LUNAR_AVAILABLE and 'resultados_c' in dir() and resultados_c:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Curvas\n",
    "    for gamma, data in resultados_c.items():\n",
    "        rewards = data['rewards']\n",
    "        if len(rewards) > 10:\n",
    "            smoothed = np.convolve(rewards, np.ones(10)/10, mode='valid')\n",
    "            axes[0].plot(smoothed, label=f'Œ≥={gamma}')\n",
    "    axes[0].axhline(y=200, color='g', linestyle='--', alpha=0.5)\n",
    "    axes[0].set_xlabel('Episodio')\n",
    "    axes[0].set_ylabel('Recompensa')\n",
    "    axes[0].set_title('Efecto de Gamma en el Aprendizaje')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Barras\n",
    "    gammas = list(resultados_c.keys())\n",
    "    means = [resultados_c[g]['mean'] for g in gammas]\n",
    "    stds = [resultados_c[g]['std'] for g in gammas]\n",
    "    axes[1].bar([str(g) for g in gammas], means, yerr=stds, capsize=5)\n",
    "    axes[1].axhline(y=200, color='g', linestyle='--')\n",
    "    axes[1].set_xlabel('Gamma (Œ≥)')\n",
    "    axes[1].set_ylabel('Recompensa Final')\n",
    "    axes[1].set_title('Rendimiento por Gamma')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. VARIANTE D: Learning Rate Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estudiar_learning_rate(timesteps=30000):\n",
    "    \"\"\"\n",
    "    Estudia el efecto del learning rate.\n",
    "    \"\"\"\n",
    "    if not LUNAR_AVAILABLE:\n",
    "        return {}\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"VARIANTE D: Learning Rate Sweep\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    lrs = [1e-4, 3e-4, 1e-3, 3e-3]\n",
    "    resultados = {}\n",
    "    \n",
    "    for lr in lrs:\n",
    "        print(f\"\\nEntrenando PPO con lr={lr}...\")\n",
    "        model, callback = entrenar(\"PPO\", timesteps, learning_rate=lr)\n",
    "        \n",
    "        env_eval = gym.make(\"LunarLander-v3\")\n",
    "        mean_reward, std_reward = evaluate_policy(model, env_eval, n_eval_episodes=10)\n",
    "        env_eval.close()\n",
    "        \n",
    "        resultados[lr] = {\n",
    "            'rewards': callback.episode_rewards,\n",
    "            'mean': mean_reward,\n",
    "            'std': std_reward\n",
    "        }\n",
    "        print(f\"  lr={lr}: {mean_reward:.1f} ¬± {std_reward:.1f}\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "if LUNAR_AVAILABLE:\n",
    "    resultados_d = estudiar_learning_rate(timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar efecto de learning rate\n",
    "if LUNAR_AVAILABLE and 'resultados_d' in dir() and resultados_d:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Curvas\n",
    "    for lr, data in resultados_d.items():\n",
    "        rewards = data['rewards']\n",
    "        if len(rewards) > 10:\n",
    "            smoothed = np.convolve(rewards, np.ones(10)/10, mode='valid')\n",
    "            axes[0].plot(smoothed, label=f'lr={lr}')\n",
    "    axes[0].set_xlabel('Episodio')\n",
    "    axes[0].set_ylabel('Recompensa')\n",
    "    axes[0].set_title('Efecto del Learning Rate')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Barras\n",
    "    lrs = list(resultados_d.keys())\n",
    "    means = [resultados_d[lr]['mean'] for lr in lrs]\n",
    "    axes[1].bar([f'{lr:.0e}' for lr in lrs], means)\n",
    "    axes[1].set_xlabel('Learning Rate')\n",
    "    axes[1].set_ylabel('Recompensa Final')\n",
    "    axes[1].set_title('Rendimiento por Learning Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Conclusiones\n",
    "\n",
    "## ¬øQu√© aprendimos?\n",
    "\n",
    "1. **Comparaci√≥n de algoritmos**:\n",
    "   - PPO suele ser el m√°s robusto y f√°cil de usar\n",
    "   - DQN puede ser m√°s sample-efficient\n",
    "   - A2C es m√°s simple pero menos estable\n",
    "\n",
    "2. **Gamma (horizonte temporal)**:\n",
    "   - Œ≥=0.99 suele ser un buen valor por defecto\n",
    "   - Valores muy bajos (0.9) no planifican suficiente\n",
    "   - Valores muy altos (0.999) pueden ser inestables\n",
    "\n",
    "3. **Learning rate**:\n",
    "   - 3e-4 suele funcionar bien para PPO\n",
    "   - LR muy alto causa inestabilidad\n",
    "   - LR muy bajo hace el aprendizaje lento\n",
    "\n",
    "4. **Acciones continuas vs discretas**:\n",
    "   - SAC funciona bien para acciones continuas\n",
    "   - Control m√°s fino pero m√°s dif√≠cil de aprender\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [PPO Paper](https://arxiv.org/abs/1707.06347)\n",
    "- [DQN Paper](https://arxiv.org/abs/1312.5602)\n",
    "- [SAC Paper](https://arxiv.org/abs/1801.01290)\n",
    "- [Stable-Baselines3 Docs](https://stable-baselines3.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## üöÄ Variantes de Entrenamiento ‚Äî LunarLander\n\nLunarLander es ideal para comparar algoritmos porque existe en dos versiones:\n- **Discreto** (`LunarLander-v3`): 4 acciones (no hacer nada, motor izq., motor der., motor principal)\n- **Continuo** (`LunarLanderContinuous-v3`): 2 valores reales (potencia motor principal y lateral)\n\nEsto permite comparar algoritmos discretos vs continuos en el mismo entorno.\n\n| Variante | Algoritmo | Entorno | Tipo | N¬∫ params |\n|----------|-----------|---------|------|-----------|\n| A | PPO | Discreto | On-policy | ~50K |\n| B | DQN | Discreto | Off-policy | ~50K |\n| C | A2C | Discreto | On-policy | ~50K |\n| D | SAC/TD3 | **Continuo** | Off-policy | ~100K |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Variantes A/B/C ‚Äî Algoritmos en Entorno Discreto\n\n```python\npython lunarlander_sb3.py --algorithm PPO    # Var. A\npython lunarlander_sb3.py --algorithm DQN    # Var. B\npython lunarlander_sb3.py --algorithm A2C    # Var. C\npython lunarlander_sb3.py --compare          # Comparar A+B+C\n```\n\nLos tres algoritmos resuelven el mismo problema (aterrizar con >200 puntos) pero con filosof√≠as distintas:\n\n**PPO (Proximal Policy Optimization)** ‚Äî Variante A\n- On-policy: aprende solo de experiencias recientes\n- Clipped surrogate objective: evita actualizaciones demasiado grandes\n- Robusto y general ‚Üí algoritmo por defecto en muchos contextos\n\n**DQN (Deep Q-Network)** ‚Äî Variante B\n- Off-policy: replay buffer, aprende de experiencias pasadas\n- M√°s eficiente en datos, pero solo funciona con acciones discretas\n- Puede sobreestimar Q-values (problema de maximizaci√≥n)\n\n**A2C (Advantage Actor-Critic)** ‚Äî Variante C\n- On-policy, m√°s simple que PPO (sin el clip)\n- Actor: aprende la pol√≠tica | Cr√≠tico: aprende el valor del estado\n- M√°s r√°pido por actualizaci√≥n, pero menos estable que PPO",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variantes A/B/C: comparar PPO, DQN y A2C\n# from lunarlander_sb3 import comparar_algoritmos\n# resultados = comparar_algoritmos(timesteps=50000)\n\nprint(\"Comparativa A/B/C: PPO vs DQN vs A2C en LunarLander discreto\")\nprint()\nprint(\"Criterio de √©xito: recompensa media > 200 puntos\")\nprint()\nprint(\"Diferencias clave:\")\ncomparativa = \"\"\"\n                PPO          DQN          A2C\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nTipo:         On-policy    Off-policy   On-policy\nBuffer:       No           S√≠ (100K)    No\nUpdate freq:  Cada 2048    Cada 4 pasos Cada 5 pasos\nEstabilidad:  Alta         Media        Media-baja\nDatos:        Menos efic.  M√°s efic.    Menos efic.\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\"\"\"\nprint(comparativa)\nprint(\"Para comparar visualmente:\")\nprint(\"  Genera: comparacion_algoritmos.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Variante D ‚Äî Entorno Continuo con SAC/TD3\n\n```python\npython lunarlander_sb3.py --continuous                    # SAC (default)\npython lunarlander_sb3.py --continuous --algorithm TD3    # TD3\npython lunarlander_sb3.py --compare-cont                  # Discreto vs Continuo\n```\n\n**¬øQu√© cambia en el entorno continuo?**\n\n| | Discreto | Continuo |\n|-|----------|----------|\n| Entorno | LunarLander-v3 | LunarLanderContinuous-v3 |\n| Espacio de acci√≥n | Discrete(4) | Box([-1,-1], [1,1]) |\n| Acci√≥n | √≠ndice 0-3 | [motor_princ, motor_lat] ‚àà ‚Ñù¬≤ |\n| Control | On/Off | Potencia precisa (0%, 30%, 100%...) |\n| Algoritmos | PPO, DQN, A2C | **SAC, TD3** |\n\nEn el discreto: \"¬øenciendo el motor principal?\" ‚Üí s√≠/no\nEn el continuo: \"¬øcu√°nta potencia al motor principal?\" ‚Üí 0.0 a 1.0",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### SAC vs TD3 para el entorno continuo\n\n**SAC (Soft Actor-Critic)**:\n- Maximiza recompensa *y* entrop√≠a de la pol√≠tica simult√°neamente\n- `objetivo = E[reward] + Œ± √ó E[-log œÄ(a|s)]`\n- La entrop√≠a Œ± es adaptativa (se ajusta autom√°ticamente)\n- Muy explorador ‚Üí bueno cuando el paisaje de recompensa es complejo\n\n**TD3 (Twin Delayed DDPG)**:\n- Usa *dos* redes Q (twin) y toma el m√≠nimo ‚Üí reduce sobreestimaci√≥n\n- Actualiza el actor con retraso (delayed, cada 2 pasos del cr√≠tico)\n- M√°s determinista que SAC, menos exploraci√≥n\n- M√°s estable en entornos con ruido\n\n```\nSAC: maximiza E[reward] + Œ±¬∑H(œÄ)  ‚Üê entrop√≠a m√°xima\nTD3: minimiza error de los 2 Q-networks y act√∫a con ruido gaussiano\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante D: SAC en entorno continuo\n# from lunarlander_sb3 import entrenar_continuo\n# model, callback = entrenar_continuo(algoritmo=\"SAC\", timesteps=100000)\n\nprint(\"Variante D: LunarLanderContinuous-v3 con SAC\")\nprint()\nprint(\"Espacio de acciones continuo:\")\nprint(\"  action = [motor_principal, motor_lateral]\")\nprint(\"  Cada valor ‚àà [-1.0, 1.0]\")\nprint(\"  Negativo = motor apagado\")\nprint()\nsac_config = \"\"\"\nSAC(\n    \"MlpPolicy\",\n    env,                           # LunarLanderContinuous-v3\n    learning_rate=3e-4,\n    buffer_size=100000,\n    learning_starts=1000,\n    batch_size=256,\n    gamma=0.99,\n    tau=0.005,                     # Soft update del target network\n    ent_coef=\"auto\",               # Entrop√≠a adaptativa autom√°tica\n)\n\"\"\"\nprint(\"Configuraci√≥n SAC:\")\nprint(sac_config)\n\nprint(\"Para comparar discreto vs continuo:\")\nprint(\"  from lunarlander_sb3 import comparar_discreto_vs_continuo\")\nprint(\"  comparar_discreto_vs_continuo(timesteps=50000)\")\nprint(\"  ‚Üí Genera: lunarlander_discreto_vs_continuo.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Comparativa Final\n\n| Aspecto | A: PPO | B: DQN | C: A2C | D: SAC/TD3 |\n|---------|--------|--------|--------|------------|\n| Entorno | Discreto | Discreto | Discreto | **Continuo** |\n| Convergencia | ~100K | ~80K | ~120K | ~150K |\n| Estabilidad | Alta | Media | Media | Alta (SAC) |\n| Control | Grosero (4 op.) | Grosero | Grosero | Preciso (continuo) |\n| Curva aprendizaje | Suave | Variable | Ruidosa | Suave |\n\n**Criterio de √©xito**: recompensa media > 200 puntos en 10 episodios de evaluaci√≥n.\n\n**Lecci√≥n**: el espacio de acci√≥n continuo permite control m√°s preciso pero es m√°s dif√≠cil de aprender. SAC y TD3 son los algoritmos est√°ndar para rob√≥tica y control continuo.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}