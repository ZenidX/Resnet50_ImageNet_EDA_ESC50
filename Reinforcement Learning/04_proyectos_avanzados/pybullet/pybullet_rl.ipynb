{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyBullet - Control Continuo y Robótica 3D\n",
    "\n",
    "**Proyecto de Nivel 4 - Reinforcement Learning**\n",
    "\n",
    "Este notebook explora el control de robots en simulación física 3D. A diferencia de entornos con acciones discretas, aquí las acciones son **continuas** (torques en articulaciones), lo que requiere algoritmos especializados.\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "\n",
    "- Entender la diferencia entre acciones discretas y continuas\n",
    "- Comparar algoritmos: PPO vs SAC vs TD3\n",
    "- Analizar la importancia de normalización de observaciones\n",
    "- Explorar curriculum por complejidad de robot\n",
    "- Entender arquitecturas de red para control continuo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup y Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación (descomentar si es necesario)\n",
    "# !pip install gymnasium stable-baselines3 matplotlib\n",
    "# Para entornos MuJoCo: pip install gymnasium[mujoco]\n",
    "# Para PyBullet legacy: pip install pybullet gym==0.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# Intentar importar PyBullet\n",
    "try:\n",
    "    import pybullet_envs\n",
    "    PYBULLET_LEGACY = True\n",
    "    print(\"PyBullet legacy disponible\")\n",
    "except ImportError:\n",
    "    PYBULLET_LEGACY = False\n",
    "    print(\"Usando entornos de Gymnasium (MuJoCo backend)\")\n",
    "\n",
    "from stable_baselines3 import PPO, SAC, TD3, A2C\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análisis de la Arquitectura\n",
    "\n",
    "### 2.1 ¿Qué son las Acciones Continuas?\n",
    "\n",
    "En entornos discretos (CartPole, Atari), las acciones son índices:\n",
    "```python\n",
    "action = 0  # izquierda\n",
    "action = 1  # derecha\n",
    "```\n",
    "\n",
    "En control continuo, las acciones son **vectores de números reales**:\n",
    "```python\n",
    "action = [0.5, -0.3, 0.8, ...]  # Torques en cada articulación\n",
    "```\n",
    "\n",
    "```\n",
    "ACCIONES DISCRETAS              ACCIONES CONTINUAS\n",
    "┌─────────────────┐              ┌─────────────────────────┐\n",
    "│   Acción = 0    │              │  Acción = [0.5, -0.3]   │\n",
    "│   Acción = 1    │              │                         │\n",
    "│   Acción = 2    │              │  cadera: 0.5 Nm         │\n",
    "│       ...       │              │  rodilla: -0.3 Nm       │\n",
    "│   (finitas)     │              │  (infinitas posibles)   │\n",
    "└─────────────────┘              └─────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostrar la diferencia\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARACIÓN: DISCRETO vs CONTINUO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Entorno discreto\n",
    "env_discrete = gym.make(\"CartPole-v1\")\n",
    "print(f\"\\nCartPole (DISCRETO):\")\n",
    "print(f\"  Tipo de acción: {env_discrete.action_space}\")\n",
    "print(f\"  Acciones posibles: {env_discrete.action_space.n}\")\n",
    "print(f\"  Ejemplo: action = 0 o 1\")\n",
    "env_discrete.close()\n",
    "\n",
    "# Entorno continuo\n",
    "try:\n",
    "    env_continuous = gym.make(\"Hopper-v4\")\n",
    "    print(f\"\\nHopper (CONTINUO):\")\n",
    "    print(f\"  Tipo de acción: {env_continuous.action_space}\")\n",
    "    print(f\"  Dimensión: {env_continuous.action_space.shape}\")\n",
    "    print(f\"  Rango: [{env_continuous.action_space.low[0]:.1f}, {env_continuous.action_space.high[0]:.1f}]\")\n",
    "    print(f\"  Ejemplo: action = {env_continuous.action_space.sample()}\")\n",
    "    env_continuous.close()\n",
    "except:\n",
    "    print(\"\\nHopper no disponible. Instalar gymnasium[mujoco]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Robots Disponibles\n",
    "\n",
    "Organizados por complejidad (número de articulaciones/acciones):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBOTS = {\n",
    "    # Nivel 1: Simple\n",
    "    \"inverted_pendulum\": {\n",
    "        \"id\": \"InvertedPendulum-v4\",\n",
    "        \"desc\": \"Péndulo invertido - Equilibrar\",\n",
    "        \"acciones\": 1,\n",
    "        \"timesteps\": 100000,\n",
    "        \"dificultad\": \"⭐\"\n",
    "    },\n",
    "    \"inverted_double\": {\n",
    "        \"id\": \"InvertedDoublePendulum-v4\",\n",
    "        \"desc\": \"Doble péndulo - Muy inestable\",\n",
    "        \"acciones\": 1,\n",
    "        \"timesteps\": 200000,\n",
    "        \"dificultad\": \"⭐⭐\"\n",
    "    },\n",
    "    \n",
    "    # Nivel 2: Intermedio\n",
    "    \"hopper\": {\n",
    "        \"id\": \"Hopper-v4\",\n",
    "        \"desc\": \"Saltador de una pierna\",\n",
    "        \"acciones\": 3,\n",
    "        \"timesteps\": 300000,\n",
    "        \"dificultad\": \"⭐⭐\"\n",
    "    },\n",
    "    \"reacher\": {\n",
    "        \"id\": \"Reacher-v4\",\n",
    "        \"desc\": \"Brazo robótico - Alcanzar objetivo\",\n",
    "        \"acciones\": 2,\n",
    "        \"timesteps\": 200000,\n",
    "        \"dificultad\": \"⭐⭐\"\n",
    "    },\n",
    "    \n",
    "    # Nivel 3: Avanzado\n",
    "    \"halfcheetah\": {\n",
    "        \"id\": \"HalfCheetah-v4\",\n",
    "        \"desc\": \"Guepardo 2D - Correr rápido\",\n",
    "        \"acciones\": 6,\n",
    "        \"timesteps\": 500000,\n",
    "        \"dificultad\": \"⭐⭐⭐\"\n",
    "    },\n",
    "    \"walker\": {\n",
    "        \"id\": \"Walker2d-v4\",\n",
    "        \"desc\": \"Bípedo 2D - Caminar\",\n",
    "        \"acciones\": 6,\n",
    "        \"timesteps\": 500000,\n",
    "        \"dificultad\": \"⭐⭐⭐\"\n",
    "    },\n",
    "    \"ant\": {\n",
    "        \"id\": \"Ant-v4\",\n",
    "        \"desc\": \"Hormiga de 4 patas\",\n",
    "        \"acciones\": 8,\n",
    "        \"timesteps\": 500000,\n",
    "        \"dificultad\": \"⭐⭐⭐\"\n",
    "    },\n",
    "    \n",
    "    # Nivel 4: Experto\n",
    "    \"humanoid\": {\n",
    "        \"id\": \"Humanoid-v4\",\n",
    "        \"desc\": \"Humanoide 3D completo\",\n",
    "        \"acciones\": 17,\n",
    "        \"timesteps\": 2000000,\n",
    "        \"dificultad\": \"⭐⭐⭐⭐\"\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"ROBOTS DISPONIBLES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Robot':<20} {'Acc':<5} {'Dif':<8} {'Timesteps':<12} {'Descripción'}\")\n",
    "print(\"-\"*70)\n",
    "for name, info in ROBOTS.items():\n",
    "    print(f\"{name:<20} {info['acciones']:<5} {info['dificultad']:<8} {info['timesteps']:<12,} {info['desc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Observación (Estado del Robot)\n",
    "\n",
    "La observación incluye información propioceptiva del robot:\n",
    "\n",
    "| Componente | Descripción |\n",
    "|------------|-------------|\n",
    "| Posiciones | Ángulos de cada articulación |\n",
    "| Velocidades | Velocidad angular de articulaciones |\n",
    "| Posición del torso | Coordenadas x, y, z |\n",
    "| Orientación | Quaternion o ángulos de Euler |\n",
    "| Velocidad lineal | Del centro de masa |\n",
    "| Fuerzas de contacto | Con el suelo (opcional) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar espacio de observación de cada robot\n",
    "print(\"ESPACIOS DE OBSERVACIÓN Y ACCIÓN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, info in list(ROBOTS.items())[:5]:  # Solo los primeros 5\n",
    "    try:\n",
    "        env = gym.make(info['id'])\n",
    "        print(f\"\\n{name.upper()}:\")\n",
    "        print(f\"  Observación: {env.observation_space.shape}\")\n",
    "        print(f\"  Acción: {env.action_space.shape}\")\n",
    "        print(f\"  Rango acción: [{env.action_space.low[0]:.1f}, {env.action_space.high[0]:.1f}]\")\n",
    "        env.close()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{name}: No disponible ({str(e)[:40]}...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 ¿Por qué SAC para Acciones Continuas?\n",
    "\n",
    "| Algoritmo | Tipo | Política | Mejor Para |\n",
    "|-----------|------|----------|------------|\n",
    "| **DQN** | Off-policy | N/A (solo discreto) | Acciones discretas |\n",
    "| **PPO** | On-policy | Estocástica | General, estable |\n",
    "| **SAC** | Off-policy | Estocástica | Continuo, sample-efficient |\n",
    "| **TD3** | Off-policy | Determinista | Continuo, estable |\n",
    "\n",
    "**SAC (Soft Actor-Critic)** es preferido para robótica porque:\n",
    "1. **Sample efficient**: Reutiliza experiencias (off-policy)\n",
    "2. **Exploración automática**: Maximiza entropía\n",
    "3. **Robusto**: Menos sensible a hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar arquitectura de red típica\n",
    "print(\"ARQUITECTURA DE RED PARA CONTROL CONTINUO\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "PPO (Actor-Critic):\n",
    "┌─────────────────────────────────────────────┐\n",
    "│  Observación (ej: 17D para Hopper)          │\n",
    "│            ↓                                │\n",
    "│  ┌─────────────────┐  ┌─────────────────┐   │\n",
    "│  │ Actor (Policy)  │  │ Critic (Value)  │   │\n",
    "│  │ 17→256→256→3    │  │ 17→256→256→1    │   │\n",
    "│  │ (media, std)    │  │ (V(s))          │   │\n",
    "│  └─────────────────┘  └─────────────────┘   │\n",
    "│            ↓                   ↓            │\n",
    "│  Normal(media, std)      Valor del estado   │\n",
    "└─────────────────────────────────────────────┘\n",
    "\n",
    "SAC (2 Críticos + Actor):\n",
    "┌─────────────────────────────────────────────┐\n",
    "│  ┌─────────────┐  ┌────────┐  ┌────────┐    │\n",
    "│  │   Actor     │  │ Critic1│  │ Critic2│    │\n",
    "│  │ s→μ,σ→a     │  │ s,a→Q1 │  │ s,a→Q2 │    │\n",
    "│  └─────────────┘  └────────┘  └────────┘    │\n",
    "│        ↓              ↓           ↓         │\n",
    "│  Acción muestreada   min(Q1, Q2) + entropy  │\n",
    "└─────────────────────────────────────────────┘\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nHiperparámetros típicos:\")\n",
    "print(\"-\"*40)\n",
    "hiperparams = [\n",
    "    (\"net_arch\", \"[256, 256]\", \"Capas ocultas\"),\n",
    "    (\"learning_rate\", \"3e-4\", \"Tasa de aprendizaje\"),\n",
    "    (\"buffer_size\", \"1,000,000\", \"Replay buffer (SAC/TD3)\"),\n",
    "    (\"batch_size\", \"256\", \"Tamaño de batch\"),\n",
    "    (\"gamma\", \"0.99\", \"Factor de descuento\"),\n",
    "    (\"tau\", \"0.005\", \"Soft update (SAC/TD3)\"),\n",
    "]\n",
    "for param, valor, desc in hiperparams:\n",
    "    print(f\"  {param:<15} {valor:<12} {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Código Base para Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoboticsCallback(BaseCallback):\n",
    "    \"\"\"Callback para registrar métricas de robots.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.best_reward = -np.inf\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        for info in self.locals.get('infos', []):\n",
    "            if 'episode' in info:\n",
    "                reward = info['episode']['r']\n",
    "                self.episode_rewards.append(reward)\n",
    "                self.episode_lengths.append(info['episode']['l'])\n",
    "                if reward > self.best_reward:\n",
    "                    self.best_reward = reward\n",
    "        return True\n",
    "\n",
    "\n",
    "def crear_entorno(robot_name=\"hopper\", render=False):\n",
    "    \"\"\"Crea un entorno de robótica.\"\"\"\n",
    "    info = ROBOTS.get(robot_name, ROBOTS[\"hopper\"])\n",
    "    render_mode = \"human\" if render else None\n",
    "    \n",
    "    try:\n",
    "        env = gym.make(info[\"id\"], render_mode=render_mode)\n",
    "    except:\n",
    "        print(f\"Entorno {info['id']} no disponible\")\n",
    "        env = gym.make(\"InvertedPendulum-v4\", render_mode=render_mode)\n",
    "    \n",
    "    return env\n",
    "\n",
    "\n",
    "def plot_training(callbacks, labels, title=\"Entrenamiento\"):\n",
    "    \"\"\"Grafica múltiples entrenamientos.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(callbacks)))\n",
    "    \n",
    "    for callback, label, color in zip(callbacks, labels, colors):\n",
    "        if not callback.episode_rewards:\n",
    "            continue\n",
    "        \n",
    "        rewards = callback.episode_rewards\n",
    "        lengths = callback.episode_lengths\n",
    "        window = min(50, len(rewards) // 4) if len(rewards) > 4 else 1\n",
    "        \n",
    "        # Recompensas\n",
    "        axes[0].plot(rewards, alpha=0.2, color=color)\n",
    "        if window > 1:\n",
    "            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            axes[0].plot(range(window-1, len(rewards)), smoothed, color=color, label=label, linewidth=2)\n",
    "        \n",
    "        # Longitud\n",
    "        axes[1].plot(lengths, alpha=0.2, color=color)\n",
    "        if window > 1:\n",
    "            smoothed = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "            axes[1].plot(range(window-1, len(lengths)), smoothed, color=color, label=label, linewidth=2)\n",
    "    \n",
    "    axes[0].set_xlabel('Episodio')\n",
    "    axes[0].set_ylabel('Recompensa')\n",
    "    axes[0].set_title('Recompensa por Episodio')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_xlabel('Episodio')\n",
    "    axes[1].set_ylabel('Pasos')\n",
    "    axes[1].set_title('Longitud de Episodio')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Funciones cargadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar en InvertedPendulum (el más simple)\n",
    "print(\"=\"*60)\n",
    "print(\"  ENTRENAMIENTO DEMO - InvertedPendulum\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = crear_entorno(\"inverted_pendulum\")\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    policy_kwargs={\"net_arch\": [dict(pi=[64, 64], vf=[64, 64])]},\n",
    ")\n",
    "\n",
    "callback = RoboticsCallback()\n",
    "\n",
    "TIMESTEPS = 30000  # Reducido para demo\n",
    "model.learn(total_timesteps=TIMESTEPS, callback=callback, progress_bar=True)\n",
    "\n",
    "print(f\"\\nEpisodios completados: {len(callback.episode_rewards)}\")\n",
    "if callback.episode_rewards:\n",
    "    print(f\"Recompensa promedio (últimos 20): {np.mean(callback.episode_rewards[-20:]):.1f}\")\n",
    "    print(f\"Mejor recompensa: {callback.best_reward:.1f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar entrenamiento\n",
    "plot_training([callback], [\"PPO\"], \"Entrenamiento Demo - InvertedPendulum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Variantes Implementadas\n",
    "\n",
    "### Variante A: PPO vs SAC vs TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_algoritmos(robot_name=\"hopper\", timesteps=50000):\n",
    "    \"\"\"\n",
    "    Compara PPO, SAC y TD3 en el mismo robot.\n",
    "    \n",
    "    PPO: On-policy, estable, menos sample-efficient\n",
    "    SAC: Off-policy, estocástico, maximiza entropía\n",
    "    TD3: Off-policy, determinista, twin critics\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  VARIANTE A: PPO vs SAC vs TD3 en {robot_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resultados = {}\n",
    "    algoritmos = [\n",
    "        (\"PPO\", PPO, {\n",
    "            \"learning_rate\": 3e-4,\n",
    "            \"n_steps\": 2048,\n",
    "            \"batch_size\": 64,\n",
    "            \"policy_kwargs\": {\"net_arch\": [dict(pi=[256, 256], vf=[256, 256])]},\n",
    "        }),\n",
    "        (\"SAC\", SAC, {\n",
    "            \"learning_rate\": 3e-4,\n",
    "            \"buffer_size\": 100000,\n",
    "            \"learning_starts\": 1000,\n",
    "            \"batch_size\": 256,\n",
    "            \"policy_kwargs\": {\"net_arch\": [256, 256]},\n",
    "        }),\n",
    "        (\"TD3\", TD3, {\n",
    "            \"learning_rate\": 3e-4,\n",
    "            \"buffer_size\": 100000,\n",
    "            \"learning_starts\": 1000,\n",
    "            \"batch_size\": 256,\n",
    "            \"policy_kwargs\": {\"net_arch\": [256, 256]},\n",
    "        }),\n",
    "    ]\n",
    "    \n",
    "    for algo_name, AlgoClass, kwargs in algoritmos:\n",
    "        print(f\"\\n[{algo_name}] Entrenando...\")\n",
    "        \n",
    "        env = crear_entorno(robot_name)\n",
    "        \n",
    "        try:\n",
    "            model = AlgoClass(\"MlpPolicy\", env, verbose=0, **kwargs)\n",
    "            callback = RoboticsCallback()\n",
    "            model.learn(total_timesteps=timesteps, callback=callback, progress_bar=True)\n",
    "            \n",
    "            resultados[algo_name] = {\n",
    "                \"callback\": callback,\n",
    "                \"best\": callback.best_reward,\n",
    "                \"mean_last\": np.mean(callback.episode_rewards[-20:]) if len(callback.episode_rewards) >= 20 else np.mean(callback.episode_rewards)\n",
    "            }\n",
    "            print(f\"  Mejor: {callback.best_reward:.1f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            resultados[algo_name] = {\"error\": str(e)}\n",
    "        \n",
    "        env.close()\n",
    "    \n",
    "    # Tabla de resultados\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Algoritmo':<10} {'Mejor':<12} {'Media (últ 20)'}\")\n",
    "    print(\"-\"*40)\n",
    "    for algo, data in resultados.items():\n",
    "        if \"error\" not in data:\n",
    "            print(f\"{algo:<10} {data['best']:<12.1f} {data['mean_last']:.1f}\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Usar un robot simple para demo rápida\n",
    "resultados_algos = comparar_algoritmos(\"inverted_pendulum\", timesteps=30000)\n",
    "\n",
    "# Graficar\n",
    "callbacks = [r[\"callback\"] for r in resultados_algos.values() if \"callback\" in r]\n",
    "labels = [k for k, v in resultados_algos.items() if \"callback\" in v]\n",
    "plot_training(callbacks, labels, \"Variante A: Comparación de Algoritmos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variante B: Normalización de Observaciones\n",
    "\n",
    "VecNormalize escala observaciones y recompensas para estabilizar entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_normalizacion(robot_name=\"hopper\", timesteps=50000):\n",
    "    \"\"\"\n",
    "    Compara entrenamiento con y sin normalización de observaciones.\n",
    "    \n",
    "    VecNormalize:\n",
    "    - Normaliza observaciones a media 0, std 1\n",
    "    - Opcionalmente normaliza recompensas\n",
    "    - Mejora estabilidad del entrenamiento\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  VARIANTE B: CON vs SIN NORMALIZACIÓN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    # Sin normalización\n",
    "    print(\"\\n[1/2] Sin normalización...\")\n",
    "    env_raw = DummyVecEnv([lambda: crear_entorno(robot_name)])\n",
    "    \n",
    "    model_raw = PPO(\"MlpPolicy\", env_raw, verbose=0)\n",
    "    callback_raw = RoboticsCallback()\n",
    "    model_raw.learn(total_timesteps=timesteps, callback=callback_raw, progress_bar=True)\n",
    "    resultados[\"Sin Normalizar\"] = callback_raw\n",
    "    env_raw.close()\n",
    "    \n",
    "    # Con normalización\n",
    "    print(\"\\n[2/2] Con VecNormalize...\")\n",
    "    env_norm = DummyVecEnv([lambda: crear_entorno(robot_name)])\n",
    "    env_norm = VecNormalize(env_norm, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "    \n",
    "    model_norm = PPO(\"MlpPolicy\", env_norm, verbose=0)\n",
    "    callback_norm = RoboticsCallback()\n",
    "    model_norm.learn(total_timesteps=timesteps, callback=callback_norm, progress_bar=True)\n",
    "    resultados[\"VecNormalize\"] = callback_norm\n",
    "    env_norm.close()\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Ejecutar comparación\n",
    "resultados_norm = comparar_normalizacion(\"inverted_pendulum\", timesteps=30000)\n",
    "\n",
    "# Graficar\n",
    "plot_training(\n",
    "    list(resultados_norm.values()),\n",
    "    list(resultados_norm.keys()),\n",
    "    \"Variante B: Efecto de VecNormalize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variante C: Curriculum por Complejidad de Robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curriculum_robots(timesteps_per_robot=30000):\n",
    "    \"\"\"\n",
    "    Curriculum Learning por complejidad de robot.\n",
    "    \n",
    "    Secuencia: InvertedPendulum → Hopper → HalfCheetah\n",
    "    \n",
    "    NOTA: Transferir entre robots diferentes es difícil porque\n",
    "    los espacios de observación/acción cambian. Aquí demostramos\n",
    "    el concepto entrenando secuencialmente.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"  VARIANTE C: CURRICULUM POR ROBOT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Robots en orden de complejidad\n",
    "    curriculum = [\n",
    "        (\"inverted_pendulum\", \"Nivel 1: InvertedPendulum (1 acción)\"),\n",
    "        (\"inverted_double\", \"Nivel 2: DoublePendulum (1 acción, inestable)\"),\n",
    "    ]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for robot_name, desc in curriculum:\n",
    "        print(f\"\\n{desc}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            env = crear_entorno(robot_name)\n",
    "            \n",
    "            model = PPO(\n",
    "                \"MlpPolicy\", env, verbose=0,\n",
    "                learning_rate=3e-4,\n",
    "                policy_kwargs={\"net_arch\": [dict(pi=[64, 64], vf=[64, 64])]},\n",
    "            )\n",
    "            \n",
    "            callback = RoboticsCallback()\n",
    "            model.learn(total_timesteps=timesteps_per_robot, callback=callback, progress_bar=True)\n",
    "            \n",
    "            all_results.append((robot_name, callback))\n",
    "            print(f\"  → Mejor: {callback.best_reward:.1f}\")\n",
    "            \n",
    "            env.close()\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Ejecutar curriculum\n",
    "resultados_curriculum = curriculum_robots(timesteps_per_robot=25000)\n",
    "\n",
    "# Graficar\n",
    "if resultados_curriculum:\n",
    "    callbacks = [r[1] for r in resultados_curriculum]\n",
    "    labels = [r[0] for r in resultados_curriculum]\n",
    "    plot_training(callbacks, labels, \"Variante C: Curriculum por Robot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variante D: Arquitectura de Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_arquitecturas(robot_name=\"inverted_pendulum\", timesteps=30000):\n",
    "    \"\"\"\n",
    "    Compara diferentes arquitecturas de red.\n",
    "    \n",
    "    Opciones:\n",
    "    - Pequeña: [64, 64] - Rápida, menos capacidad\n",
    "    - Mediana: [256, 256] - Balance estándar\n",
    "    - Grande: [400, 300] - Más capacidad, más lenta\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  VARIANTE D: ARQUITECTURA DE RED\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    arquitecturas = [\n",
    "        (\"[64,64]\", [64, 64]),\n",
    "        (\"[256,256]\", [256, 256]),\n",
    "        (\"[400,300]\", [400, 300]),\n",
    "    ]\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    for nombre, arch in arquitecturas:\n",
    "        print(f\"\\nEntrenando con net_arch={nombre}...\")\n",
    "        \n",
    "        env = crear_entorno(robot_name)\n",
    "        \n",
    "        model = PPO(\n",
    "            \"MlpPolicy\", env, verbose=0,\n",
    "            policy_kwargs={\"net_arch\": [dict(pi=arch, vf=arch)]},\n",
    "        )\n",
    "        \n",
    "        callback = RoboticsCallback()\n",
    "        start = time.time()\n",
    "        model.learn(total_timesteps=timesteps, callback=callback, progress_bar=True)\n",
    "        train_time = time.time() - start\n",
    "        \n",
    "        # Contar parámetros\n",
    "        total_params = sum(p.numel() for p in model.policy.parameters())\n",
    "        \n",
    "        resultados[nombre] = {\n",
    "            \"callback\": callback,\n",
    "            \"params\": total_params,\n",
    "            \"time\": train_time,\n",
    "            \"best\": callback.best_reward,\n",
    "        }\n",
    "        \n",
    "        print(f\"  Parámetros: {total_params:,}\")\n",
    "        print(f\"  Tiempo: {train_time:.1f}s\")\n",
    "        print(f\"  Mejor: {callback.best_reward:.1f}\")\n",
    "        \n",
    "        env.close()\n",
    "    \n",
    "    # Tabla comparativa\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESUMEN\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Arquitectura':<15} {'Parámetros':<12} {'Tiempo':<10} {'Mejor'}\")\n",
    "    print(\"-\"*50)\n",
    "    for nombre, data in resultados.items():\n",
    "        print(f\"{nombre:<15} {data['params']:<12,} {data['time']:<10.1f} {data['best']:.1f}\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Ejecutar comparación\n",
    "resultados_arch = comparar_arquitecturas(\"inverted_pendulum\", timesteps=25000)\n",
    "\n",
    "# Graficar\n",
    "callbacks = [r[\"callback\"] for r in resultados_arch.values()]\n",
    "labels = list(resultados_arch.keys())\n",
    "plot_training(callbacks, labels, \"Variante D: Arquitectura de Red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variante E: Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_learning_rates(robot_name=\"inverted_pendulum\", timesteps=30000):\n",
    "    \"\"\"\n",
    "    Compara diferentes learning rates.\n",
    "    \n",
    "    LR muy alto: Inestable, puede divergir\n",
    "    LR muy bajo: Convergencia lenta\n",
    "    LR óptimo: ~3e-4 para la mayoría de casos\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  VARIANTE E: LEARNING RATE STUDY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    learning_rates = [1e-4, 3e-4, 1e-3, 3e-3]\n",
    "    resultados = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nEntrenando con lr={lr}...\")\n",
    "        \n",
    "        env = crear_entorno(robot_name)\n",
    "        \n",
    "        model = PPO(\"MlpPolicy\", env, verbose=0, learning_rate=lr)\n",
    "        callback = RoboticsCallback()\n",
    "        model.learn(total_timesteps=timesteps, callback=callback, progress_bar=True)\n",
    "        \n",
    "        resultados[f\"lr={lr}\"] = callback\n",
    "        print(f\"  Mejor: {callback.best_reward:.1f}\")\n",
    "        \n",
    "        env.close()\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Ejecutar\n",
    "resultados_lr = comparar_learning_rates(\"inverted_pendulum\", timesteps=25000)\n",
    "\n",
    "# Graficar\n",
    "plot_training(\n",
    "    list(resultados_lr.values()),\n",
    "    list(resultados_lr.keys()),\n",
    "    \"Variante E: Efecto del Learning Rate\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluación Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_robot(model, robot_name=\"inverted_pendulum\", n_episodios=5):\n",
    "    \"\"\"Evalúa un modelo entrenado.\"\"\"\n",
    "    env = crear_entorno(robot_name)\n",
    "    \n",
    "    print(f\"\\nEvaluando en {robot_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    rewards = []\n",
    "    lengths = []\n",
    "    \n",
    "    for ep in range(n_episodios):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, term, trunc, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            done = term or trunc\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        lengths.append(steps)\n",
    "        print(f\"  Episodio {ep+1}: {total_reward:.1f} puntos ({steps} pasos)\")\n",
    "    \n",
    "    print(f\"\\nPromedio: {np.mean(rewards):.1f} ± {np.std(rewards):.1f}\")\n",
    "    print(f\"Mejor: {max(rewards):.1f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return rewards, lengths\n",
    "\n",
    "# Evaluar el modelo del demo\n",
    "evaluar_robot(model, \"inverted_pendulum\", n_episodios=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resumen Comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla resumen de todos los experimentos\n",
    "print(\"=\"*70)\n",
    "print(\"RESUMEN DE EXPERIMENTOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                    GUÍA DE SELECCIÓN DE ALGORITMO                  │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│  ¿Acciones discretas?                                              │\n",
    "│       SÍ → DQN, PPO                                                │\n",
    "│       NO (continuas) → SAC, TD3, PPO                               │\n",
    "│                                                                    │\n",
    "│  ¿Prioridad sample efficiency?                                     │\n",
    "│       SÍ → SAC, TD3 (off-policy, reutilizan datos)                 │\n",
    "│       NO → PPO (on-policy, más estable)                            │\n",
    "│                                                                    │\n",
    "│  ¿Necesitas exploración automática?                                │\n",
    "│       SÍ → SAC (maximiza entropía)                                 │\n",
    "│       NO → TD3 (determinista)                                      │\n",
    "│                                                                    │\n",
    "│  ¿Robot muy complejo (Humanoid)?                                   │\n",
    "│       → SAC con VecNormalize                                       │\n",
    "│       → Red grande [400, 300]                                      │\n",
    "│       → 2M+ timesteps                                              │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "HIPERPARÁMETROS RECOMENDADOS POR ROBOT:\n",
    "\n",
    "┌──────────────────┬───────────┬─────────┬───────────┬─────────────┐\n",
    "│ Robot            │ Algoritmo │ LR      │ Net Arch  │ Timesteps   │\n",
    "├──────────────────┼───────────┼─────────┼───────────┼─────────────┤\n",
    "│ InvertedPendulum │ PPO       │ 3e-4    │ [64,64]   │ 100K        │\n",
    "│ Hopper           │ SAC       │ 3e-4    │ [256,256] │ 300K        │\n",
    "│ HalfCheetah      │ SAC       │ 3e-4    │ [256,256] │ 500K        │\n",
    "│ Walker2d         │ SAC       │ 3e-4    │ [256,256] │ 500K        │\n",
    "│ Ant              │ SAC       │ 3e-4    │ [256,256] │ 500K        │\n",
    "│ Humanoid         │ SAC       │ 3e-4    │ [400,300] │ 2M+         │\n",
    "└──────────────────┴───────────┴─────────┴───────────┴─────────────┘\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusiones\n",
    "\n",
    "### Preguntas Respondidas\n",
    "\n",
    "**¿Por qué SAC para acciones continuas?**\n",
    "- SAC es **off-policy** (reutiliza experiencias, más eficiente)\n",
    "- Maximiza **entropía** (exploración automática)\n",
    "- Usa **twin critics** (más estable)\n",
    "- Funciona con acciones de cualquier dimensión\n",
    "\n",
    "**¿Diferencia entre PPO, SAC y TD3?**\n",
    "\n",
    "| Aspecto | PPO | SAC | TD3 |\n",
    "|---------|-----|-----|-----|\n",
    "| Tipo | On-policy | Off-policy | Off-policy |\n",
    "| Política | Estocástica | Estocástica | Determinista |\n",
    "| Sample efficiency | Baja | Alta | Alta |\n",
    "| Exploración | Entropia fija | Automática | Ruido añadido |\n",
    "| Estabilidad | Alta | Media | Alta |\n",
    "\n",
    "**¿Cómo escalar a Humanoid?**\n",
    "1. Usar SAC (mejor para alta dimensión)\n",
    "2. Normalizar observaciones con VecNormalize\n",
    "3. Red más grande [400, 300]\n",
    "4. Entrenar 2M+ timesteps\n",
    "5. Paciencia - robots complejos aprenden lento\n",
    "\n",
    "### Lecciones Aprendidas\n",
    "\n",
    "| Concepto | Hallazgo |\n",
    "|----------|----------|\n",
    "| VecNormalize | Mejora estabilidad significativamente |\n",
    "| Arquitectura | [256, 256] es buen balance general |\n",
    "| Learning Rate | 3e-4 funciona bien para la mayoría |\n",
    "| Algoritmo | SAC > TD3 > PPO para robótica |\n",
    "\n",
    "### Siguientes Pasos\n",
    "\n",
    "1. Probar robots más complejos (Walker, Ant, Humanoid)\n",
    "2. Implementar curriculum automático\n",
    "3. Explorar Domain Randomization para sim2real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Referencias\n",
    "\n",
    "- [SAC Paper](https://arxiv.org/abs/1801.01290) - Soft Actor-Critic\n",
    "- [TD3 Paper](https://arxiv.org/abs/1802.09477) - Twin Delayed DDPG\n",
    "- [PPO Paper](https://arxiv.org/abs/1707.06347) - Proximal Policy Optimization\n",
    "- [Stable-Baselines3 Docs](https://stable-baselines3.readthedocs.io/)\n",
    "- [MuJoCo Documentation](https://mujoco.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Variantes de Entrenamiento — PyBullet / MuJoCo\n\nLas variantes en robótica exploran dos dimensiones: **qué algoritmo** y **qué robot**.\nLos 8 robots tienen complejidades muy distintas (de péndulo a humanoide), y los 3 algoritmos tienen fortalezas diferentes en control continuo.\n\n| Variante | Estrategia | Algoritmos | Robots |\n|----------|-----------|------------|--------|\n| A | PPO por robot | PPO | Cualquiera |\n| B | SAC por robot | SAC | Cualquiera |\n| C | TD3 por robot | TD3 | Cualquiera |\n| D | Matriz comparativa | PPO+SAC+TD3 | pendulum, hopper, cheetah, ant |\n\n**Todos los robots usan control continuo**: las acciones son vectores de fuerzas/torques aplicados a las articulaciones, no acciones discretas.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Variantes A/B/C — Un Algoritmo, Un Robot\n\n```python\npython pybullet_robotica.py --algorithm PPO --env ant       # Var. A\npython pybullet_robotica.py --algorithm SAC --env hopper    # Var. B\npython pybullet_robotica.py --algorithm TD3 --env halfcheetah  # Var. C\n```\n\nLos tres algoritmos resuelven control continuo pero con filosofías distintas:\n\n**PPO** — Variante A\n- On-policy, política estocástica (Gaussiana)\n- Robusto, general, fácil de ajustar\n- Recomendado como primer algoritmo a probar\n\n**SAC (Soft Actor-Critic)** — Variante B\n- Off-policy con buffer de experiencias\n- Maximiza recompensa *y* entropía: `J(π) = E[r] + α·H(π)`\n- Muy explorador → excelente en espacios de estado complejos\n- Entropía adaptativa: α se ajusta automáticamente\n\n**TD3 (Twin Delayed DDPG)** — Variante C\n- Off-policy, política determinista + ruido gaussiano para explorar\n- Dos Q-networks: `Q = min(Q1, Q2)` → reduce sobreestimación\n- Actor actualizado cada 2 pasos del crítico (delayed) → estabilidad\n- Más estable que SAC, menos explorador",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variantes A/B/C: un algoritmo en un robot\n# from pybullet_robotica import entrenar_robot, comparar_algoritmos\n\n# Entrenar un robot con un algoritmo:\n# model, callback = entrenar_robot(env_name=\"ant\", timesteps=100000, algorithm=\"SAC\")\n\n# Comparar PPO vs SAC vs TD3 en el mismo robot:\n# resultados = comparar_algoritmos(env_name=\"hopper\", timesteps=50000)\n\nprint(\"Robots disponibles (de menos a más complejo):\")\nrobots = {\n    \"inverted_pendulum\": \"Péndulo invertido — equilibrio simple\",\n    \"inverted_double\":   \"Doble péndulo — muy inestable\",\n    \"reacher\":           \"Brazo robótico — alcanzar objetivo\",\n    \"hopper\":            \"Saltador de una pierna\",\n    \"walker\":            \"Bípedo 2D — caminar\",\n    \"halfcheetah\":       \"Guepardo 2D — correr rápido\",\n    \"ant\":               \"Hormiga 4 patas — locomoción compleja\",\n    \"humanoid\":          \"Humanoide 3D — el más difícil (2M steps)\",\n}\nfor robot, desc in robots.items():\n    print(f\"  {robot:<22}: {desc}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Comparativa técnica: PPO vs SAC vs TD3\n\n```\n                    PPO          SAC          TD3\n────────────────────────────────────────────────────────\nTipo:           On-policy    Off-policy   Off-policy\nBuffer:         No           Sí (1M)      Sí (1M)\nPolítica:       Estocástica  Estocástica  Determinista+ruido\nCrítico:        1 V-network  2 Q-networks 2 Q-networks\nActor update:   Cada batch   Cada step    Cada 2 steps (delay)\nEntropía:       Fija         Adaptativa   N/A\n────────────────────────────────────────────────────────\nMejor en:       General      Complejo     Con ruido\nSample eff.:    Baja         Alta         Alta\n────────────────────────────────────────────────────────\n```\n\n**Guía de selección**:\n- Robot simple (péndulo, hopper) → **PPO** o **TD3**\n- Robot complejo (ant, humanoid) → **SAC** (más exploración)\n- Entorno con mucho ruido → **TD3** (más estable)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Variante D — Matriz Comparativa Algoritmo × Robot\n\n```python\npython pybullet_robotica.py --compare-matrix\n```\n\nEntrena las 12 combinaciones (3 algoritmos × 4 robots) y genera una tabla comparativa de recompensas finales.\n\n**Robots evaluados** (de menor a mayor complejidad):\n1. `inverted_pendulum` — equilibrio simple\n2. `hopper` — salto unipodal\n3. `halfcheetah` — carrera 2D\n4. `ant` — locomoción 4 patas\n\n**Resultado esperado**: la tabla muestra que:\n- PPO es competitivo en entornos simples pero queda atrás en complejos\n- SAC y TD3 escalan mejor con la complejidad del robot\n- TD3 suele ser más estable (menor varianza entre ejecuciones)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante D: Matriz comparativa Algoritmo × Robot\n# from pybullet_robotica import comparar_algoritmos_por_robot\n# resultados = comparar_algoritmos_por_robot(timesteps=50000)\n# Genera: robot_matriz_comparacion.png\n\nprint(\"Variante D: Matriz Algoritmo × Robot\")\nprint(\"  Algoritmos: PPO, SAC, TD3\")\nprint(\"  Robots: inverted_pendulum, hopper, halfcheetah, ant\")\nprint(\"  Total: 12 combinaciones\")\nprint()\nprint(\"Salida:\")\nprint(\"  - Grid 4×3 de curvas de aprendizaje\")\nprint(\"  - Tabla de recompensa media final por combinación\")\nprint()\nprint(\"Tabla de resultados esperada:\")\ntabla = \"\"\"\nRobot                   PPO          SAC          TD3\n─────────────────────────────────────────────────────\ninverted_pendulum       1000.0       980.0        990.0\nhopper                   800.0      1200.0       1100.0\nhalfcheetah             1500.0      3000.0       2800.0\nant                      500.0      2000.0       1800.0\n─────────────────────────────────────────────────────\n(valores aproximados, dependen de timesteps y semilla)\n\"\"\"\nprint(tabla)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Comparativa Final\n\n| Robot | Complejidad | Espacio estado | Espacio acción | Algo recomendado |\n|-------|-------------|----------------|----------------|-----------------|\n| inverted_pendulum | Muy baja | 5D | 1D | PPO o TD3 |\n| hopper | Baja | 15D | 3D | PPO o SAC |\n| halfcheetah | Media | 26D | 6D | SAC o TD3 |\n| ant | Alta | 111D | 8D | SAC |\n| humanoid | Muy alta | 376D | 17D | SAC |\n\n**Lección principal**: en robótica y control continuo, SAC y TD3 dominan sobre PPO en entornos complejos. La mayor sample efficiency de los algoritmos off-policy es crucial cuando cada timestep de simulación es costoso computacionalmente.\n\n**Tip**: para el humanoid, SAC con 2M steps puede resolver el entorno. Con PPO necesitarías mucho más.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}