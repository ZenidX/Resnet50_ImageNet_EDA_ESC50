{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nibbler (Snake) con Reinforcement Learning\n",
    "\n",
    "En este notebook crearemos un entorno del cl√°sico juego **Nibbler/Snake** y entrenaremos un agente de RL para jugarlo.\n",
    "\n",
    "## Contenido\n",
    "1. [Crear el Entorno](#1-entorno)\n",
    "2. [Visualizaci√≥n del Juego](#2-visualizacion)\n",
    "3. [Agente DQN](#3-agente)\n",
    "4. [Entrenamiento](#4-entrenamiento)\n",
    "5. [Evaluaci√≥n y Demo](#5-evaluacion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import random\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Gymnasium\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    from gymnasium import spaces\n",
    "except ImportError:\n",
    "    !pip install gymnasium -q\n",
    "    import gymnasium as gym\n",
    "    from gymnasium import spaces\n",
    "\n",
    "# PyTorch para DQN\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(f\"‚úÖ PyTorch disponible (device: {'cuda' if torch.cuda.is_available() else 'cpu'})\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è PyTorch no disponible. Instala con: pip install torch\")\n",
    "\n",
    "%matplotlib inline\n",
    "print(\"‚úÖ Imports cargados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='1-entorno'></a>\n",
    "# 1. Crear el Entorno Nibbler\n",
    "\n",
    "Vamos a crear un entorno compatible con Gymnasium para el juego de la serpiente.\n",
    "\n",
    "## Reglas del Juego\n",
    "- La serpiente se mueve en una cuadr√≠cula\n",
    "- Puede ir: ‚Üë Arriba, ‚Üí Derecha, ‚Üì Abajo, ‚Üê Izquierda\n",
    "- Come comida (manzanas) para crecer y ganar puntos\n",
    "- Muere si choca con las paredes o consigo misma\n",
    "- Objetivo: Sobrevivir el mayor tiempo posible y comer mucho\n",
    "\n",
    "## Representaci√≥n\n",
    "- `0` = Celda vac√≠a\n",
    "- `1` = Cuerpo de la serpiente\n",
    "- `2` = Cabeza de la serpiente\n",
    "- `3` = Comida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NibblerEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Entorno del juego Nibbler (Snake) compatible con Gymnasium.\n",
    "    \n",
    "    Observaci√≥n: Matriz grid_size x grid_size con valores:\n",
    "        0 = vac√≠o, 1 = cuerpo, 2 = cabeza, 3 = comida\n",
    "    \n",
    "    Acciones: 0=arriba, 1=derecha, 2=abajo, 3=izquierda\n",
    "    \n",
    "    Recompensas:\n",
    "        +10 por comer comida\n",
    "        -10 por morir\n",
    "        +0.1 por acercarse a la comida\n",
    "        -0.1 por alejarse de la comida\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {'render_modes': ['human', 'rgb_array']}\n",
    "    \n",
    "    def __init__(self, grid_size=10, render_mode=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        # Espacio de acciones: 4 direcciones\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # Espacio de observaci√≥n: grid con valores 0-3\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=3,\n",
    "            shape=(grid_size, grid_size),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        \n",
    "        # Direcciones: arriba, derecha, abajo, izquierda\n",
    "        self.directions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        self.direction_names = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']\n",
    "        \n",
    "        # Estado del juego\n",
    "        self.snake = None\n",
    "        self.food = None\n",
    "        self.direction = None\n",
    "        self.score = 0\n",
    "        self.steps = 0\n",
    "        self.max_steps = grid_size * grid_size * 2  # L√≠mite de pasos\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Inicializar serpiente en el centro\n",
    "        center = self.grid_size // 2\n",
    "        self.snake = deque([(center, center)])\n",
    "        self.direction = 1  # Empezar yendo a la derecha\n",
    "        self.score = 0\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Colocar comida\n",
    "        self._place_food()\n",
    "        \n",
    "        return self._get_observation(), {}\n",
    "    \n",
    "    def _place_food(self):\n",
    "        \"\"\"Coloca la comida en una posici√≥n aleatoria libre.\"\"\"\n",
    "        empty_cells = []\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                if (i, j) not in self.snake:\n",
    "                    empty_cells.append((i, j))\n",
    "        \n",
    "        if empty_cells:\n",
    "            self.food = random.choice(empty_cells)\n",
    "        else:\n",
    "            self.food = None  # Serpiente llena todo el tablero (victoria)\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Devuelve el estado del tablero como matriz.\"\"\"\n",
    "        grid = np.zeros((self.grid_size, self.grid_size), dtype=np.uint8)\n",
    "        \n",
    "        # Cuerpo de la serpiente\n",
    "        for segment in self.snake:\n",
    "            grid[segment[0], segment[1]] = 1\n",
    "        \n",
    "        # Cabeza\n",
    "        head = self.snake[0]\n",
    "        grid[head[0], head[1]] = 2\n",
    "        \n",
    "        # Comida\n",
    "        if self.food:\n",
    "            grid[self.food[0], self.food[1]] = 3\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "    def _distance_to_food(self, pos):\n",
    "        \"\"\"Distancia Manhattan a la comida.\"\"\"\n",
    "        if self.food is None:\n",
    "            return 0\n",
    "        return abs(pos[0] - self.food[0]) + abs(pos[1] - self.food[1])\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Evitar que la serpiente vaya en direcci√≥n opuesta (muerte instant√°nea)\n",
    "        opposite = (self.direction + 2) % 4\n",
    "        if action == opposite and len(self.snake) > 1:\n",
    "            action = self.direction  # Mantener direcci√≥n actual\n",
    "        \n",
    "        self.direction = action\n",
    "        \n",
    "        # Calcular nueva posici√≥n de la cabeza\n",
    "        head = self.snake[0]\n",
    "        delta = self.directions[action]\n",
    "        new_head = (head[0] + delta[0], head[1] + delta[1])\n",
    "        \n",
    "        # Distancia antes del movimiento\n",
    "        dist_before = self._distance_to_food(head)\n",
    "        \n",
    "        # Verificar colisiones\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0\n",
    "        \n",
    "        # Colisi√≥n con paredes\n",
    "        if (new_head[0] < 0 or new_head[0] >= self.grid_size or\n",
    "            new_head[1] < 0 or new_head[1] >= self.grid_size):\n",
    "            terminated = True\n",
    "            reward = -10\n",
    "        \n",
    "        # Colisi√≥n consigo misma\n",
    "        elif new_head in self.snake:\n",
    "            terminated = True\n",
    "            reward = -10\n",
    "        \n",
    "        else:\n",
    "            # Mover la serpiente\n",
    "            self.snake.appendleft(new_head)\n",
    "            \n",
    "            # ¬øComi√≥ comida?\n",
    "            if new_head == self.food:\n",
    "                self.score += 1\n",
    "                reward = 10\n",
    "                self._place_food()\n",
    "                \n",
    "                # Victoria: serpiente llena todo\n",
    "                if self.food is None:\n",
    "                    terminated = True\n",
    "                    reward = 100\n",
    "            else:\n",
    "                # No comi√≥, quitar la cola\n",
    "                self.snake.pop()\n",
    "                \n",
    "                # Recompensa por acercarse/alejarse de la comida\n",
    "                dist_after = self._distance_to_food(new_head)\n",
    "                if dist_after < dist_before:\n",
    "                    reward = 0.1  # Se acerc√≥\n",
    "                else:\n",
    "                    reward = -0.1  # Se alej√≥\n",
    "        \n",
    "        # L√≠mite de pasos (evitar loops infinitos)\n",
    "        if self.steps >= self.max_steps:\n",
    "            truncated = True\n",
    "        \n",
    "        info = {'score': self.score, 'length': len(self.snake)}\n",
    "        \n",
    "        return self._get_observation(), reward, terminated, truncated, info\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Renderiza el estado actual del juego.\"\"\"\n",
    "        grid = self._get_observation()\n",
    "        \n",
    "        # S√≠mbolos para visualizaci√≥n en consola\n",
    "        symbols = {0: '¬∑', 1: '‚ñà', 2: '‚ñì', 3: 'üçé'}\n",
    "        \n",
    "        print(\"‚îå\" + \"‚îÄ\" * (self.grid_size * 2 - 1) + \"‚îê\")\n",
    "        for row in grid:\n",
    "            line = \"‚îÇ\"\n",
    "            for cell in row:\n",
    "                line += symbols[cell] + \" \" if cell != 3 else symbols[cell]\n",
    "            print(line.rstrip() + \"‚îÇ\")\n",
    "        print(\"‚îî\" + \"‚îÄ\" * (self.grid_size * 2 - 1) + \"‚îò\")\n",
    "        print(f\"Score: {self.score} | Length: {len(self.snake)} | Steps: {self.steps}\")\n",
    "    \n",
    "    def render_matplotlib(self, ax=None):\n",
    "        \"\"\"Renderiza usando matplotlib para visualizaci√≥n en notebook.\"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        \n",
    "        grid = self._get_observation()\n",
    "        \n",
    "        # Colores: vac√≠o=blanco, cuerpo=verde, cabeza=verde oscuro, comida=rojo\n",
    "        cmap = colors.ListedColormap(['white', '#90EE90', '#228B22', 'red'])\n",
    "        bounds = [0, 1, 2, 3, 4]\n",
    "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "        \n",
    "        ax.imshow(grid, cmap=cmap, norm=norm)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f'Score: {self.score} | Length: {len(self.snake)}')\n",
    "        \n",
    "        # Dibujar cuadr√≠cula\n",
    "        for i in range(self.grid_size + 1):\n",
    "            ax.axhline(i - 0.5, color='gray', linewidth=0.5)\n",
    "            ax.axvline(i - 0.5, color='gray', linewidth=0.5)\n",
    "        \n",
    "        return ax\n",
    "\n",
    "\n",
    "# Probar el entorno\n",
    "print(\"=\" * 50)\n",
    "print(\"ENTORNO NIBBLER CREADO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "env = NibblerEnv(grid_size=8)\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "print(f\"\\nEspacio de observaci√≥n: {env.observation_space}\")\n",
    "print(f\"Espacio de acciones: {env.action_space}\")\n",
    "print(f\"Acciones: 0=‚Üë, 1=‚Üí, 2=‚Üì, 3=‚Üê\")\n",
    "print(f\"\\nEstado inicial:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='2-visualizacion'></a>\n",
    "# 2. Visualizaci√≥n del Juego\n",
    "\n",
    "Vamos a probar el entorno con un agente aleatorio y visualizarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jugar un episodio con agente aleatorio\n",
    "env = NibblerEnv(grid_size=8)\n",
    "obs, info = env.reset(seed=123)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"AGENTE ALEATORIO JUGANDO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "frames = [env._get_observation().copy()]\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(50):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    frames.append(obs.copy())\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(f\"\\nJuego terminado en paso {step + 1}\")\n",
    "        print(f\"Score final: {info['score']}\")\n",
    "        print(f\"Longitud final: {info['length']}\")\n",
    "        print(f\"Recompensa total: {total_reward:.1f}\")\n",
    "        break\n",
    "\n",
    "# Mostrar algunos frames\n",
    "n_frames = min(6, len(frames))\n",
    "fig, axes = plt.subplots(1, n_frames, figsize=(3 * n_frames, 3))\n",
    "\n",
    "indices = np.linspace(0, len(frames) - 1, n_frames, dtype=int)\n",
    "cmap = colors.ListedColormap(['white', '#90EE90', '#228B22', 'red'])\n",
    "\n",
    "for ax, idx in zip(axes, indices):\n",
    "    ax.imshow(frames[idx], cmap=cmap, vmin=0, vmax=3)\n",
    "    ax.set_title(f'Step {idx}')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Agente Aleatorio - Secuencia de Juego', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animaci√≥n interactiva (ejecutar varias veces para ver diferentes partidas)\n",
    "def jugar_episodio_animado(env, agente=None, delay=0.2, max_steps=100):\n",
    "    \"\"\"Juega un episodio mostrando la animaci√≥n.\"\"\"\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        ax.clear()\n",
    "        env.render_matplotlib(ax)\n",
    "        ax.set_title(f'Step: {step} | Score: {env.score} | Reward: {total_reward:.1f}')\n",
    "        plt.pause(0.01)\n",
    "        display(fig)\n",
    "        \n",
    "        if agente is not None:\n",
    "            action = agente.seleccionar_accion(obs)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        time.sleep(delay)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    plt.close()\n",
    "    print(f\"\\nüéÆ Juego terminado\")\n",
    "    print(f\"   Score: {info['score']}\")\n",
    "    print(f\"   Longitud: {info['length']}\")\n",
    "    print(f\"   Recompensa total: {total_reward:.1f}\")\n",
    "    \n",
    "    return info['score'], total_reward\n",
    "\n",
    "# Descomentar para ver animaci√≥n (tarda un poco)\n",
    "# env = NibblerEnv(grid_size=8)\n",
    "# jugar_episodio_animado(env, delay=0.15, max_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='3-agente'></a>\n",
    "# 3. Agente DQN\n",
    "\n",
    "Crearemos un agente DQN que aprenda a jugar al Nibbler.\n",
    "\n",
    "## Arquitectura\n",
    "- **Input**: Estado del tablero (grid_size √ó grid_size)\n",
    "- **Red**: CNN para procesar la cuadr√≠cula\n",
    "- **Output**: Q-valor para cada una de las 4 acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TORCH_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è PyTorch no disponible. Salta a la siguiente secci√≥n.\")\n",
    "else:\n",
    "    \n",
    "    class ReplayBuffer:\n",
    "        \"\"\"Buffer para Experience Replay.\"\"\"\n",
    "        \n",
    "        def __init__(self, capacity=50000):\n",
    "            self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "        def push(self, state, action, reward, next_state, done):\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        def sample(self, batch_size):\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            return (np.array(states), np.array(actions), np.array(rewards, dtype=np.float32),\n",
    "                    np.array(next_states), np.array(dones, dtype=np.float32))\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.buffer)\n",
    "    \n",
    "    \n",
    "    class DQNNetwork(nn.Module):\n",
    "        \"\"\"Red neuronal convolucional para el Nibbler.\"\"\"\n",
    "        \n",
    "        def __init__(self, grid_size, n_actions=4):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            \n",
    "            conv_out_size = 64 * grid_size * grid_size\n",
    "            \n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(conv_out_size, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, n_actions)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # x shape: (batch, grid_size, grid_size)\n",
    "            if x.dim() == 2:\n",
    "                x = x.unsqueeze(0)  # A√±adir batch dim\n",
    "            if x.dim() == 3:\n",
    "                x = x.unsqueeze(1)  # A√±adir channel dim\n",
    "            \n",
    "            x = x.float() / 3.0  # Normalizar a [0, 1]\n",
    "            x = self.conv(x)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "    \n",
    "    \n",
    "    class DQNAgent:\n",
    "        \"\"\"Agente DQN para el Nibbler.\"\"\"\n",
    "        \n",
    "        def __init__(self, grid_size, n_actions=4, lr=0.0005, gamma=0.99,\n",
    "                     epsilon=1.0, epsilon_min=0.05, epsilon_decay=0.9995,\n",
    "                     buffer_size=50000, batch_size=64, target_update=100):\n",
    "            \n",
    "            self.grid_size = grid_size\n",
    "            self.n_actions = n_actions\n",
    "            self.gamma = gamma\n",
    "            self.epsilon = epsilon\n",
    "            self.epsilon_min = epsilon_min\n",
    "            self.epsilon_decay = epsilon_decay\n",
    "            self.batch_size = batch_size\n",
    "            self.target_update = target_update\n",
    "            self.learn_step = 0\n",
    "            \n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            \n",
    "            # Redes\n",
    "            self.q_network = DQNNetwork(grid_size, n_actions).to(self.device)\n",
    "            self.target_network = DQNNetwork(grid_size, n_actions).to(self.device)\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "            \n",
    "            self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "            \n",
    "            self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        def seleccionar_accion(self, state):\n",
    "            \"\"\"Selecciona acci√≥n con Œµ-greedy.\"\"\"\n",
    "            if random.random() < self.epsilon:\n",
    "                return random.randint(0, self.n_actions - 1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                state_t = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "                q_values = self.q_network(state_t)\n",
    "                return q_values.argmax().item()\n",
    "        \n",
    "        def almacenar(self, state, action, reward, next_state, done):\n",
    "            \"\"\"Almacena transici√≥n en el buffer.\"\"\"\n",
    "            self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        def entrenar(self):\n",
    "            \"\"\"Realiza un paso de entrenamiento.\"\"\"\n",
    "            if len(self.memory) < self.batch_size:\n",
    "                return 0\n",
    "            \n",
    "            # Samplear batch\n",
    "            states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "            \n",
    "            states_t = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "            actions_t = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "            rewards_t = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "            next_states_t = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "            dones_t = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            # Q-valores actuales\n",
    "            q_values = self.q_network(states_t)\n",
    "            q_value = q_values.gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Q-valores target\n",
    "            with torch.no_grad():\n",
    "                next_q_values = self.target_network(next_states_t)\n",
    "                next_q_value = next_q_values.max(1)[0]\n",
    "                target = rewards_t + (1 - dones_t) * self.gamma * next_q_value\n",
    "            \n",
    "            # Calcular p√©rdida\n",
    "            loss = self.loss_fn(q_value, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Actualizar target network\n",
    "            self.learn_step += 1\n",
    "            if self.learn_step % self.target_update == 0:\n",
    "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "            \n",
    "            return loss.item()\n",
    "        \n",
    "        def decay_epsilon(self):\n",
    "            \"\"\"Reduce epsilon.\"\"\"\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        def guardar(self, path):\n",
    "            \"\"\"Guarda el modelo.\"\"\"\n",
    "            torch.save({\n",
    "                'q_network': self.q_network.state_dict(),\n",
    "                'target_network': self.target_network.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                'epsilon': self.epsilon\n",
    "            }, path)\n",
    "        \n",
    "        def cargar(self, path):\n",
    "            \"\"\"Carga un modelo guardado.\"\"\"\n",
    "            checkpoint = torch.load(path, map_location=self.device)\n",
    "            self.q_network.load_state_dict(checkpoint['q_network'])\n",
    "            self.target_network.load_state_dict(checkpoint['target_network'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            self.epsilon = checkpoint['epsilon']\n",
    "    \n",
    "    \n",
    "    print(\"‚úÖ Agente DQN definido\")\n",
    "    print(f\"   Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='4-entrenamiento'></a>\n",
    "# 4. Entrenamiento\n",
    "\n",
    "Entrenaremos el agente durante varios episodios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TORCH_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è PyTorch no disponible.\")\n",
    "else:\n",
    "    def entrenar_nibbler(grid_size=8, n_episodios=500, verbose=True):\n",
    "        \"\"\"Entrena el agente DQN en el Nibbler.\"\"\"\n",
    "        \n",
    "        env = NibblerEnv(grid_size=grid_size)\n",
    "        agente = DQNAgent(\n",
    "            grid_size=grid_size,\n",
    "            lr=0.0005,\n",
    "            gamma=0.95,\n",
    "            epsilon=1.0,\n",
    "            epsilon_min=0.05,\n",
    "            epsilon_decay=0.9995,\n",
    "            batch_size=64,\n",
    "            target_update=100\n",
    "        )\n",
    "        \n",
    "        # M√©tricas\n",
    "        scores = []\n",
    "        rewards = []\n",
    "        lengths = []\n",
    "        losses = []\n",
    "        \n",
    "        best_score = 0\n",
    "        \n",
    "        for ep in range(n_episodios):\n",
    "            state, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            total_loss = 0\n",
    "            steps = 0\n",
    "            \n",
    "            while True:\n",
    "                action = agente.seleccionar_accion(state)\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                agente.almacenar(state, action, reward, next_state, done)\n",
    "                loss = agente.entrenar()\n",
    "                \n",
    "                total_reward += reward\n",
    "                total_loss += loss\n",
    "                steps += 1\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            agente.decay_epsilon()\n",
    "            \n",
    "            scores.append(info['score'])\n",
    "            rewards.append(total_reward)\n",
    "            lengths.append(info['length'])\n",
    "            losses.append(total_loss / max(steps, 1))\n",
    "            \n",
    "            if info['score'] > best_score:\n",
    "                best_score = info['score']\n",
    "                agente.guardar('nibbler_best.pth')\n",
    "            \n",
    "            if verbose and (ep + 1) % 50 == 0:\n",
    "                avg_score = np.mean(scores[-50:])\n",
    "                avg_reward = np.mean(rewards[-50:])\n",
    "                print(f\"Ep {ep+1:4d} | Score: {info['score']:2d} | Avg: {avg_score:.1f} | \"\n",
    "                      f\"Best: {best_score} | Œµ: {agente.epsilon:.3f}\")\n",
    "        \n",
    "        return agente, scores, rewards, lengths, losses\n",
    "    \n",
    "    # ENTRENAR\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ENTRENAMIENTO NIBBLER DQN\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Grid: 8x8 | Episodios: 500\")\n",
    "    print(\"Esto puede tardar unos minutos...\\n\")\n",
    "    \n",
    "    agente, scores, rewards, lengths, losses = entrenar_nibbler(\n",
    "        grid_size=8,\n",
    "        n_episodios=500,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Entrenamiento completado\")\n",
    "    print(f\"   Mejor score: {max(scores)}\")\n",
    "    print(f\"   Score promedio (√∫ltimos 50): {np.mean(scores[-50:]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curvas de aprendizaje\n",
    "if TORCH_AVAILABLE and 'scores' in dir():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    window = 30\n",
    "    \n",
    "    # Score\n",
    "    axes[0, 0].plot(scores, alpha=0.3, color='blue')\n",
    "    if len(scores) >= window:\n",
    "        scores_smooth = np.convolve(scores, np.ones(window)/window, mode='valid')\n",
    "        axes[0, 0].plot(range(window-1, len(scores)), scores_smooth, color='blue', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Episodio')\n",
    "    axes[0, 0].set_ylabel('Score (comidas)')\n",
    "    axes[0, 0].set_title('Score por Episodio')\n",
    "    axes[0, 0].axhline(y=max(scores), color='green', linestyle='--', label=f'Best: {max(scores)}')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Recompensa\n",
    "    axes[0, 1].plot(rewards, alpha=0.3, color='green')\n",
    "    if len(rewards) >= window:\n",
    "        rewards_smooth = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "        axes[0, 1].plot(range(window-1, len(rewards)), rewards_smooth, color='green', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Episodio')\n",
    "    axes[0, 1].set_ylabel('Recompensa Total')\n",
    "    axes[0, 1].set_title('Recompensa por Episodio')\n",
    "    \n",
    "    # Longitud\n",
    "    axes[1, 0].plot(lengths, alpha=0.3, color='orange')\n",
    "    if len(lengths) >= window:\n",
    "        lengths_smooth = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "        axes[1, 0].plot(range(window-1, len(lengths)), lengths_smooth, color='orange', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Episodio')\n",
    "    axes[1, 0].set_ylabel('Longitud Serpiente')\n",
    "    axes[1, 0].set_title('Longitud Final por Episodio')\n",
    "    \n",
    "    # P√©rdida\n",
    "    axes[1, 1].plot(losses, alpha=0.5, color='red')\n",
    "    axes[1, 1].set_xlabel('Episodio')\n",
    "    axes[1, 1].set_ylabel('P√©rdida (Loss)')\n",
    "    axes[1, 1].set_title('P√©rdida durante Entrenamiento')\n",
    "    \n",
    "    plt.suptitle('Curvas de Aprendizaje - Nibbler DQN', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('nibbler_training.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Gr√°ficos guardados en: nibbler_training.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Primero ejecuta el entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='5-evaluacion'></a>\n",
    "# 5. Evaluaci√≥n y Demo\n",
    "\n",
    "Evaluamos el agente entrenado y vemos c√≥mo juega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE and 'agente' in dir():\n",
    "    def evaluar_agente(env, agente, n_episodios=20):\n",
    "        \"\"\"Eval√∫a el agente sin exploraci√≥n.\"\"\"\n",
    "        old_epsilon = agente.epsilon\n",
    "        agente.epsilon = 0  # Greedy\n",
    "        \n",
    "        scores = []\n",
    "        lengths = []\n",
    "        \n",
    "        for _ in range(n_episodios):\n",
    "            state, _ = env.reset()\n",
    "            \n",
    "            while True:\n",
    "                action = agente.seleccionar_accion(state)\n",
    "                state, _, terminated, truncated, info = env.step(action)\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            \n",
    "            scores.append(info['score'])\n",
    "            lengths.append(info['length'])\n",
    "        \n",
    "        agente.epsilon = old_epsilon\n",
    "        return scores, lengths\n",
    "    \n",
    "    # Evaluar\n",
    "    print(\"=\" * 50)\n",
    "    print(\"EVALUACI√ìN DEL AGENTE (20 episodios, sin exploraci√≥n)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    env = NibblerEnv(grid_size=8)\n",
    "    eval_scores, eval_lengths = evaluar_agente(env, agente, n_episodios=20)\n",
    "    \n",
    "    print(f\"\\nScore: {np.mean(eval_scores):.1f} ¬± {np.std(eval_scores):.1f}\")\n",
    "    print(f\"Mejor score: {max(eval_scores)}\")\n",
    "    print(f\"Longitud promedio: {np.mean(eval_lengths):.1f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Primero entrena el agente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Ver una partida del agente entrenado\n",
    "if TORCH_AVAILABLE and 'agente' in dir():\n",
    "    print(\"=\" * 50)\n",
    "    print(\"DEMO: Partida del Agente Entrenado\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    env = NibblerEnv(grid_size=8)\n",
    "    state, _ = env.reset(seed=42)\n",
    "    \n",
    "    old_epsilon = agente.epsilon\n",
    "    agente.epsilon = 0  # Sin exploraci√≥n\n",
    "    \n",
    "    frames = [env._get_observation().copy()]\n",
    "    actions_taken = []\n",
    "    \n",
    "    for step in range(100):\n",
    "        action = agente.seleccionar_accion(state)\n",
    "        actions_taken.append(action)\n",
    "        \n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        frames.append(env._get_observation().copy())\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    agente.epsilon = old_epsilon\n",
    "    \n",
    "    print(f\"\\nResultado:\")\n",
    "    print(f\"  Score: {info['score']}\")\n",
    "    print(f\"  Longitud: {info['length']}\")\n",
    "    print(f\"  Pasos: {len(frames) - 1}\")\n",
    "    \n",
    "    # Mostrar frames de la partida\n",
    "    n_show = min(12, len(frames))\n",
    "    indices = np.linspace(0, len(frames) - 1, n_show, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 6, figsize=(15, 5))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    cmap = colors.ListedColormap(['white', '#90EE90', '#228B22', 'red'])\n",
    "    \n",
    "    for ax, idx in zip(axes, indices):\n",
    "        ax.imshow(frames[idx], cmap=cmap, vmin=0, vmax=3)\n",
    "        ax.set_title(f'Step {idx}', fontsize=10)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        for i in range(9):\n",
    "            ax.axhline(i - 0.5, color='gray', linewidth=0.3)\n",
    "            ax.axvline(i - 0.5, color='gray', linewidth=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Partida del Agente - Score: {info[\"score\"]}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Primero entrena el agente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animaci√≥n en tiempo real (descomentar para ver)\n",
    "if TORCH_AVAILABLE and 'agente' in dir():\n",
    "    print(\"Descomenta el c√≥digo de abajo para ver la animaci√≥n en tiempo real\")\n",
    "    \n",
    "    # env = NibblerEnv(grid_size=8)\n",
    "    # agente.epsilon = 0\n",
    "    # jugar_episodio_animado(env, agente, delay=0.2, max_steps=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Resumen\n",
    "\n",
    "En este notebook hemos:\n",
    "\n",
    "1. **Creado un entorno custom** compatible con Gymnasium para el juego Nibbler/Snake\n",
    "2. **Implementado un agente DQN** con CNN para procesar el tablero\n",
    "3. **Entrenado el agente** durante 500 episodios\n",
    "4. **Evaluado y visualizado** el comportamiento aprendido\n",
    "\n",
    "## Ideas para Mejorar\n",
    "\n",
    "- Usar **Double DQN** o **Dueling DQN** para mejor estabilidad\n",
    "- A√±adir **m√°s informaci√≥n al estado** (direcci√≥n actual, distancia a comida)\n",
    "- Probar con **grids m√°s grandes** (10x10, 15x15)\n",
    "- Implementar **reward shaping** m√°s sofisticado\n",
    "- Usar **prioritized experience replay**\n",
    "\n",
    "## Para la Pr√°ctica Final\n",
    "\n",
    "Puedes usar este entorno como base para tu pr√°ctica. Modifica:\n",
    "- El tama√±o del grid\n",
    "- La arquitectura de la red\n",
    "- Los hiperpar√°metros\n",
    "- El sistema de recompensas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
