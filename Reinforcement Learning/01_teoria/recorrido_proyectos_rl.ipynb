{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recorrido por los Proyectos de Reinforcement Learning\n",
    "\n",
    "Este notebook relaciona cada proyecto, ejemplo y modelo del directorio con los conceptos te√≥ricos vistos en clase.\n",
    "\n",
    "## Mapa de Contenidos\n",
    "\n",
    "```\n",
    "NIVEL 1: TEOR√çA (01_teoria/)\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ Conceptos B√°sicos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ 02_fundamentos/core/agentes.py\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ Agente, Entorno, Estado     (QLearningAgent, SARSAAgent)\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ Pol√≠tica, Recompensa\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ Exploraci√≥n vs Explotaci√≥n\n",
    "    ‚îÇ\n",
    "NIVEL 2: FUNDAMENTOS (02_fundamentos/)\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ Q-Learning ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ 02_fundamentos/ejemplos/ejemplo_qlearning_taxi.py\n",
    "    ‚îÇ                                  (Q-Learning tabular en Taxi-v3)\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ DQN ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ 02_fundamentos/ejemplos/ejemplo_cartpole_dqn.py\n",
    "    ‚îÇ\n",
    "NIVEL 3: PROYECTOS DQN (03_proyectos_dqn/)\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ DQN Aplicado ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ 03_proyectos_dqn/nibbler/ (Snake)\n",
    "    ‚îÇ                                  03_proyectos_dqn/flappybird/\n",
    "    ‚îÇ                                  03_proyectos_dqn/racing/\n",
    "    ‚îÇ\n",
    "NIVEL 4: PROYECTOS AVANZADOS (04_proyectos_avanzados/)\n",
    "    ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ Algoritmos Avanzados ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ 04_proyectos_avanzados/lunarlander/ (PPO, A2C)\n",
    "        (PPO, SAC, TD3)                04_proyectos_avanzados/highway/ (DQN, PPO)\n",
    "                                       04_proyectos_avanzados/pybullet/ (SAC, TD3)\n",
    "                                       04_proyectos_avanzados/minigrid/ (PPO)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura del Directorio\n",
    "\n",
    "```\n",
    "Reinforcement Learning/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 01_teoria/                         # NIVEL 1: Fundamentos te√≥ricos\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ reinforcement_learning_clase.ipynb\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ recorrido_proyectos_rl.ipynb   # Este notebook\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 02_fundamentos/                    # NIVEL 2: Implementaciones base\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ core/                          # Algoritmos\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agentes.py                 # Q-Learning, SARSA, DQN\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py                   # Gr√°ficos y evaluaci√≥n\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ejemplos/                      # Tutoriales b√°sicos\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ ejemplo_cartpole_dqn.py\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ ejemplo_qlearning_taxi.py\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 03_proyectos_dqn/                  # NIVEL 3: Proyectos con DQN\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ nibbler/                       # Snake con DQN\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ flappybird/                    # Flappy Bird\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ racing/                        # Carreras multi-agente\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 04_proyectos_avanzados/            # NIVEL 4: Algoritmos avanzados\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ lunarlander/                   # PPO, A2C, DQN\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ highway/                       # Conducci√≥n aut√≥noma\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ minigrid/                      # Laberintos 2D\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ pybullet/                      # Rob√≥tica 3D\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ modelos/                           # Pesos entrenados (.pth)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necesarios para este recorrido\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# A√±adir el directorio ra√≠z al path (subimos un nivel desde 01_teoria/)\n",
    "RL_DIR = Path().absolute().parent\n",
    "if str(RL_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(RL_DIR))\n",
    "\n",
    "# Tambi√©n a√±adir 02_fundamentos para imports de core\n",
    "FUND_DIR = RL_DIR / \"02_fundamentos\"\n",
    "if str(FUND_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(FUND_DIR))\n",
    "\n",
    "print(f\"Directorio RL: {RL_DIR}\")\n",
    "print(f\"\\nEstructura:\")\n",
    "for item in sorted(RL_DIR.iterdir()):\n",
    "    if item.name.startswith('.'):\n",
    "        continue\n",
    "    prefix = \"üìÅ\" if item.is_dir() else \"üìÑ\"\n",
    "    print(f\"  {prefix} {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. M√≥dulo Core: Las Implementaciones Base\n",
    "\n",
    "**Ubicaci√≥n**: `02_fundamentos/core/`\n",
    "\n",
    "El m√≥dulo `core/` contiene las implementaciones fundamentales de los algoritmos vistos en teor√≠a.\n",
    "\n",
    "## Relaci√≥n con la Teor√≠a\n",
    "\n",
    "| Concepto Te√≥rico | Implementaci√≥n en `core/agentes.py` |\n",
    "|------------------|-------------------------------------|\n",
    "| Tabla Q | `QLearningAgent.Q` (defaultdict) |\n",
    "| Ecuaci√≥n de Bellman | `QLearningAgent.aprender()` |\n",
    "| Pol√≠tica Œµ-greedy | `QLearningAgent.seleccionar_accion()` |\n",
    "| Experience Replay | `ReplayBuffer` (deque circular) |\n",
    "| Red Neuronal para Q | `DQNetwork` (nn.Module) |\n",
    "| Target Network | `DQNAgent.target_network` |\n",
    "| TD Error | Diferencia entre target y predicci√≥n |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos las clases disponibles en core/agentes.py\n",
    "from core import agentes\n",
    "import inspect\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLASES EN 02_fundamentos/core/agentes.py\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "clases = [\n",
    "    (\"ReplayBuffer\", \"Buffer circular para Experience Replay (DQN)\"),\n",
    "    (\"QLearningAgent\", \"Agente Q-Learning tabular\"),\n",
    "    (\"SARSAAgent\", \"Agente SARSA (on-policy)\"),\n",
    "    (\"DQNetwork\", \"Red neuronal para aproximar Q(s,a)\"),\n",
    "    (\"DQNAgent\", \"Agente DQN completo con target network\"),\n",
    "]\n",
    "\n",
    "for nombre, desc in clases:\n",
    "    if hasattr(agentes, nombre):\n",
    "        print(f\"\\n‚úÖ {nombre}\")\n",
    "        print(f\"   {desc}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {nombre} - No encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Ver la implementaci√≥n de Q-Learning\n",
    "from core.agentes import QLearningAgent\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANATOM√çA DE QLearningAgent\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear un agente de ejemplo\n",
    "agente = QLearningAgent(n_estados=16, n_acciones=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "\n",
    "print(f\"\"\"\n",
    "Par√°metros del agente:\n",
    "  - n_estados: {agente.n_estados if hasattr(agente, 'n_estados') else 'N/A'}\n",
    "  - n_acciones: {agente.n_acciones}\n",
    "  - alpha (learning rate): {agente.alpha}\n",
    "  - gamma (descuento): {agente.gamma}\n",
    "  - epsilon (exploraci√≥n): {agente.epsilon}\n",
    "\n",
    "Correspondencia con la teor√≠a:\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇ Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]               ‚îÇ\n",
    "  ‚îÇ                                                                 ‚îÇ\n",
    "  ‚îÇ alpha (Œ±)  = {agente.alpha}  ‚Üí Tasa de aprendizaje                       ‚îÇ\n",
    "  ‚îÇ gamma (Œ≥)  = {agente.gamma} ‚Üí Factor de descuento                       ‚îÇ\n",
    "  ‚îÇ epsilon(Œµ) = {agente.epsilon}  ‚Üí Probabilidad de exploraci√≥n              ‚îÇ\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Ver la estructura del ReplayBuffer (Experience Replay)\n",
    "from core.agentes import ReplayBuffer\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIENCE REPLAY BUFFER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "buffer = ReplayBuffer(capacidad=1000)\n",
    "\n",
    "# Simular algunas experiencias\n",
    "import numpy as np\n",
    "for i in range(5):\n",
    "    estado = np.array([i, i*2])\n",
    "    accion = i % 2\n",
    "    recompensa = 1.0 if i == 4 else -0.1\n",
    "    siguiente = np.array([i+1, (i+1)*2])\n",
    "    terminado = (i == 4)\n",
    "    buffer.agregar(estado, accion, recompensa, siguiente, terminado)\n",
    "\n",
    "print(f\"\"\"\n",
    "Experiencias almacenadas: {len(buffer)}\n",
    "\n",
    "¬øPara qu√© sirve el Experience Replay?\n",
    "  1. Rompe la correlaci√≥n temporal entre muestras consecutivas\n",
    "  2. Reutiliza experiencias pasadas (eficiencia de datos)\n",
    "  3. Estabiliza el entrenamiento de redes neuronales\n",
    "\n",
    "Cada experiencia es una tupla (s, a, r, s', done):\n",
    "\"\"\")\n",
    "\n",
    "for i, exp in enumerate(list(buffer.buffer)[:3]):\n",
    "    s, a, r, s_next, done = exp\n",
    "    print(f\"  Exp {i+1}: s={s}, a={a}, r={r:.1f}, s'={s_next}, done={done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Ejemplos B√°sicos (Nivel 2)\n",
    "\n",
    "**Ubicaci√≥n**: `02_fundamentos/ejemplos/`\n",
    "\n",
    "Los ejemplos son tutoriales completos que demuestran los algoritmos fundamentales.\n",
    "\n",
    "## 2.1 Q-Learning Tabular: Taxi-v3\n",
    "\n",
    "**Archivo**: `02_fundamentos/ejemplos/ejemplo_qlearning_taxi.py`\n",
    "\n",
    "### Relaci√≥n con la Teor√≠a\n",
    "\n",
    "| Concepto | Aplicaci√≥n en Taxi |\n",
    "|----------|-------------------|\n",
    "| **Estados discretos** | 500 estados posibles (5√ó5√ó5√ó4) |\n",
    "| **Acciones discretas** | 6 acciones (N, S, E, W, pickup, dropoff) |\n",
    "| **Tabla Q** | Matriz 500√ó6 |\n",
    "| **Recompensas** | +20 entrega, -10 acci√≥n ilegal, -1 por paso |\n",
    "| **Episodios** | Termina al entregar pasajero |\n",
    "\n",
    "### Por qu√© funciona Q-Learning tabular aqu√≠\n",
    "\n",
    "- El espacio de estados es **peque√±o y discreto** (500 estados)\n",
    "- Podemos almacenar Q(s,a) en una tabla real\n",
    "- No necesitamos aproximaci√≥n de funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el entorno Taxi-v3\n",
    "import gymnasium as gym\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTORNO: Taxi-v3\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Espacio de estados: {env.observation_space}\n",
    "  - 500 estados discretos\n",
    "  - Codifica: posici√≥n taxi (25) √ó posici√≥n pasajero (5) √ó destino (4)\n",
    "\n",
    "Espacio de acciones: {env.action_space}\n",
    "  - 0: Sur\n",
    "  - 1: Norte  \n",
    "  - 2: Este\n",
    "  - 3: Oeste\n",
    "  - 4: Recoger pasajero\n",
    "  - 5: Dejar pasajero\n",
    "\n",
    "Recompensas:\n",
    "  - +20: Entregar pasajero correctamente\n",
    "  - -10: Recoger/dejar en lugar incorrecto\n",
    "  - -1: Cada paso (incentiva eficiencia)\n",
    "\"\"\")\n",
    "\n",
    "# Mostrar un estado inicial\n",
    "obs, _ = env.reset(seed=42)\n",
    "print(f\"Estado inicial (codificado): {obs}\")\n",
    "print(f\"\\nVisualizaci√≥n ASCII:\")\n",
    "print(env.render())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 DQN: CartPole-v1\n",
    "\n",
    "**Archivo**: `02_fundamentos/ejemplos/ejemplo_cartpole_dqn.py`\n",
    "\n",
    "### Relaci√≥n con la Teor√≠a\n",
    "\n",
    "| Concepto | Aplicaci√≥n en CartPole |\n",
    "|----------|------------------------|\n",
    "| **Estados continuos** | 4 valores: posici√≥n, velocidad, √°ngulo, vel. angular |\n",
    "| **Por qu√© no tabular** | Estados continuos ‚Üí infinitos valores posibles |\n",
    "| **Red neuronal** | Aproxima Q(s,a) para todo s |\n",
    "| **Experience Replay** | Buffer de 10,000 experiencias |\n",
    "| **Target Network** | Actualizada cada 10 episodios |\n",
    "\n",
    "### Arquitectura de la Red\n",
    "\n",
    "```\n",
    "Estado [4] ‚Üí Dense(64) ‚Üí ReLU ‚Üí Dense(64) ‚Üí ReLU ‚Üí Q-valores [2]\n",
    "   ‚îÇ                                                    ‚îÇ\n",
    "   ‚îÇ         (posici√≥n, velocidad,                      ‚îÇ\n",
    "   ‚îÇ          √°ngulo, vel_angular)              (izq, derecha)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el entorno CartPole\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTORNO: CartPole-v1\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Espacio de estados: {env.observation_space}\n",
    "  - Estado CONTINUO de 4 dimensiones:\n",
    "    [0] Posici√≥n del carro     (-4.8 a 4.8)\n",
    "    [1] Velocidad del carro    (-‚àû a ‚àû)\n",
    "    [2] √Ångulo del poste       (-0.42 a 0.42 rad ‚âà ¬±24¬∞)\n",
    "    [3] Velocidad angular      (-‚àû a ‚àû)\n",
    "\n",
    "Espacio de acciones: {env.action_space}\n",
    "  - 0: Empujar hacia la IZQUIERDA\n",
    "  - 1: Empujar hacia la DERECHA\n",
    "\n",
    "Recompensa: +1 por cada paso que el poste sigue en pie\n",
    "\n",
    "Termina cuando:\n",
    "  - El poste se inclina m√°s de 12¬∞ (¬±0.21 rad)\n",
    "  - El carro sale del √°rea visible\n",
    "  - Se alcanzan 500 pasos (¬°√©xito!)\n",
    "\"\"\")\n",
    "\n",
    "# Mostrar una observaci√≥n\n",
    "obs, _ = env.reset(seed=42)\n",
    "print(f\"Observaci√≥n de ejemplo: {obs}\")\n",
    "print(f\"  Posici√≥n carro: {obs[0]:.4f}\")\n",
    "print(f\"  Velocidad carro: {obs[1]:.4f}\")\n",
    "print(f\"  √Ångulo poste: {obs[2]:.4f} rad ({np.degrees(obs[2]):.2f}¬∞)\")\n",
    "print(f\"  Velocidad angular: {obs[3]:.4f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver la arquitectura DQN del ejemplo\n",
    "print(\"=\"*60)\n",
    "print(\"ARQUITECTURA DQN PARA CARTPOLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from core.agentes import DQNetwork\n",
    "    \n",
    "    # Crear red para CartPole (4 inputs, 2 outputs)\n",
    "    red = DQNetwork(input_size=4, n_acciones=2)\n",
    "    \n",
    "    print(f\"\\nArquitectura de la red:\")\n",
    "    print(red)\n",
    "    \n",
    "    # Contar par√°metros\n",
    "    n_params = sum(p.numel() for p in red.parameters())\n",
    "    print(f\"\\nTotal de par√°metros entrenables: {n_params:,}\")\n",
    "    \n",
    "    # Hacer una predicci√≥n de ejemplo\n",
    "    estado = torch.FloatTensor([[0.02, 0.01, 0.03, -0.02]])\n",
    "    with torch.no_grad():\n",
    "        q_valores = red(estado)\n",
    "    \n",
    "    print(f\"\\nEjemplo de predicci√≥n:\")\n",
    "    print(f\"  Estado: {estado.numpy()[0]}\")\n",
    "    print(f\"  Q(s, izq): {q_valores[0, 0]:.4f}\")\n",
    "    print(f\"  Q(s, der): {q_valores[0, 1]:.4f}\")\n",
    "    print(f\"  Mejor acci√≥n: {'DERECHA' if q_valores[0, 1] > q_valores[0, 0] else 'IZQUIERDA'}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"PyTorch no disponible: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Proyectos DQN (Nivel 3)\n",
    "\n",
    "**Ubicaci√≥n**: `03_proyectos_dqn/`\n",
    "\n",
    "Proyectos que aplican DQN en escenarios visuales con Pygame.\n",
    "\n",
    "## Mapa de Proyectos\n",
    "\n",
    "| Proyecto | Algoritmo | Librer√≠a | Dificultad | Tipo de Observaci√≥n |\n",
    "|----------|-----------|----------|------------|---------------------|\n",
    "| **Nibbler** | DQN | PyTorch | ‚≠ê‚≠ê | Grid/Estado discreto |\n",
    "| **Racing** | DQN | PyTorch | ‚≠ê‚≠ê | Sensores continuos |\n",
    "| **Flappy Bird** | DQN/PPO | SB3 | ‚≠ê‚≠ê | Vector o p√≠xeles |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Nibbler (Snake) - DQN desde Cero\n",
    "\n",
    "**Directorio**: `03_proyectos_dqn/nibbler/`\n",
    "\n",
    "### Descripci√≥n\n",
    "El cl√°sico juego de la serpiente (Snake) implementado con Pygame, donde un agente DQN aprende a comer manzanas y evitar chocar.\n",
    "\n",
    "### Relaci√≥n con la Teor√≠a\n",
    "\n",
    "| Concepto Te√≥rico | Implementaci√≥n en Nibbler |\n",
    "|------------------|---------------------------|\n",
    "| **Estado** | Vector con: direcci√≥n, posici√≥n relativa de comida, peligros cercanos |\n",
    "| **Acciones** | 3 acciones: recto, girar izquierda, girar derecha |\n",
    "| **Recompensa** | +10 comer, -10 morir, +1 acercarse a comida |\n",
    "| **Red neuronal** | MLP: estado ‚Üí capas ocultas ‚Üí 3 Q-valores |\n",
    "| **Experience Replay** | S√≠, buffer de experiencias |\n",
    "| **Target Network** | S√≠, actualizada cada N pasos |\n",
    "\n",
    "### Dise√±o del Estado (Ingenier√≠a de Features)\n",
    "\n",
    "```\n",
    "Estado = [\n",
    "    peligro_adelante,   # ¬øHay pared/cuerpo adelante?\n",
    "    peligro_izquierda,  # ¬øHay peligro a la izquierda?\n",
    "    peligro_derecha,    # ¬øHay peligro a la derecha?\n",
    "    direccion_arriba,   # ¬øVoy hacia arriba?\n",
    "    direccion_abajo,    # ¬øVoy hacia abajo?\n",
    "    direccion_izq,      # ¬øVoy hacia la izquierda?\n",
    "    direccion_der,      # ¬øVoy hacia la derecha?\n",
    "    comida_arriba,      # ¬øComida est√° arriba?\n",
    "    comida_abajo,       # ¬øComida est√° abajo?\n",
    "    comida_izq,         # ¬øComida est√° a la izquierda?\n",
    "    comida_der          # ¬øComida est√° a la derecha?\n",
    "]\n",
    "```\n",
    "\n",
    "**Nota**: Este es un buen ejemplo de c√≥mo el **dise√±o del estado** afecta el aprendizaje. Un estado bien dise√±ado facilita que el agente aprenda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar archivos del proyecto Nibbler\n",
    "from pathlib import Path\n",
    "\n",
    "nibbler_dir = RL_DIR / \"03_proyectos_dqn\" / \"nibbler\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROYECTO: Nibbler (Snake)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if nibbler_dir.exists():\n",
    "    print(f\"\\nArchivos en {nibbler_dir.relative_to(RL_DIR)}:\")\n",
    "    for f in sorted(nibbler_dir.iterdir()):\n",
    "        if f.is_file():\n",
    "            size = f.stat().st_size / 1024\n",
    "            print(f\"  üìÑ {f.name} ({size:.1f} KB)\")\n",
    "else:\n",
    "    print(f\"Directorio no encontrado: {nibbler_dir}\")\n",
    "\n",
    "# Verificar modelo guardado\n",
    "modelo_nibbler = RL_DIR / \"modelos\" / \"nibbler_best.pth\"\n",
    "if modelo_nibbler.exists():\n",
    "    size = modelo_nibbler.stat().st_size / 1024\n",
    "    print(f\"\\n‚úÖ Modelo entrenado disponible: modelos/nibbler_best.pth ({size:.1f} KB)\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Modelo no encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Racing (Carreras Multi-Agente)\n",
    "\n",
    "**Directorio**: `03_proyectos_dqn/racing/`\n",
    "\n",
    "### Descripci√≥n\n",
    "Juego de carreras con m√∫ltiples coches (4-8) que aprenden simult√°neamente a navegar un circuito.\n",
    "\n",
    "### Relaci√≥n con la Teor√≠a\n",
    "\n",
    "| Concepto | Aplicaci√≥n |\n",
    "|----------|------------|\n",
    "| **Multi-agente** | Cada coche es un agente independiente |\n",
    "| **Estado** | Sensores de distancia (raycast) + velocidad |\n",
    "| **Acciones** | Acelerar, frenar, girar |\n",
    "| **Reward shaping** | +1 avanzar, -100 chocar, bonus por velocidad |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar modelos de racing\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROYECTO: Racing (Multi-Agente)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "modelos_dir = RL_DIR / \"modelos\"\n",
    "if modelos_dir.exists():\n",
    "    car_models = list(modelos_dir.glob(\"car_agent_*.pth\"))\n",
    "    print(f\"\\nModelos de coches entrenados: {len(car_models)}\")\n",
    "    for m in sorted(car_models):\n",
    "        size = m.stat().st_size / 1024\n",
    "        print(f\"  üöó {m.name} ({size:.1f} KB)\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Caracter√≠sticas del proyecto:\n",
    "  - 4-8 coches compitiendo simult√°neamente\n",
    "  - Cada coche tiene sensores de distancia (raycast)\n",
    "  - Aprenden a mantener la trayectoria y evitar colisiones\n",
    "  - Visualizaci√≥n con Pygame\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Proyectos Avanzados (Nivel 4)\n",
    "\n",
    "**Ubicaci√≥n**: `04_proyectos_avanzados/`\n",
    "\n",
    "Proyectos que usan algoritmos avanzados (PPO, SAC, TD3) con Stable-Baselines3.\n",
    "\n",
    "## Mapa de Proyectos Avanzados\n",
    "\n",
    "| Proyecto | Algoritmo | Dificultad | Tipo de Observaci√≥n |\n",
    "|----------|-----------|------------|---------------------|\n",
    "| **LunarLander** | PPO/DQN/A2C | ‚≠ê‚≠ê | Vector 8D |\n",
    "| **Highway** | DQN/PPO | ‚≠ê‚≠ê‚≠ê | Cinem√°tico 5√ó5 |\n",
    "| **MiniGrid** | PPO | ‚≠ê‚≠ê‚≠ê | Imagen 7√ó7√ó3 |\n",
    "| **PyBullet** | PPO/SAC/TD3 | ‚≠ê‚≠ê‚≠ê‚≠ê | Sensores rob√≥ticos |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 LunarLander - PPO con Stable-Baselines3\n",
    "\n",
    "**Directorio**: `04_proyectos_avanzados/lunarlander/`\n",
    "\n",
    "### Descripci√≥n\n",
    "Aterrizar una nave espacial en la superficie lunar usando propulsores.\n",
    "\n",
    "### PPO vs DQN\n",
    "\n",
    "| Aspecto | DQN | PPO |\n",
    "|---------|-----|-----|\n",
    "| Tipo | Value-based | Policy-based |\n",
    "| Acciones | Solo discretas | Discretas y continuas |\n",
    "| Estabilidad | Menos estable | Muy estable |\n",
    "| Eficiencia de datos | Mejor (replay) | Peor (on-policy) |\n",
    "\n",
    "**PPO (Proximal Policy Optimization)** es uno de los algoritmos m√°s usados en la pr√°ctica por su robustez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de c√≥digo de LunarLander con SB3\n",
    "print(\"=\"*60)\n",
    "print(\"LUNARLANDER CON STABLE-BASELINES3\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "codigo_lunar = '''\n",
    "# C√≥digo simplificado de 04_proyectos_avanzados/lunarlander/lunarlander_sb3.py\n",
    "\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "import gymnasium as gym\n",
    "\n",
    "# 1. Crear entorno\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "# 2. Crear agente (¬°solo 1 l√≠nea!)\n",
    "modelo = PPO(\n",
    "    \"MlpPolicy\",          # Red neuronal MLP\n",
    "    env,\n",
    "    learning_rate=0.0003,\n",
    "    gamma=0.99,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Entrenar\n",
    "modelo.learn(total_timesteps=100_000)\n",
    "\n",
    "# 4. Guardar\n",
    "modelo.save(\"lunarlander_ppo\")\n",
    "\n",
    "# 5. Demo\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "obs, _ = env.reset()\n",
    "while True:\n",
    "    action, _ = modelo.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "'''\n",
    "print(codigo_lunar)\n",
    "\n",
    "print(\"\\n¬°Stable-Baselines3 abstrae toda la complejidad!\")\n",
    "print(\"  - Experience Replay: Autom√°tico\")\n",
    "print(\"  - Target Network: Autom√°tico\")\n",
    "print(\"  - Normalizaci√≥n: Autom√°tico\")\n",
    "print(\"  - Logging: Autom√°tico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Otros Proyectos Avanzados\n",
    "\n",
    "### Highway (Conducci√≥n Aut√≥noma)\n",
    "- **Directorio**: `04_proyectos_avanzados/highway/`\n",
    "- **Observaci√≥n cinem√°tica**: Matriz 5√ó5 con veh√≠culos cercanos\n",
    "- **Escenarios**: autopista, merge, roundabout, parking\n",
    "\n",
    "### MiniGrid (Laberintos)\n",
    "- **Directorio**: `04_proyectos_avanzados/minigrid/`\n",
    "- **Observaci√≥n parcial**: El agente solo ve 7√ó7 celdas frente a √©l\n",
    "- **Wrappers**: Transforman la observaci√≥n (flatten, RGB)\n",
    "\n",
    "### PyBullet (Rob√≥tica 3D)\n",
    "- **Directorio**: `04_proyectos_avanzados/pybullet/`\n",
    "- **Acciones continuas**: Torques en cada articulaci√≥n\n",
    "- **Algoritmos**: SAC, TD3 (para acciones continuas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Modelos Guardados\n",
    "\n",
    "**Ubicaci√≥n**: `modelos/`\n",
    "\n",
    "El directorio contiene pesos de redes neuronales ya entrenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar todos los modelos guardados\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODELOS GUARDADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "modelos_dir = RL_DIR / \"modelos\"\n",
    "\n",
    "if modelos_dir.exists():\n",
    "    modelos = list(modelos_dir.glob(\"*.pth\")) + list(modelos_dir.glob(\"*.zip\"))\n",
    "    \n",
    "    print(f\"\\nModelos encontrados: {len(modelos)}\\n\")\n",
    "    \n",
    "    for m in sorted(modelos):\n",
    "        size = m.stat().st_size / 1024\n",
    "        \n",
    "        # Determinar tipo y proyecto\n",
    "        if \"cartpole\" in m.name.lower():\n",
    "            proyecto = \"CartPole (DQN) - Nivel 2\"\n",
    "            icono = \"üéØ\"\n",
    "        elif \"nibbler\" in m.name.lower():\n",
    "            proyecto = \"Nibbler/Snake (DQN) - Nivel 3\"\n",
    "            icono = \"üêç\"\n",
    "        elif \"car_agent\" in m.name.lower():\n",
    "            proyecto = \"Racing (Multi-agente) - Nivel 3\"\n",
    "            icono = \"üöó\"\n",
    "        elif \"lunar\" in m.name.lower():\n",
    "            proyecto = \"LunarLander (PPO/DQN) - Nivel 4\"\n",
    "            icono = \"üöÄ\"\n",
    "        else:\n",
    "            proyecto = \"Desconocido\"\n",
    "            icono = \"üì¶\"\n",
    "        \n",
    "        print(f\"{icono} {m.name}\")\n",
    "        print(f\"   Proyecto: {proyecto}\")\n",
    "        print(f\"   Tama√±o: {size:.1f} KB\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Directorio de modelos no encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Resumen: De la Teor√≠a a la Pr√°ctica\n",
    "\n",
    "## Flujo de Aprendizaje por Niveles\n",
    "\n",
    "```\n",
    "NIVEL 1: TEOR√çA\n",
    "‚îî‚îÄ‚îÄ 01_teoria/reinforcement_learning_clase.ipynb\n",
    "    ‚îú‚îÄ‚îÄ Conceptos: Agente, Estado, Acci√≥n, Recompensa\n",
    "    ‚îú‚îÄ‚îÄ MDP y Ecuaci√≥n de Bellman\n",
    "    ‚îî‚îÄ‚îÄ Q-Learning y DQN\n",
    "\n",
    "NIVEL 2: FUNDAMENTOS\n",
    "‚îú‚îÄ‚îÄ 02_fundamentos/core/agentes.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Ver c√≥mo se implementa Q-Learning\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Entender ReplayBuffer\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Estudiar arquitectura DQN\n",
    "‚îî‚îÄ‚îÄ 02_fundamentos/ejemplos/\n",
    "    ‚îú‚îÄ‚îÄ ejemplo_qlearning_taxi.py ‚Üí Q-Learning tabular\n",
    "    ‚îî‚îÄ‚îÄ ejemplo_cartpole_dqn.py ‚Üí DQN b√°sico\n",
    "\n",
    "NIVEL 3: PROYECTOS DQN\n",
    "‚îî‚îÄ‚îÄ 03_proyectos_dqn/\n",
    "    ‚îú‚îÄ‚îÄ nibbler/ ‚Üí DQN desde cero con Pygame\n",
    "    ‚îú‚îÄ‚îÄ flappybird/ ‚Üí DQN/PPO\n",
    "    ‚îî‚îÄ‚îÄ racing/ ‚Üí Multi-agente\n",
    "\n",
    "NIVEL 4: PROYECTOS AVANZADOS\n",
    "‚îî‚îÄ‚îÄ 04_proyectos_avanzados/\n",
    "    ‚îú‚îÄ‚îÄ lunarlander/ ‚Üí Stable-Baselines3 (PPO, A2C)\n",
    "    ‚îú‚îÄ‚îÄ highway/ ‚Üí Conducci√≥n aut√≥noma\n",
    "    ‚îú‚îÄ‚îÄ minigrid/ ‚Üí Observaci√≥n parcial\n",
    "    ‚îî‚îÄ‚îÄ pybullet/ ‚Üí Rob√≥tica 3D (SAC, TD3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla de Correspondencia Completa\n",
    "\n",
    "| Concepto Te√≥rico | D√≥nde se Aplica | Archivo de Referencia |\n",
    "|------------------|-----------------|----------------------|\n",
    "| **Ciclo Agente-Entorno** | Todos los proyectos | `02_fundamentos/core/agentes.py` |\n",
    "| **Pol√≠tica Œµ-greedy** | Q-Learning, DQN | `02_fundamentos/core/agentes.py` |\n",
    "| **Ecuaci√≥n de Bellman** | Q-Learning | `02_fundamentos/core/agentes.py` |\n",
    "| **Tabla Q** | Taxi | `02_fundamentos/ejemplos/ejemplo_qlearning_taxi.py` |\n",
    "| **Estados continuos** | CartPole, LunarLander | `02_fundamentos/ejemplos/ejemplo_cartpole_dqn.py` |\n",
    "| **Red Neuronal para Q** | DQN | `02_fundamentos/core/agentes.py:DQNetwork` |\n",
    "| **Experience Replay** | DQN | `02_fundamentos/core/agentes.py:ReplayBuffer` |\n",
    "| **Target Network** | DQN | `02_fundamentos/core/agentes.py:DQNAgent` |\n",
    "| **PPO** | LunarLander, Highway | `04_proyectos_avanzados/lunarlander/` |\n",
    "| **SAC/TD3** | PyBullet (rob√≥tica) | `04_proyectos_avanzados/pybullet/` |\n",
    "| **Multi-agente** | Racing | `03_proyectos_dqn/racing/` |\n",
    "| **Wrappers** | MiniGrid | `04_proyectos_avanzados/minigrid/` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comandos R√°pidos para Ejecutar Proyectos\n",
    "\n",
    "```bash\n",
    "# Desde el directorio 'Reinforcement Learning/'\n",
    "\n",
    "# Nivel 2: Ejemplos b√°sicos\n",
    "python 02_fundamentos/ejemplos/ejemplo_qlearning_taxi.py\n",
    "python 02_fundamentos/ejemplos/ejemplo_cartpole_dqn.py\n",
    "\n",
    "# Nivel 3: Proyectos DQN\n",
    "python 03_proyectos_dqn/nibbler/nibbler_game.py           # Jugar\n",
    "python 03_proyectos_dqn/nibbler/nibbler_game.py --train   # Entrenar\n",
    "python 03_proyectos_dqn/racing/racing_game.py             # Carreras\n",
    "python 03_proyectos_dqn/flappybird/flappybird_dqn.py      # Flappy Bird\n",
    "\n",
    "# Nivel 4: Proyectos Avanzados\n",
    "python 04_proyectos_avanzados/lunarlander/lunarlander_sb3.py         # Entrenar\n",
    "python 04_proyectos_avanzados/lunarlander/lunarlander_sb3.py --demo  # Demo\n",
    "python 04_proyectos_avanzados/highway/highway_conduccion.py\n",
    "python 04_proyectos_avanzados/minigrid/minigrid_navegacion.py\n",
    "python 04_proyectos_avanzados/pybullet/pybullet_robotica.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusi√≥n\n",
    "\n",
    "Este directorio de Reinforcement Learning est√° organizado en **4 niveles de complejidad**:\n",
    "\n",
    "1. **Nivel 1 (Teor√≠a)** ‚Üí `01_teoria/` - Notebooks con fundamentos te√≥ricos\n",
    "2. **Nivel 2 (Fundamentos)** ‚Üí `02_fundamentos/` - Implementaciones base y ejemplos\n",
    "3. **Nivel 3 (DQN)** ‚Üí `03_proyectos_dqn/` - Proyectos visuales con DQN\n",
    "4. **Nivel 4 (Avanzado)** ‚Üí `04_proyectos_avanzados/` - PPO, SAC, TD3\n",
    "\n",
    "Cada proyecto demuestra conceptos espec√≠ficos de RL en acci√≥n, permitiendo ver c√≥mo la matem√°tica se traduce en c√≥digo funcional.\n",
    "\n",
    "**¬°Experimenta, modifica, y crea tus propios agentes!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
