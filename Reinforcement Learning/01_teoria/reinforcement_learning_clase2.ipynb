{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Clase 2: Entrenamiento con Stable-Baselines3\n",
    "\n",
    "## Continuaci√≥n de reinforcement_learning_clase.ipynb\n",
    "\n",
    "En el notebook anterior aprendimos:\n",
    "- Conceptos fundamentales de RL (agente, entorno, recompensa, pol√≠tica)\n",
    "- Algoritmos tabulares (SARSA, Q-Learning)\n",
    "- Deep Q-Network (DQN) desde cero\n",
    "- Gymnasium para entornos\n",
    "\n",
    "En este notebook aprenderemos:\n",
    "- **Stable-Baselines3**: La librer√≠a est√°ndar para entrenar agentes RL\n",
    "- **Algoritmos avanzados**: PPO, A2C, SAC, TD3\n",
    "- **Entrenamiento pr√°ctico**: Callbacks, logging, guardado de modelos\n",
    "- **Proyectos reales**: Conexi√≥n con `04_proyectos_avanzados/`\n",
    "\n",
    "---\n",
    "\n",
    "## √çndice\n",
    "\n",
    "1. [¬øPor qu√© Stable-Baselines3?](#1-porque-sb3)\n",
    "2. [Instalaci√≥n y Setup](#2-instalacion)\n",
    "3. [Tu Primer Agente con SB3](#3-primer-agente)\n",
    "4. [Algoritmos Disponibles](#4-algoritmos)\n",
    "5. [Pol√≠ticas (Redes Neuronales)](#5-politicas)\n",
    "6. [Entrenamiento Avanzado](#6-entrenamiento)\n",
    "7. [Callbacks y Logging](#7-callbacks)\n",
    "8. [Guardar y Cargar Modelos](#8-guardar-cargar)\n",
    "9. [Evaluaci√≥n de Agentes](#9-evaluacion)\n",
    "10. [Hiperpar√°metros](#10-hiperparametros)\n",
    "11. [Proyectos Pr√°cticos](#11-proyectos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='1-porque-sb3'></a>\n",
    "# 1. ¬øPor qu√© Stable-Baselines3?\n",
    "\n",
    "## El Problema: Implementar RL es Dif√≠cil\n",
    "\n",
    "Implementar algoritmos de RL correctamente es sorprendentemente dif√≠cil:\n",
    "\n",
    "| Desaf√≠o | Descripci√≥n |\n",
    "|---------|-------------|\n",
    "| **Bugs sutiles** | Un signo mal puesto puede hacer que el agente no aprenda |\n",
    "| **Hiperpar√°metros** | Cada algoritmo tiene 10+ par√°metros que ajustar |\n",
    "| **Reproducibilidad** | Peque√±os cambios causan grandes diferencias |\n",
    "| **Eficiencia** | Optimizaciones de bajo nivel para velocidad |\n",
    "| **Debugging** | Es dif√≠cil saber si el c√≥digo est√° mal o el algoritmo es lento |\n",
    "\n",
    "## La Soluci√≥n: Stable-Baselines3\n",
    "\n",
    "**Stable-Baselines3 (SB3)** es una librer√≠a que proporciona implementaciones fiables y probadas de los algoritmos de RL m√°s importantes.\n",
    "\n",
    "### Caracter√≠sticas\n",
    "\n",
    "| Caracter√≠stica | Beneficio |\n",
    "|----------------|----------|\n",
    "| **Implementaciones correctas** | Probadas en benchmarks, sin bugs |\n",
    "| **API unificada** | Todos los algoritmos tienen la misma interfaz |\n",
    "| **Integraci√≥n con Gymnasium** | Funciona directamente con cualquier entorno |\n",
    "| **PyTorch backend** | F√°cil de extender y modificar |\n",
    "| **Documentaci√≥n excelente** | Tutoriales, ejemplos, API reference |\n",
    "| **Comunidad activa** | Mantenido y actualizado regularmente |\n",
    "\n",
    "### Cu√°ndo Usar SB3 vs Implementaci√≥n Propia\n",
    "\n",
    "| Usar SB3 | Implementar t√∫ mismo |\n",
    "|----------|---------------------|\n",
    "| Proyectos reales | Aprender c√≥mo funcionan los algoritmos |\n",
    "| Necesitas resultados r√°pidos | Investigaci√≥n de nuevos algoritmos |\n",
    "| Comparar algoritmos | Modificaciones profundas al algoritmo |\n",
    "| Producci√≥n | Prop√≥sitos educativos |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='2-instalacion'></a>\n",
    "# 2. Instalaci√≥n y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALACI√ìN\n",
    "# ===========\n",
    "# Ejecutar solo si no est√°n instaladas las dependencias\n",
    "\n",
    "# Stable-Baselines3 (incluye PyTorch si no lo tienes)\n",
    "# !pip install stable-baselines3[extra]\n",
    "\n",
    "# Para entornos Box2D (LunarLander, BipedalWalker)\n",
    "# !pip install gymnasium[box2d]\n",
    "\n",
    "# Para visualizaci√≥n de entrenamiento\n",
    "# !pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "# =======\n",
    "\n",
    "# Stable-Baselines3\n",
    "try:\n",
    "    from stable_baselines3 import PPO, A2C, DQN, SAC, TD3\n",
    "    from stable_baselines3.common.env_util import make_vec_env\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "    from stable_baselines3.common.monitor import Monitor\n",
    "    SB3_AVAILABLE = True\n",
    "    print(\"‚úÖ Stable-Baselines3 disponible\")\n",
    "except ImportError:\n",
    "    SB3_AVAILABLE = False\n",
    "    print(\"‚ùå Stable-Baselines3 no instalado. Ejecuta: pip install stable-baselines3[extra]\")\n",
    "\n",
    "# Gymnasium\n",
    "import gymnasium as gym\n",
    "print(\"‚úÖ Gymnasium disponible\")\n",
    "\n",
    "# Utilidades\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Configuraci√≥n para Windows\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "print(\"\\n‚úÖ Imports completados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='3-primer-agente'></a>\n",
    "# 3. Tu Primer Agente con SB3\n",
    "\n",
    "Entrenar un agente con SB3 requiere solo **3 l√≠neas de c√≥digo**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TU PRIMER AGENTE EN 3 L√çNEAS\n",
    "# ============================\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    # 1. CREAR EL MODELO\n",
    "    # - PPO: Algoritmo (Proximal Policy Optimization)\n",
    "    # - \"MlpPolicy\": Red neuronal fully-connected (Multi-Layer Perceptron)\n",
    "    # - \"CartPole-v1\": Entorno de Gymnasium\n",
    "    model = PPO(\"MlpPolicy\", \"CartPole-v1\", verbose=1)\n",
    "\n",
    "    # 2. ENTRENAR\n",
    "    # - total_timesteps: N√∫mero total de pasos de interacci√≥n\n",
    "    # - CartPole se resuelve en ~20,000 pasos con PPO\n",
    "    model.learn(total_timesteps=20_000)\n",
    "\n",
    "    # 3. EVALUAR\n",
    "    # - Crea un entorno nuevo para evaluar\n",
    "    # - Ejecuta 10 episodios y calcula estad√≠sticas\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    print(f\"\\nüéØ Recompensa media: {mean_reward:.2f} ¬± {std_reward:.2f}\")\n",
    "    print(f\"   (M√°ximo posible en CartPole: 500)\")\n",
    "    env.close()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Instala stable-baselines3 para ejecutar este c√≥digo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VER EL AGENTE EN ACCI√ìN\n",
    "# =======================\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    # Crear entorno con renderizado\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    \n",
    "    # Ejecutar un episodio\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(500):\n",
    "        # El modelo predice la acci√≥n\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Ejecutar acci√≥n\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nüéÆ Episodio completado: {total_reward} pasos\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='4-algoritmos'></a>\n",
    "# 4. Algoritmos Disponibles\n",
    "\n",
    "SB3 incluye los algoritmos m√°s importantes de RL moderno:\n",
    "\n",
    "## Tabla Resumen\n",
    "\n",
    "| Algoritmo | Tipo | Acciones | Uso Principal |\n",
    "|-----------|------|----------|---------------|\n",
    "| **PPO** | On-policy, Actor-Critic | Discretas y Continuas | General, muy estable |\n",
    "| **A2C** | On-policy, Actor-Critic | Discretas y Continuas | M√°s simple que PPO |\n",
    "| **DQN** | Off-policy, Value-based | Solo Discretas | Juegos, estados discretos |\n",
    "| **SAC** | Off-policy, Actor-Critic | Solo Continuas | Rob√≥tica, control |\n",
    "| **TD3** | Off-policy, Actor-Critic | Solo Continuas | Rob√≥tica, alternativa a SAC |\n",
    "| **DDPG** | Off-policy, Actor-Critic | Solo Continuas | Predecesor de TD3/SAC |\n",
    "| **HER** | Extensi√≥n | Con los anteriores | Tareas con metas |\n",
    "\n",
    "## ¬øCu√°l Elegir?\n",
    "\n",
    "```\n",
    "¬øTu entorno tiene acciones DISCRETAS o CONTINUAS?\n",
    "                    |\n",
    "        +-----------+-----------+\n",
    "        |                       |\n",
    "    DISCRETAS                CONTINUAS\n",
    "        |                       |\n",
    "  ¬øNecesitas                ¬øNecesitas\n",
    "  sample efficiency?        sample efficiency?\n",
    "        |                       |\n",
    "    +---+---+               +---+---+\n",
    "    |       |               |       |\n",
    "   S√≠      No              S√≠      No\n",
    "    |       |               |       |\n",
    "   DQN    PPO             SAC     PPO\n",
    "                          TD3\n",
    "```\n",
    "\n",
    "### Recomendaciones Pr√°cticas\n",
    "\n",
    "| Situaci√≥n | Algoritmo | Raz√≥n |\n",
    "|-----------|-----------|-------|\n",
    "| **No sabes cu√°l usar** | PPO | Funciona bien casi siempre |\n",
    "| **Videojuegos (Atari)** | DQN | Dise√±ado para esto |\n",
    "| **Rob√≥tica / Control** | SAC o TD3 | Eficientes con acciones continuas |\n",
    "| **Muchos entornos paralelos** | PPO o A2C | Aprovechan bien la paralelizaci√≥n |\n",
    "| **Poca experiencia disponible** | SAC o DQN | Reutilizan experiencias (off-policy) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARACI√ìN DE ALGORITMOS\n",
    "# =========================\n",
    "# Entrenamos varios algoritmos en el mismo entorno y comparamos\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    from stable_baselines3 import PPO, A2C, DQN\n",
    "    \n",
    "    # Entorno simple para comparar\n",
    "    env_id = \"CartPole-v1\"\n",
    "    timesteps = 10_000  # Pocos pasos para demo r√°pida\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    # PPO\n",
    "    print(\"Entrenando PPO...\")\n",
    "    model_ppo = PPO(\"MlpPolicy\", env_id, verbose=0)\n",
    "    model_ppo.learn(total_timesteps=timesteps)\n",
    "    mean_r, _ = evaluate_policy(model_ppo, gym.make(env_id), n_eval_episodes=10, warn=False)\n",
    "    resultados[\"PPO\"] = mean_r\n",
    "    print(f\"  PPO: {mean_r:.1f}\")\n",
    "    \n",
    "    # A2C\n",
    "    print(\"Entrenando A2C...\")\n",
    "    model_a2c = A2C(\"MlpPolicy\", env_id, verbose=0)\n",
    "    model_a2c.learn(total_timesteps=timesteps)\n",
    "    mean_r, _ = evaluate_policy(model_a2c, gym.make(env_id), n_eval_episodes=10, warn=False)\n",
    "    resultados[\"A2C\"] = mean_r\n",
    "    print(f\"  A2C: {mean_r:.1f}\")\n",
    "    \n",
    "    # DQN\n",
    "    print(\"Entrenando DQN...\")\n",
    "    model_dqn = DQN(\"MlpPolicy\", env_id, verbose=0)\n",
    "    model_dqn.learn(total_timesteps=timesteps)\n",
    "    mean_r, _ = evaluate_policy(model_dqn, gym.make(env_id), n_eval_episodes=10, warn=False)\n",
    "    resultados[\"DQN\"] = mean_r\n",
    "    print(f\"  DQN: {mean_r:.1f}\")\n",
    "    \n",
    "    # Visualizar\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(resultados.keys(), resultados.values(), color=['blue', 'green', 'orange'])\n",
    "    plt.ylabel(\"Recompensa Media\")\n",
    "    plt.title(f\"Comparaci√≥n de Algoritmos en {env_id} ({timesteps:,} pasos)\")\n",
    "    plt.axhline(y=500, color='red', linestyle='--', label='M√°ximo')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='5-politicas'></a>\n",
    "# 5. Pol√≠ticas (Redes Neuronales)\n",
    "\n",
    "La \"pol√≠tica\" en SB3 es la red neuronal que decide las acciones.\n",
    "\n",
    "## Pol√≠ticas Predefinidas\n",
    "\n",
    "| Pol√≠tica | Descripci√≥n | Uso |\n",
    "|----------|-------------|-----|\n",
    "| `MlpPolicy` | Red fully-connected | Estados vectoriales (ej: CartPole) |\n",
    "| `CnnPolicy` | Red convolucional | Estados imagen (ej: Atari) |\n",
    "| `MultiInputPolicy` | Combina MLP + CNN | Estados mixtos (Dict) |\n",
    "\n",
    "## Arquitectura por Defecto de MlpPolicy\n",
    "\n",
    "```\n",
    "Observaci√≥n (estado)\n",
    "        ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Linear(obs_dim, 64)‚îÇ ‚Üê Capa 1\n",
    "‚îÇ     + Tanh        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Linear(64, 64)    ‚îÇ ‚Üê Capa 2\n",
    "‚îÇ     + Tanh        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Linear(64, n_act) ‚îÇ ‚Üê Capa de salida\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚Üì\n",
    "Acci√≥n (o distribuci√≥n de acciones)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERSONALIZAR LA ARQUITECTURA\n",
    "# ============================\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    # Definir arquitectura custom\n",
    "    # - net_arch: Lista de capas para policy y value networks\n",
    "    # - [dict(pi=[...], vf=[...])] separa las arquitecturas\n",
    "    \n",
    "    policy_kwargs = dict(\n",
    "        net_arch=[128, 128, 64]  # 3 capas: 128, 128, 64 neuronas\n",
    "    )\n",
    "    \n",
    "    # Crear modelo con arquitectura custom\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        \"CartPole-v1\",\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Arquitectura de la pol√≠tica:\")\n",
    "    print(model.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARQUITECTURAS SEPARADAS PARA ACTOR Y CRITIC\n",
    "# ============================================\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    # En algoritmos Actor-Critic (PPO, A2C), puedes separar las arquitecturas\n",
    "    \n",
    "    policy_kwargs = dict(\n",
    "        net_arch=dict(\n",
    "            pi=[64, 64],    # Actor (pol√≠tica): 2 capas de 64\n",
    "            vf=[128, 128]   # Critic (valor): 2 capas de 128\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        \"CartPole-v1\",\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Modelo con arquitecturas separadas creado\")\n",
    "    print(f\"   Actor: 2 capas de 64\")\n",
    "    print(f\"   Critic: 2 capas de 128\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='6-entrenamiento'></a>\n",
    "# 6. Entrenamiento Avanzado\n",
    "\n",
    "## Entornos Vectorizados\n",
    "\n",
    "Para entrenar m√°s r√°pido, usa m√∫ltiples entornos en paralelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRENAMIENTO CON ENTORNOS PARALELOS\n",
    "# ====================================\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    from stable_baselines3.common.env_util import make_vec_env\n",
    "    \n",
    "    # Crear 4 entornos paralelos\n",
    "    # - Cada entorno recolecta experiencias simult√°neamente\n",
    "    # - El entrenamiento es ~4x m√°s r√°pido\n",
    "    vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "    \n",
    "    # PPO funciona especialmente bien con m√∫ltiples entornos\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        verbose=1,\n",
    "        n_steps=1024,      # Pasos por entorno antes de actualizar\n",
    "        batch_size=64,     # Tama√±o del minibatch\n",
    "        n_epochs=10,       # √âpocas de optimizaci√≥n por actualizaci√≥n\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüöÄ Entrenando con {vec_env.num_envs} entornos paralelos...\")\n",
    "    model.learn(total_timesteps=20_000)\n",
    "    \n",
    "    # Evaluar\n",
    "    mean_r, std_r = evaluate_policy(model, gym.make(\"CartPole-v1\"), n_eval_episodes=10)\n",
    "    print(f\"\\nüéØ Resultado: {mean_r:.1f} ¬± {std_r:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='7-callbacks'></a>\n",
    "# 7. Callbacks y Logging\n",
    "\n",
    "Los callbacks permiten ejecutar c√≥digo durante el entrenamiento:\n",
    "- Guardar checkpoints\n",
    "- Evaluar peri√≥dicamente\n",
    "- Early stopping\n",
    "- Logging personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALLBACKS B√ÅSICOS\n",
    "# =================\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    from stable_baselines3.common.callbacks import (\n",
    "        EvalCallback,\n",
    "        CheckpointCallback,\n",
    "        CallbackList\n",
    "    )\n",
    "    \n",
    "    # Directorio para guardar\n",
    "    log_dir = \"./logs/\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. CheckpointCallback: Guarda el modelo cada N pasos\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=5000,              # Guardar cada 5000 pasos\n",
    "        save_path=log_dir,           # Directorio\n",
    "        name_prefix=\"ppo_cartpole\"   # Prefijo del archivo\n",
    "    )\n",
    "    \n",
    "    # 2. EvalCallback: Eval√∫a y guarda el mejor modelo\n",
    "    eval_env = gym.make(\"CartPole-v1\")\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=log_dir,   # Guarda el mejor\n",
    "        log_path=log_dir,               # Logs de evaluaci√≥n\n",
    "        eval_freq=2000,                 # Evaluar cada 2000 pasos\n",
    "        n_eval_episodes=5,              # 5 episodios por evaluaci√≥n\n",
    "        deterministic=True\n",
    "    )\n",
    "    \n",
    "    # Combinar callbacks\n",
    "    callbacks = CallbackList([checkpoint_callback, eval_callback])\n",
    "    \n",
    "    # Entrenar con callbacks\n",
    "    model = PPO(\"MlpPolicy\", \"CartPole-v1\", verbose=1)\n",
    "    model.learn(total_timesteps=20_000, callback=callbacks)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Modelos guardados en {log_dir}\")\n",
    "    print(f\"   Archivos: {os.listdir(log_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSORBOARD PARA VISUALIZACI√ìN\n",
    "# ==============================\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    # SB3 integra autom√°ticamente con TensorBoard\n",
    "    \n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        \"CartPole-v1\",\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./tensorboard_logs/\"  # Directorio de logs\n",
    "    )\n",
    "    \n",
    "    # Entrenar (los logs se guardan autom√°ticamente)\n",
    "    model.learn(total_timesteps=10_000)\n",
    "    \n",
    "    print(\"\\nüìä Para visualizar en TensorBoard:\")\n",
    "    print(\"   tensorboard --logdir=./tensorboard_logs/\")\n",
    "    print(\"   Luego abre http://localhost:6006 en tu navegador\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='8-guardar-cargar'></a>\n",
    "# 8. Guardar y Cargar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUARDAR Y CARGAR MODELOS\n",
    "# ========================\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    # Entrenar un modelo\n",
    "    model = PPO(\"MlpPolicy\", \"CartPole-v1\", verbose=0)\n",
    "    model.learn(total_timesteps=10_000)\n",
    "    \n",
    "    # GUARDAR\n",
    "    # -------\n",
    "    model.save(\"ppo_cartpole\")  # Guarda como ppo_cartpole.zip\n",
    "    print(\"‚úÖ Modelo guardado: ppo_cartpole.zip\")\n",
    "    \n",
    "    # Eliminar el modelo de memoria\n",
    "    del model\n",
    "    \n",
    "    # CARGAR\n",
    "    # ------\n",
    "    model = PPO.load(\"ppo_cartpole\")  # Carga desde ppo_cartpole.zip\n",
    "    print(\"‚úÖ Modelo cargado\")\n",
    "    \n",
    "    # Verificar que funciona\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    mean_r, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "    print(f\"üéØ Recompensa tras cargar: {mean_r:.1f}\")\n",
    "    \n",
    "    # CONTINUAR ENTRENAMIENTO\n",
    "    # -----------------------\n",
    "    # Necesitas pasar el entorno al cargar\n",
    "    model = PPO.load(\"ppo_cartpole\", env=gym.make(\"CartPole-v1\"))\n",
    "    model.learn(total_timesteps=5_000)  # Contin√∫a entrenando\n",
    "    print(\"‚úÖ Entrenamiento continuado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='9-evaluacion'></a>\n",
    "# 9. Evaluaci√≥n de Agentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUACI√ìN COMPLETA\n",
    "# ===================\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = PPO(\"MlpPolicy\", \"CartPole-v1\", verbose=0)\n",
    "    model.learn(total_timesteps=20_000)\n",
    "    \n",
    "    # Crear entorno de evaluaci√≥n\n",
    "    eval_env = gym.make(\"CartPole-v1\")\n",
    "    \n",
    "    # Evaluar con estad√≠sticas\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model,\n",
    "        eval_env,\n",
    "        n_eval_episodes=20,       # N√∫mero de episodios\n",
    "        deterministic=True,        # Acciones deterministas (sin exploraci√≥n)\n",
    "        return_episode_rewards=False\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Evaluaci√≥n (20 episodios):\")\n",
    "    print(f\"   Recompensa media: {mean_reward:.2f}\")\n",
    "    print(f\"   Desviaci√≥n est√°ndar: {std_reward:.2f}\")\n",
    "    print(f\"   Rango esperado: [{mean_reward-2*std_reward:.1f}, {mean_reward+2*std_reward:.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUACI√ìN DETALLADA CON HISTORIAL\n",
    "# ===================================\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    # Obtener recompensas individuales\n",
    "    episode_rewards, episode_lengths = evaluate_policy(\n",
    "        model,\n",
    "        eval_env,\n",
    "        n_eval_episodes=20,\n",
    "        return_episode_rewards=True\n",
    "    )\n",
    "    \n",
    "    # Visualizar distribuci√≥n\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].hist(episode_rewards, bins=10, edgecolor='black')\n",
    "    axes[0].axvline(np.mean(episode_rewards), color='red', linestyle='--', label=f'Media: {np.mean(episode_rewards):.1f}')\n",
    "    axes[0].set_xlabel(\"Recompensa por Episodio\")\n",
    "    axes[0].set_ylabel(\"Frecuencia\")\n",
    "    axes[0].set_title(\"Distribuci√≥n de Recompensas\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].plot(episode_rewards, 'o-')\n",
    "    axes[1].axhline(np.mean(episode_rewards), color='red', linestyle='--')\n",
    "    axes[1].set_xlabel(\"Episodio\")\n",
    "    axes[1].set_ylabel(\"Recompensa\")\n",
    "    axes[1].set_title(\"Recompensa por Episodio\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='10-hiperparametros'></a>\n",
    "# 10. Hiperpar√°metros\n",
    "\n",
    "Cada algoritmo tiene sus propios hiperpar√°metros. Aqu√≠ los m√°s importantes:\n",
    "\n",
    "## PPO\n",
    "\n",
    "| Par√°metro | Default | Descripci√≥n |\n",
    "|-----------|---------|-------------|\n",
    "| `learning_rate` | 3e-4 | Tasa de aprendizaje |\n",
    "| `n_steps` | 2048 | Pasos por actualizaci√≥n |\n",
    "| `batch_size` | 64 | Tama√±o del minibatch |\n",
    "| `n_epochs` | 10 | √âpocas por actualizaci√≥n |\n",
    "| `gamma` | 0.99 | Factor de descuento |\n",
    "| `clip_range` | 0.2 | Rango de clipping de PPO |\n",
    "| `ent_coef` | 0.0 | Coeficiente de entrop√≠a |\n",
    "\n",
    "## DQN\n",
    "\n",
    "| Par√°metro | Default | Descripci√≥n |\n",
    "|-----------|---------|-------------|\n",
    "| `learning_rate` | 1e-4 | Tasa de aprendizaje |\n",
    "| `buffer_size` | 1M | Tama√±o del replay buffer |\n",
    "| `batch_size` | 32 | Tama√±o del batch |\n",
    "| `gamma` | 0.99 | Factor de descuento |\n",
    "| `exploration_fraction` | 0.1 | Fracci√≥n de exploraci√≥n |\n",
    "| `target_update_interval` | 10000 | Actualizaci√≥n de target network |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJEMPLO: PPO CON HIPERPAR√ÅMETROS CUSTOM\n",
    "# =======================================\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        \"CartPole-v1\",\n",
    "        \n",
    "        # Hiperpar√°metros de aprendizaje\n",
    "        learning_rate=1e-3,      # M√°s alto = aprende m√°s r√°pido pero menos estable\n",
    "        gamma=0.99,              # Factor de descuento\n",
    "        \n",
    "        # Hiperpar√°metros de muestreo\n",
    "        n_steps=1024,            # Pasos antes de cada actualizaci√≥n\n",
    "        batch_size=64,           # Tama√±o del minibatch\n",
    "        n_epochs=10,             # √âpocas de optimizaci√≥n\n",
    "        \n",
    "        # Hiperpar√°metros de PPO espec√≠ficos\n",
    "        clip_range=0.2,          # Rango de clipping\n",
    "        ent_coef=0.01,           # Bonus por entrop√≠a (promueve exploraci√≥n)\n",
    "        \n",
    "        # Otros\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.learn(total_timesteps=20_000)\n",
    "    \n",
    "    mean_r, _ = evaluate_policy(model, gym.make(\"CartPole-v1\"), n_eval_episodes=10)\n",
    "    print(f\"\\nüéØ Resultado: {mean_r:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='11-proyectos'></a>\n",
    "# 11. Proyectos Pr√°cticos\n",
    "\n",
    "Ahora conectamos con los proyectos en `04_proyectos_avanzados/`:\n",
    "\n",
    "| Proyecto | Archivo | Algoritmo | Entorno |\n",
    "|----------|---------|-----------|--------|\n",
    "| LunarLander | `lunarlander/lunarlander_sb3.py` | PPO, DQN, A2C | LunarLander-v2 |\n",
    "| Highway | `highway/highway_conduccion.py` | DQN, PPO | highway-env |\n",
    "| MiniGrid | `minigrid/minigrid_navegacion.py` | PPO | MiniGrid |\n",
    "| PyBullet | `pybullet/pybullet_robotica.py` | SAC, TD3 | PyBullet |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJEMPLO: LUNARLANDER\n",
    "# ====================\n",
    "# Este es uno de los entornos m√°s populares para aprender RL\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    try:\n",
    "        # Crear entorno (requiere gymnasium[box2d])\n",
    "        env = gym.make(\"LunarLander-v2\")\n",
    "        \n",
    "        print(\"üöÄ LunarLander-v2\")\n",
    "        print(f\"   Observaciones: {env.observation_space}\")\n",
    "        print(f\"   Acciones: {env.action_space}\")\n",
    "        print(f\"   Acciones disponibles:\")\n",
    "        print(f\"     0: No hacer nada\")\n",
    "        print(f\"     1: Motor izquierdo\")\n",
    "        print(f\"     2: Motor principal\")\n",
    "        print(f\"     3: Motor derecho\")\n",
    "        \n",
    "        # Entrenar con PPO\n",
    "        print(\"\\nüèãÔ∏è Entrenando PPO (esto toma ~1-2 minutos)...\")\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "        model.learn(total_timesteps=50_000)\n",
    "        \n",
    "        # Evaluar\n",
    "        mean_r, std_r = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "        print(f\"\\nüéØ Resultado: {mean_r:.1f} ¬± {std_r:.1f}\")\n",
    "        print(f\"   (>200 = aterrizaje exitoso)\")\n",
    "        \n",
    "        env.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "        print(\"   Instala Box2D: pip install gymnasium[box2d]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJEMPLO: ACCIONES CONTINUAS CON SAC\n",
    "# ====================================\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    try:\n",
    "        # Pendulum tiene acciones continuas\n",
    "        env = gym.make(\"Pendulum-v1\")\n",
    "        \n",
    "        print(\"üéØ Pendulum-v1\")\n",
    "        print(f\"   Observaciones: {env.observation_space}\")\n",
    "        print(f\"   Acciones: {env.action_space}\")\n",
    "        print(f\"   Acci√≥n: torque continuo en [-2, 2]\")\n",
    "        \n",
    "        # SAC es ideal para acciones continuas\n",
    "        print(\"\\nüèãÔ∏è Entrenando SAC...\")\n",
    "        model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "        model.learn(total_timesteps=10_000)\n",
    "        \n",
    "        # Evaluar\n",
    "        mean_r, std_r = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "        print(f\"\\nüéØ Resultado: {mean_r:.1f} ¬± {std_r:.1f}\")\n",
    "        print(f\"   (M√°ximo te√≥rico: 0, m√°s negativo = peor)\")\n",
    "        \n",
    "        env.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Resumen\n",
    "\n",
    "## Lo que Aprendimos\n",
    "\n",
    "| Tema | Concepto Clave |\n",
    "|------|----------------|\n",
    "| **SB3 B√°sico** | `model = PPO(...); model.learn(); model.predict()` |\n",
    "| **Algoritmos** | PPO (general), DQN (discreto), SAC (continuo) |\n",
    "| **Pol√≠ticas** | `MlpPolicy` (vectores), `CnnPolicy` (im√°genes) |\n",
    "| **Paralelo** | `make_vec_env(env_id, n_envs=N)` |\n",
    "| **Callbacks** | `EvalCallback`, `CheckpointCallback` |\n",
    "| **Guardar** | `model.save()`, `Model.load()` |\n",
    "\n",
    "## Pr√≥ximos Pasos\n",
    "\n",
    "1. **Explorar los proyectos** en `04_proyectos_avanzados/`\n",
    "2. **Experimentar con hiperpar√°metros** en diferentes entornos\n",
    "3. **Crear tu propio entorno** y entrenarlo con SB3\n",
    "4. **Leer la documentaci√≥n**: https://stable-baselines3.readthedocs.io/\n",
    "\n",
    "---\n",
    "\n",
    "## Recursos\n",
    "\n",
    "- **Documentaci√≥n SB3**: https://stable-baselines3.readthedocs.io/\n",
    "- **RL Baselines Zoo**: https://github.com/DLR-RM/rl-baselines3-zoo (hiperpar√°metros optimizados)\n",
    "- **Hugging Face Hub**: https://huggingface.co/models?library=stable-baselines3 (modelos pre-entrenados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
