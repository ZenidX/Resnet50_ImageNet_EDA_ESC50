{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flappy Bird con Deep Reinforcement Learning\n",
    "\n",
    "## Objetivos de este Notebook\n",
    "\n",
    "En este notebook aprender√°s:\n",
    "\n",
    "1. **Timing cr√≠tico en RL**: Por qu√© Flappy Bird es un desaf√≠o especial\n",
    "2. **Observaci√≥n Simple vs RGB**: Trade-offs de diferentes representaciones\n",
    "3. **DQN vs PPO**: Cu√°ndo usar cada algoritmo\n",
    "4. **Variantes de arquitectura**: Dueling DQN, Frame Stacking\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisitos\n",
    "\n",
    "```bash\n",
    "pip install flappy-bird-gymnasium stable-baselines3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# A√±adir directorio al path\n",
    "FLAPPY_DIR = Path().absolute()\n",
    "if str(FLAPPY_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(FLAPPY_DIR))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Gymnasium\n",
    "import gymnasium as gym\n",
    "try:\n",
    "    import flappy_bird_gymnasium\n",
    "    FLAPPY_AVAILABLE = True\n",
    "    print(\"Flappy Bird Gymnasium disponible\")\n",
    "except ImportError:\n",
    "    FLAPPY_AVAILABLE = False\n",
    "    print(\"Instalar con: pip install flappy-bird-gymnasium\")\n",
    "\n",
    "# Stable-Baselines3\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "\n",
    "# PyTorch (para variantes custom)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Directorio: {FLAPPY_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Descripci√≥n del Juego\n",
    "\n",
    "## ¬øQu√© es Flappy Bird?\n",
    "\n",
    "Un juego donde controlas un p√°jaro que debe pasar entre tubos:\n",
    "\n",
    "```\n",
    "    ‚ñà‚ñà‚ñà‚ñà                ‚ñà‚ñà‚ñà‚ñà\n",
    "    ‚ñà‚ñà‚ñà‚ñà                ‚ñà‚ñà‚ñà‚ñà\n",
    "    ‚ñà‚ñà‚ñà‚ñà    üê¶          ‚ñà‚ñà‚ñà‚ñà\n",
    "    ‚ñà‚ñà‚ñà‚ñà   ‚Üêp√°jaro      ‚ñà‚ñà‚ñà‚ñà\n",
    "                        ‚ñà‚ñà‚ñà‚ñà\n",
    "    ‚ñà‚ñà‚ñà‚ñà                ‚ñà‚ñà‚ñà‚ñà\n",
    "    ‚ñà‚ñà‚ñà‚ñà                ‚ñà‚ñà‚ñà‚ñà\n",
    "    ‚ñà‚ñà‚ñà‚ñà                ‚ñà‚ñà‚ñà‚ñà\n",
    "```\n",
    "\n",
    "### ¬øPor qu√© es dif√≠cil para RL?\n",
    "\n",
    "1. **Timing cr√≠tico**: El momento exacto de saltar importa mucho\n",
    "2. **Recompensa sparse**: Solo +1 al pasar un tubo, 0 en otros pasos\n",
    "3. **Muerte instant√°nea**: Un error = game over\n",
    "4. **F√≠sica simple pero precisa**: La gravedad y el salto son predecibles pero requieren precisi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorar el entorno\n",
    "if FLAPPY_AVAILABLE:\n",
    "    env = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ENTORNO: Flappy Bird\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nEspacio de observaci√≥n: {env.observation_space}\")\n",
    "    print(f\"Espacio de acciones: {env.action_space}\")\n",
    "    \n",
    "    # Ver una observaci√≥n\n",
    "    obs, info = env.reset()\n",
    "    print(f\"\\nObservaci√≥n (12D LiDAR):\")\n",
    "    print(f\"  Shape: {obs.shape}\")\n",
    "    print(f\"  Valores: {obs}\")\n",
    "    \n",
    "    env.close()\n",
    "else:\n",
    "    print(\"Flappy Bird no disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. An√°lisis de la Arquitectura\n",
    "\n",
    "## 2.1 Estado / Observaci√≥n\n",
    "\n",
    "### Opci√≥n A: Observaci√≥n Simple (12D LiDAR)\n",
    "\n",
    "Cuando `use_lidar=True`, el entorno devuelve un vector de 12 valores:\n",
    "\n",
    "| √çndice | Descripci√≥n |\n",
    "|--------|-------------|\n",
    "| 0-3 | Raycast horizontal (distancia a tubos) |\n",
    "| 4-7 | Raycast vertical (gap de tubos) |\n",
    "| 8-9 | Posici√≥n y velocidad del p√°jaro |\n",
    "| 10-11 | Informaci√≥n del siguiente tubo |\n",
    "\n",
    "### Opci√≥n B: Observaci√≥n RGB (288√ó512√ó3)\n",
    "\n",
    "Cuando `use_lidar=False`, el entorno devuelve la imagen completa del juego.\n",
    "\n",
    "### Comparaci√≥n\n",
    "\n",
    "| Aspecto | Simple (12D) | RGB (288√ó512) |\n",
    "|---------|--------------|---------------|\n",
    "| Velocidad | Muy r√°pida | Lenta (CNN) |\n",
    "| Informaci√≥n | Suficiente | Completa |\n",
    "| Red necesaria | MLP simple | CNN profunda |\n",
    "| Memoria | ~1 KB | ~430 KB por frame |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar observaciones\n",
    "if FLAPPY_AVAILABLE:\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPARACI√ìN DE OBSERVACIONES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Simple\n",
    "    env_simple = gym.make(\"FlappyBird-v0\", use_lidar=True)\n",
    "    obs_simple, _ = env_simple.reset()\n",
    "    print(f\"\\nObservaci√≥n SIMPLE (LiDAR):\")\n",
    "    print(f\"  Shape: {obs_simple.shape}\")\n",
    "    print(f\"  Tipo: {obs_simple.dtype}\")\n",
    "    print(f\"  Memoria: {obs_simple.nbytes} bytes\")\n",
    "    env_simple.close()\n",
    "    \n",
    "    # RGB\n",
    "    env_rgb = gym.make(\"FlappyBird-v0\", use_lidar=False)\n",
    "    obs_rgb, _ = env_rgb.reset()\n",
    "    print(f\"\\nObservaci√≥n RGB:\")\n",
    "    print(f\"  Shape: {obs_rgb.shape}\")\n",
    "    print(f\"  Tipo: {obs_rgb.dtype}\")\n",
    "    print(f\"  Memoria: {obs_rgb.nbytes:,} bytes ({obs_rgb.nbytes/1024:.1f} KB)\")\n",
    "    env_rgb.close()\n",
    "    \n",
    "    # Visualizar\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Simple como barras\n",
    "    axes[0].barh(range(12), obs_simple)\n",
    "    axes[0].set_yticks(range(12))\n",
    "    axes[0].set_yticklabels([f'Dim {i}' for i in range(12)])\n",
    "    axes[0].set_xlabel('Valor')\n",
    "    axes[0].set_title('Observaci√≥n Simple (12D)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # RGB como imagen\n",
    "    axes[1].imshow(obs_rgb)\n",
    "    axes[1].set_title('Observaci√≥n RGB')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Espacio de Acciones\n",
    "\n",
    "**Solo 2 acciones**:\n",
    "\n",
    "| Acci√≥n | Efecto |\n",
    "|--------|--------|\n",
    "| 0 | No hacer nada (caer por gravedad) |\n",
    "| 1 | Saltar (impulso hacia arriba) |\n",
    "\n",
    "### ¬øPor qu√© es dif√≠cil con solo 2 acciones?\n",
    "\n",
    "El **timing** es todo. Saltar demasiado pronto o tarde = muerte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Funci√≥n de Recompensa\n",
    "\n",
    "| Evento | Recompensa |\n",
    "|--------|------------|\n",
    "| Pasar un tubo | +1.0 |\n",
    "| Cada paso | +0.1 (peque√±o bonus por sobrevivir) |\n",
    "| Morir | -1.0 (o -5.0 seg√∫n configuraci√≥n) |\n",
    "\n",
    "### Problema: Recompensa Sparse\n",
    "\n",
    "El agente puede pasar muchos pasos sin recompensa positiva significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 ¬øDQN o PPO?\n",
    "\n",
    "### Pregunta: ¬øQu√© algoritmo funciona mejor para Flappy Bird?\n",
    "\n",
    "| Aspecto | DQN | PPO |\n",
    "|---------|-----|-----|\n",
    "| Tipo | Value-based | Policy Gradient |\n",
    "| Acciones | Solo discretas | Discretas y continuas |\n",
    "| Sample efficiency | Mejor (replay buffer) | Peor (on-policy) |\n",
    "| Estabilidad | Menos estable | Muy estable |\n",
    "| Timing cr√≠tico | Regular | **Mejor** |\n",
    "\n",
    "**Conclusi√≥n**: PPO suele funcionar mejor para juegos con timing cr√≠tico porque aprende una **pol√≠tica suave** en lugar de valores Q discretos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. C√≥digo Original\n",
    "\n",
    "Importamos las funciones del archivo original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback para registrar scores\n",
    "class FlappyCallback(BaseCallback):\n",
    "    \"\"\"Callback para registrar scores en Flappy Bird.\"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.scores = []\n",
    "        self.episode_rewards = []\n",
    "        self.best_score = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        for info in self.locals.get('infos', []):\n",
    "            if 'episode' in info:\n",
    "                reward = info['episode']['r']\n",
    "                self.episode_rewards.append(reward)\n",
    "                score = max(0, int(reward))\n",
    "                self.scores.append(score)\n",
    "                if score > self.best_score:\n",
    "                    self.best_score = score\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"  Nuevo r√©cord: {score} tubos!\")\n",
    "        return True\n",
    "\n",
    "\n",
    "def entrenar_flappy(timesteps=50000, algorithm=\"DQN\", use_simple_obs=True):\n",
    "    \"\"\"Entrena un agente en Flappy Bird.\"\"\"\n",
    "    print(f\"\\nEntrenando {algorithm} ({timesteps:,} timesteps)...\")\n",
    "    \n",
    "    env = gym.make(\"FlappyBird-v0\", render_mode=None, use_lidar=use_simple_obs)\n",
    "    env = Monitor(env)\n",
    "    \n",
    "    policy = \"MlpPolicy\" if use_simple_obs else \"CnnPolicy\"\n",
    "    policy_kwargs = {\"net_arch\": [256, 256]} if use_simple_obs else None\n",
    "    \n",
    "    if algorithm == \"DQN\":\n",
    "        model = DQN(\n",
    "            policy, env, policy_kwargs=policy_kwargs, verbose=0,\n",
    "            learning_rate=0.0001, buffer_size=50000,\n",
    "            learning_starts=5000, batch_size=32,\n",
    "            gamma=0.99, exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.01, target_update_interval=1000\n",
    "        )\n",
    "    else:\n",
    "        model = PPO(\n",
    "            policy, env, policy_kwargs=policy_kwargs, verbose=0,\n",
    "            learning_rate=0.0003, n_steps=2048,\n",
    "            batch_size=64, n_epochs=10, gamma=0.99\n",
    "        )\n",
    "    \n",
    "    callback = FlappyCallback(verbose=0)\n",
    "    model.learn(total_timesteps=timesteps, callback=callback, progress_bar=True)\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"  Mejor score: {callback.best_score} tubos\")\n",
    "    return model, callback\n",
    "\n",
    "print(\"Funciones de entrenamiento cargadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Entrenamiento R√°pido (Demo)\n",
    "\n",
    "Entrenamos brevemente para ver el proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAPPY_AVAILABLE:\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENTRENAMIENTO DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Entrenar DQN\n",
    "    model_dqn, cb_dqn = entrenar_flappy(timesteps=30000, algorithm=\"DQN\")\n",
    "else:\n",
    "    print(\"Flappy Bird no disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar progreso\n",
    "if FLAPPY_AVAILABLE and cb_dqn.scores:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    scores = cb_dqn.scores\n",
    "    plt.plot(scores, alpha=0.3, label='Score por episodio')\n",
    "    \n",
    "    if len(scores) > 20:\n",
    "        smoothed = np.convolve(scores, np.ones(20)/20, mode='valid')\n",
    "        plt.plot(range(19, len(scores)), smoothed, 'r-', linewidth=2, label='Media m√≥vil (20)')\n",
    "    \n",
    "    plt.axhline(y=cb_dqn.best_score, color='green', linestyle='--', label=f'Mejor: {cb_dqn.best_score}')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Tubos pasados')\n",
    "    plt.title('Progreso del Entrenamiento (DQN)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. VARIANTES DE ARQUITECTURA\n",
    "\n",
    "## Variante A: DQN vs PPO - Comparaci√≥n Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_dqn_ppo(timesteps=50000):\n",
    "    \"\"\"\n",
    "    Compara DQN vs PPO en Flappy Bird.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"VARIANTE A: DQN vs PPO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    # DQN\n",
    "    model_dqn, cb_dqn = entrenar_flappy(timesteps, \"DQN\")\n",
    "    resultados['DQN'] = {'scores': cb_dqn.scores, 'best': cb_dqn.best_score}\n",
    "    \n",
    "    # PPO\n",
    "    model_ppo, cb_ppo = entrenar_flappy(timesteps, \"PPO\")\n",
    "    resultados['PPO'] = {'scores': cb_ppo.scores, 'best': cb_ppo.best_score}\n",
    "    \n",
    "    # Visualizar\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Curvas\n",
    "    for name, data in resultados.items():\n",
    "        scores = data['scores']\n",
    "        if len(scores) > 20:\n",
    "            smoothed = np.convolve(scores, np.ones(20)/20, mode='valid')\n",
    "            axes[0].plot(smoothed, label=f\"{name} (mejor: {data['best']})\")\n",
    "    \n",
    "    axes[0].set_xlabel('Episodio')\n",
    "    axes[0].set_ylabel('Tubos pasados')\n",
    "    axes[0].set_title('Curvas de Aprendizaje')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Barras finales\n",
    "    names = list(resultados.keys())\n",
    "    bests = [resultados[n]['best'] for n in names]\n",
    "    axes[1].bar(names, bests, color=['blue', 'orange'])\n",
    "    axes[1].set_ylabel('Mejor Score')\n",
    "    axes[1].set_title('Mejor Score por Algoritmo')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "if FLAPPY_AVAILABLE:\n",
    "    resultados_a = comparar_dqn_ppo(timesteps=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variante B: Dueling DQN\n",
    "\n",
    "### Idea\n",
    "Separar la red en dos streams:\n",
    "- **Value stream V(s)**: Valor del estado\n",
    "- **Advantage stream A(s,a)**: Ventaja de cada acci√≥n\n",
    "\n",
    "Q(s,a) = V(s) + A(s,a) - mean(A)\n",
    "\n",
    "### Ventajas\n",
    "- Aprende qu√© estados son valiosos independientemente de la acci√≥n\n",
    "- Mejor para juegos donde a veces la acci√≥n no importa mucho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling DQN: Separa V(s) y A(s,a).\n",
    "    \n",
    "    Q(s,a) = V(s) + (A(s,a) - mean(A(s,:)))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=12, n_actions=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder compartido\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Value stream: V(s)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Un solo valor\n",
    "        )\n",
    "        \n",
    "        # Advantage stream: A(s,a)\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)  # Una ventaja por acci√≥n\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        \n",
    "        value = self.value_stream(features)           # (batch, 1)\n",
    "        advantage = self.advantage_stream(features)   # (batch, n_actions)\n",
    "        \n",
    "        # Combinar: Q = V + (A - mean(A))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "\n",
    "class DuelingDQNAgent:\n",
    "    \"\"\"Agente con Dueling DQN.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=12, n_actions=2):\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.q_network = DuelingDQN(input_size, n_actions).to(self.device)\n",
    "        self.target_network = DuelingDQN(input_size, n_actions).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.0001)\n",
    "        self.memory = deque(maxlen=50000)\n",
    "        self.batch_size = 32\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            return self.q_network(state_t).argmax().item()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states_t = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions_t = torch.LongTensor(actions).to(self.device)\n",
    "        rewards_t = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states_t = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones_t = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        q_values = self.q_network(states_t).gather(1, actions_t.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states_t).max(1)[0]\n",
    "            target = rewards_t + (1 - dones_t) * self.gamma * next_q\n",
    "        \n",
    "        loss = nn.MSELoss()(q_values, target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "# Verificar arquitectura\n",
    "dueling_net = DuelingDQN(input_size=12, n_actions=2)\n",
    "n_params = sum(p.numel() for p in dueling_net.parameters())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VARIANTE B: Dueling DQN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{dueling_net}\")\n",
    "print(f\"\\nPar√°metros totales: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_dueling_dqn(episodes=200):\n",
    "    \"\"\"Entrena Dueling DQN en Flappy Bird.\"\"\"\n",
    "    if not FLAPPY_AVAILABLE:\n",
    "        print(\"Flappy Bird no disponible\")\n",
    "        return None, []\n",
    "    \n",
    "    print(\"\\nEntrenando Dueling DQN...\")\n",
    "    \n",
    "    env = gym.make(\"FlappyBird-v0\", render_mode=None, use_lidar=True)\n",
    "    agent = DuelingDQNAgent(input_size=12, n_actions=2)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        if ep % 10 == 0:\n",
    "            agent.update_target()\n",
    "        \n",
    "        score = max(0, int(total_reward))\n",
    "        scores.append(score)\n",
    "        \n",
    "        if (ep + 1) % 50 == 0:\n",
    "            print(f\"  Ep {ep+1}: Score promedio = {np.mean(scores[-50:]):.1f}\")\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"  Mejor score: {max(scores)}\")\n",
    "    return agent, scores\n",
    "\n",
    "if FLAPPY_AVAILABLE:\n",
    "    agent_dueling, scores_dueling = entrenar_dueling_dqn(episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variante C: Frame Stacking\n",
    "\n",
    "### Idea\n",
    "Apilar varios frames consecutivos para dar informaci√≥n temporal:\n",
    "- El agente ve los √∫ltimos N estados\n",
    "- Puede inferir velocidad y direcci√≥n\n",
    "\n",
    "### Ventajas\n",
    "- Informaci√≥n temporal sin LSTM/RNN\n",
    "- Puede mejorar el timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_con_frame_stacking(timesteps=50000, n_stack=4):\n",
    "    \"\"\"\n",
    "    Entrena DQN con frame stacking.\n",
    "    \n",
    "    Args:\n",
    "        timesteps: Pasos de entrenamiento\n",
    "        n_stack: N√∫mero de frames a apilar\n",
    "    \"\"\"\n",
    "    if not FLAPPY_AVAILABLE:\n",
    "        print(\"Flappy Bird no disponible\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\nEntrenando DQN con Frame Stacking (n={n_stack})...\")\n",
    "    \n",
    "    # Crear entorno con frame stacking\n",
    "    def make_env():\n",
    "        env = gym.make(\"FlappyBird-v0\", render_mode=None, use_lidar=True)\n",
    "        return Monitor(env)\n",
    "    \n",
    "    env = DummyVecEnv([make_env])\n",
    "    env = VecFrameStack(env, n_stack=n_stack)\n",
    "    \n",
    "    # El estado ahora tiene shape (12 * n_stack,) = (48,) para n_stack=4\n",
    "    print(f\"  Observaci√≥n original: 12D\")\n",
    "    print(f\"  Observaci√≥n con stacking: {12 * n_stack}D\")\n",
    "    \n",
    "    model = DQN(\n",
    "        \"MlpPolicy\", env,\n",
    "        policy_kwargs={\"net_arch\": [256, 256]},\n",
    "        verbose=0,\n",
    "        learning_rate=0.0001,\n",
    "        buffer_size=50000,\n",
    "        learning_starts=5000,\n",
    "        batch_size=32,\n",
    "        gamma=0.99\n",
    "    )\n",
    "    \n",
    "    callback = FlappyCallback(verbose=0)\n",
    "    model.learn(total_timesteps=timesteps, callback=callback, progress_bar=True)\n",
    "    \n",
    "    print(f\"  Mejor score: {callback.best_score}\")\n",
    "    return model, callback\n",
    "\n",
    "if FLAPPY_AVAILABLE:\n",
    "    model_stacked, cb_stacked = entrenar_con_frame_stacking(timesteps=30000, n_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variante D: Simple vs RGB - Comparaci√≥n Detallada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_observaciones(timesteps=30000):\n",
    "    \"\"\"\n",
    "    Compara observaci√≥n simple (12D) vs RGB.\n",
    "    \"\"\"\n",
    "    if not FLAPPY_AVAILABLE:\n",
    "        print(\"Flappy Bird no disponible\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"VARIANTE D: Simple vs RGB\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    import time\n",
    "    resultados = {}\n",
    "    \n",
    "    # Simple\n",
    "    print(\"\\nEntrenando con observaci√≥n SIMPLE (12D)...\")\n",
    "    t0 = time.time()\n",
    "    model_simple, cb_simple = entrenar_flappy(timesteps, \"DQN\", use_simple_obs=True)\n",
    "    time_simple = time.time() - t0\n",
    "    resultados['Simple (12D)'] = {\n",
    "        'scores': cb_simple.scores,\n",
    "        'best': cb_simple.best_score,\n",
    "        'time': time_simple\n",
    "    }\n",
    "    \n",
    "    # RGB (m√°s lento)\n",
    "    print(\"\\nEntrenando con observaci√≥n RGB (esto tardar√° m√°s)...\")\n",
    "    t0 = time.time()\n",
    "    model_rgb, cb_rgb = entrenar_flappy(timesteps // 2, \"DQN\", use_simple_obs=False)  # Menos timesteps porque es lento\n",
    "    time_rgb = time.time() - t0\n",
    "    resultados['RGB'] = {\n",
    "        'scores': cb_rgb.scores,\n",
    "        'best': cb_rgb.best_score,\n",
    "        'time': time_rgb\n",
    "    }\n",
    "    \n",
    "    # Tabla comparativa\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n{'Observaci√≥n':<15} {'Mejor Score':<15} {'Tiempo (s)':<15}\")\n",
    "    print(\"-\" * 45)\n",
    "    for name, data in resultados.items():\n",
    "        print(f\"{name:<15} {data['best']:<15} {data['time']:<15.1f}\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "if FLAPPY_AVAILABLE:\n",
    "    resultados_d = comparar_observaciones(timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Comparaci√≥n Final de Todas las Variantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recopilar todos los resultados\n",
    "if FLAPPY_AVAILABLE:\n",
    "    print(\"=\"*60)\n",
    "    print(\"RESUMEN DE TODAS LAS VARIANTES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    variantes = [\n",
    "        (\"DQN Original\", cb_dqn.scores if 'cb_dqn' in dir() else []),\n",
    "        (\"Dueling DQN\", scores_dueling if 'scores_dueling' in dir() else []),\n",
    "        (\"Frame Stacking\", cb_stacked.scores if 'cb_stacked' in dir() and cb_stacked else []),\n",
    "    ]\n",
    "    \n",
    "    # Filtrar variantes con datos\n",
    "    variantes = [(n, s) for n, s in variantes if len(s) > 0]\n",
    "    \n",
    "    if variantes:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Curvas\n",
    "        plt.subplot(1, 2, 1)\n",
    "        for name, scores in variantes:\n",
    "            if len(scores) > 20:\n",
    "                smoothed = np.convolve(scores, np.ones(20)/20, mode='valid')\n",
    "                plt.plot(smoothed, label=f\"{name} (mejor: {max(scores)})\")\n",
    "        plt.xlabel('Episodio')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Comparaci√≥n de Variantes')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Barras\n",
    "        plt.subplot(1, 2, 2)\n",
    "        names = [n for n, _ in variantes]\n",
    "        bests = [max(s) for _, s in variantes]\n",
    "        plt.bar(names, bests)\n",
    "        plt.ylabel('Mejor Score')\n",
    "        plt.title('Mejor Score por Variante')\n",
    "        plt.xticks(rotation=15)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Tabla\n",
    "        print(f\"\\n{'Variante':<20} {'Mejor':<10} {'Promedio':<10}\")\n",
    "        print(\"-\" * 40)\n",
    "        for name, scores in variantes:\n",
    "            print(f\"{name:<20} {max(scores):<10} {np.mean(scores[-20:]):<10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Conclusiones\n",
    "\n",
    "## ¬øQu√© aprendimos?\n",
    "\n",
    "1. **Observaci√≥n Simple vs RGB**:\n",
    "   - La observaci√≥n simple (12D LiDAR) es suficiente y mucho m√°s r√°pida\n",
    "   - RGB es overkill para este juego simple\n",
    "\n",
    "2. **DQN vs PPO**:\n",
    "   - PPO suele ser m√°s estable para juegos con timing cr√≠tico\n",
    "   - DQN puede requerir m√°s tuning\n",
    "\n",
    "3. **Variantes de arquitectura**:\n",
    "   - Dueling DQN puede ayudar cuando el valor del estado es importante\n",
    "   - Frame stacking a√±ade informaci√≥n temporal\n",
    "\n",
    "## Siguientes Pasos\n",
    "\n",
    "- Entrenar por m√°s timesteps (500K+)\n",
    "- Probar Prioritized Experience Replay\n",
    "- Experimentar con diferentes hiperpar√°metros\n",
    "- A√±adir reward shaping\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [Dueling Network Architectures for Deep RL](https://arxiv.org/abs/1511.06581)\n",
    "- [PPO: Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)\n",
    "- [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## üê¶ Variantes de Entrenamiento ‚Äî Flappy Bird\n\nLas variantes en Flappy Bird exploran dos dimensiones independientes:\n- **Algoritmo**: DQN (off-policy) vs PPO (on-policy)\n- **Observaci√≥n**: Simple 12D (LIDAR) vs RGB completo (imagen)\n\nEsto da 4 combinaciones posibles que ilustran c√≥mo cada elecci√≥n afecta la velocidad y calidad del aprendizaje.\n\n| Variante | Algoritmo | Observaci√≥n | Tiempo entreno | Dificultad |\n|----------|-----------|-------------|----------------|------------|\n| A | DQN | Simple 12D | R√°pido | Baja |\n| B | PPO | Simple 12D | R√°pido | Baja |\n| C | DQN | RGB imagen | Lento | Media |\n| D | Comparativa | Simple 12D | ‚Äî | ‚Äî |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Variante A ‚Äî DQN + Observaci√≥n Simple *(implementaci√≥n actual)*\n\n```python\npython flappybird_dqn.py --algorithm DQN --simple\n```\n\n**Observaci√≥n**: vector de 12 valores (sensores LIDAR sint√©ticos)\n- Distancias a tuber√≠as y suelo\n- Velocidad vertical del p√°jaro\n- Posici√≥n relativa\n\n**Algoritmo DQN** (off-policy):\n- Guarda experiencias en un replay buffer (100K)\n- Aprende de muestras aleatorias del buffer ‚Üí decorrelaci√≥n\n- M√°s eficiente en datos: puede reutilizar experiencias viejas\n- Puede sobreestimar Q-values (problema conocido)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante A: DQN + Observaci√≥n Simple\n# from flappybird_dqn import entrenar_flappy\n# model, callback = entrenar_flappy(timesteps=100000, algorithm=\"DQN\", use_simple_obs=True)\n\nprint(\"Variante A: DQN + Observaci√≥n Simple\")\nprint(\"  Observaci√≥n: 12D vector (LIDAR sint√©tico)\")\nprint(\"  Algoritmo: DQN (off-policy, replay buffer)\")\nprint()\nprint(\"Configuraci√≥n DQN:\")\ndqn_config = \"\"\"\nDQN(\n    \"MlpPolicy\",          # Red densa (no CNN)\n    env,\n    learning_rate=0.0001,\n    buffer_size=100000,   # Replay buffer: 100K experiencias\n    learning_starts=10000,\n    batch_size=32,\n    gamma=0.99,\n    exploration_fraction=0.1,  # 10% del tiempo explorando\n    exploration_final_eps=0.01,\n)\n\"\"\"\nprint(dqn_config)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Variante B ‚Äî PPO + Observaci√≥n Simple\n\n```python\npython flappybird_dqn.py --algorithm PPO --simple\n```\n\n**Observaci√≥n**: misma que Variante A (12D vector)\n\n**Algoritmo PPO** (on-policy):\n- No usa replay buffer: aprende solo de experiencias recientes\n- Actualiza la pol√≠tica con el *clipped surrogate objective* ‚Üí estabilidad\n- M√°s robusto pero menos eficiente en datos (descarta experiencias pasadas)\n- Generalmente produce curvas de aprendizaje m√°s suaves\n\n**Comparaci√≥n A vs B con misma observaci√≥n**:\n```\nDQN: alta varianza, converge m√°s r√°pido en datos, puede divergir\nPPO: baja varianza, m√°s pasos necesarios, m√°s estable\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante B: PPO + Observaci√≥n Simple\n# model, callback = entrenar_flappy(timesteps=100000, algorithm=\"PPO\", use_simple_obs=True)\n\nprint(\"Variante B: PPO + Observaci√≥n Simple\")\nprint(\"  Observaci√≥n: 12D vector (igual que Var. A)\")\nprint(\"  Algoritmo: PPO (on-policy, sin replay buffer)\")\nprint()\nprint(\"Configuraci√≥n PPO:\")\nppo_config = \"\"\"\nPPO(\n    \"MlpPolicy\",\n    env,\n    learning_rate=0.0003,\n    n_steps=2048,    # Recoger 2048 pasos antes de actualizar\n    batch_size=64,\n    n_epochs=10,     # Reutilizar cada batch 10 veces\n    gamma=0.99,\n    clip_range=0.2,  # Clip del ratio pol√≠tica nueva/vieja\n)\n\"\"\"\nprint(ppo_config)\nprint(\"Diferencia clave: PPO reutiliza cada batch 10 veces (n_epochs=10)\")\nprint(\"                 DQN reutiliza miles de veces (del replay buffer)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Variante C ‚Äî DQN + Imagen RGB\n\n```python\npython flappybird_dqn.py --algorithm DQN\n```\n\n**Observaci√≥n**: imagen completa del juego (288√ó512 p√≠xeles RGB)\n\nEl agente ve los p√≠xeles *exactamente igual que un humano* ver√≠a la pantalla. Esto requiere una **CNN** para extraer caracter√≠sticas visuales antes de decidir la acci√≥n.\n\n**Por qu√© es m√°s dif√≠cil**:\n1. Espacio de observaci√≥n enorme: 288√ó512√ó3 ‚âà 443K valores por frame\n2. La CNN necesita aprender a detectar tuber√≠as, movimiento, etc.\n3. Requiere muchos m√°s pasos de entrenamiento\n\n**Ventaja conceptual**: no necesita ingenier√≠a de caracter√≠sticas. El agente aprende qu√© mirar por s√≠ solo.\n\n```\nImagen 288√ó512√ó3 ‚Üí CNN ‚Üí features ‚Üí MLP ‚Üí acci√≥n\nVector 12D       ‚Üí MLP ‚Üí acci√≥n            (Var. A/B)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante C: DQN + Imagen RGB\n# model, callback = entrenar_flappy(timesteps=300000, algorithm=\"DQN\", use_simple_obs=False)\n\nprint(\"Variante C: DQN + Imagen RGB\")\nprint(\"  Observaci√≥n: imagen 288√ó512√ó3 (p√≠xeles)\")\nprint(\"  Algoritmo: DQN con CnnPolicy\")\nprint()\nprint(\"Diferencia en pol√≠tica:\")\nprint(\"  Var. A/B: MlpPolicy  ‚Üí red densa (entrada: 12 valores)\")\nprint(\"  Var. C:   CnnPolicy  ‚Üí CNN + red densa (entrada: imagen)\")\nprint()\nprint(\"Configuraci√≥n con imagen:\")\ncnn_config = \"\"\"\nenv = gym.make(\"FlappyBird-v0\", use_lidar=False)  # Sin LIDAR ‚Üí imagen completa\n\nDQN(\n    \"CnnPolicy\",   # CNN autom√°tica de Stable-Baselines3\n    env,\n    learning_rate=0.0001,\n    buffer_size=100000,\n    ...\n)\n\"\"\"\nprint(cnn_config)\nprint(\"Nota: necesita ~3x m√°s timesteps que la observaci√≥n simple\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Variante D ‚Äî Comparativa DQN vs PPO\n\n```python\npython flappybird_dqn.py --compare-algorithms\n# O para comparar las 4 combinaciones:\npython flappybird_dqn.py --compare-all\n```\n\nEjecuta m√∫ltiples variantes y genera gr√°ficas comparativas para ver el impacto de cada elecci√≥n de dise√±o.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante D: Comparativa DQN vs PPO (misma observaci√≥n simple)\n# from flappybird_dqn import comparar_algoritmos\n# resultados = comparar_algoritmos(timesteps=100000)\n\nprint(\"Variante D: Comparativa DQN vs PPO\")\nprint(\"  Controla: misma observaci√≥n simple (12D)\")\nprint(\"  Variable: algoritmo (DQN vs PPO)\")\nprint(\"  Genera: flappy_comparacion_algoritmos.png\")\nprint()\nprint(\"Comparativa completa (4 combinaciones):\")\nprint(\"  from flappybird_dqn import comparar_todo\")\nprint(\"  resultados = comparar_todo(timesteps=80000)\")\nprint(\"  Genera: flappy_comparacion_completa.png\")\nprint()\nprint(\"M√©tricas de comparaci√≥n:\")\nprint(\"  - Score m√°ximo alcanzado (tubos pasados)\")\nprint(\"  - Media de score en los √∫ltimos 50 episodios\")\nprint(\"  - Velocidad de convergencia (ep. para llegar a score > 5)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Resumen: Cu√°ndo Usar Cada Variante\n\n| Objetivo | Variante recomendada |\n|----------|---------------------|\n| Aprender r√°pido | A: DQN + Simple |\n| Resultado m√°s estable | B: PPO + Simple |\n| Aprender solo de p√≠xeles (sin ing. de features) | C: DQN + RGB |\n| Comparar impacto del algoritmo | D: comparar-algorithms |\n| Ver todo el panorama | D: compare-all |\n\n**Lecci√≥n principal**: la elecci√≥n de observaci√≥n tiene m√°s impacto que la elecci√≥n de algoritmo. Un vector bien dise√±ado de 12 valores supera casi siempre a la imagen en entornos simples.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}