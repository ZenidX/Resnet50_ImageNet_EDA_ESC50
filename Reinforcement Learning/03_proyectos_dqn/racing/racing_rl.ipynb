{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Racing Multi-Agente con DQN\n",
    "\n",
    "## Objetivos de este Notebook\n",
    "\n",
    "En este notebook aprender√°s:\n",
    "\n",
    "1. **Arquitectura multi-agente**: ¬øLos coches comparten red neuronal o son independientes?\n",
    "2. **Dise√±o del estado**: C√≥mo funcionan los sensores raycast\n",
    "3. **Espacio de acciones**: Por qu√© 9 acciones discretas\n",
    "4. **Funci√≥n de recompensa**: C√≥mo incentivar el comportamiento deseado\n",
    "5. **Variantes de arquitectura**: Implementaciones alternativas\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisitos\n",
    "\n",
    "- Conceptos b√°sicos de DQN (Experience Replay, Target Network)\n",
    "- PyTorch b√°sico\n",
    "- Pygame (para visualizaci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# A√±adir el directorio actual al path para importar racing_game\n",
    "RACING_DIR = Path().absolute()\n",
    "if str(RACING_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(RACING_DIR))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import math\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Pygame (para visualizaci√≥n)\n",
    "import pygame\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "print(f\"Directorio: {RACING_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Descripci√≥n del Juego\n",
    "\n",
    "## ¬øQu√© es Racing Multi-Agente?\n",
    "\n",
    "Un juego donde **4 coches** (configurable) compiten simult√°neamente en un circuito ovalado. Cada coche:\n",
    "\n",
    "- Tiene **sensores** que detectan la distancia a los bordes\n",
    "- Aprende a **acelerar, frenar y girar** para mantenerse en la pista\n",
    "- **Muere** si choca con los bordes\n",
    "\n",
    "```\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ         CIRCUITO OVALADO               ‚îÇ\n",
    "    ‚îÇ    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó        ‚îÇ\n",
    "    ‚îÇ   ‚ïî‚ïù                          ‚ïö‚ïó       ‚îÇ\n",
    "    ‚îÇ  ‚ïî‚ïù   üöó üöô üöï üöó              ‚ïö‚ïó      ‚îÇ\n",
    "    ‚îÇ  ‚ïë     ‚Üê Coches compitiendo    ‚ïë      ‚îÇ\n",
    "    ‚îÇ  ‚ïö‚ïó                           ‚ïî‚ïù      ‚îÇ\n",
    "    ‚îÇ   ‚ïö‚ïó                         ‚ïî‚ïù       ‚îÇ\n",
    "    ‚îÇ    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù        ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. An√°lisis de la Arquitectura\n",
    "\n",
    "## 2.1 Estado / Observaci√≥n (6 dimensiones)\n",
    "\n",
    "Cada coche recibe un **vector de 6 valores**:\n",
    "\n",
    "| √çndice | Componente | Rango | Descripci√≥n |\n",
    "|--------|------------|-------|-------------|\n",
    "| 0 | sensor_izq_60¬∞ | [0, 1] | Distancia al borde a -60¬∞ |\n",
    "| 1 | sensor_izq_30¬∞ | [0, 1] | Distancia al borde a -30¬∞ |\n",
    "| 2 | sensor_frente | [0, 1] | Distancia al borde al frente |\n",
    "| 3 | sensor_der_30¬∞ | [0, 1] | Distancia al borde a +30¬∞ |\n",
    "| 4 | sensor_der_60¬∞ | [0, 1] | Distancia al borde a +60¬∞ |\n",
    "| 5 | velocidad | [0, 1] | Velocidad normalizada |\n",
    "\n",
    "### Diagrama de Sensores Raycast\n",
    "\n",
    "```\n",
    "                    sensor[2] (frente)\n",
    "                         ‚Üë\n",
    "                        /|\\\n",
    "           sensor[1]   / | \\   sensor[3]\n",
    "              (-30¬∞)  /  |  \\  (+30¬∞)\n",
    "                     /   |   \\\n",
    "        sensor[0]   /    |    \\   sensor[4]\n",
    "           (-60¬∞)  /     üöó     \\  (+60¬∞)\n",
    "                  /    coche    \\\n",
    "```\n",
    "\n",
    "**Valor del sensor**: \n",
    "- `1.0` = Lejos del borde (seguro)\n",
    "- `0.0` = Muy cerca del borde (peligro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar c√≥mo funciona el estado\n",
    "# Importamos la clase Car del juego original\n",
    "from racing_game import Car, Track, WINDOW_WIDTH, WINDOW_HEIGHT\n",
    "\n",
    "# Crear una pista y un coche de ejemplo\n",
    "track = Track(WINDOW_WIDTH, WINDOW_HEIGHT)\n",
    "start_positions = track.get_start_positions(1)\n",
    "pos, angle = start_positions[0]\n",
    "\n",
    "car = Car(0, (255, 0, 0), pos, angle)\n",
    "\n",
    "# Obtener estado inicial\n",
    "car._update_sensors(track)\n",
    "state = car.get_state()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ESTADO DEL COCHE (6 dimensiones)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nVector de estado: {state}\")\n",
    "print(f\"\\nDesglose:\")\n",
    "sensor_names = [\"Izq 60¬∞\", \"Izq 30¬∞\", \"Frente\", \"Der 30¬∞\", \"Der 60¬∞\", \"Velocidad\"]\n",
    "for i, (name, value) in enumerate(zip(sensor_names, state)):\n",
    "    bar = \"‚ñà\" * int(value * 20) + \"‚ñë\" * (20 - int(value * 20))\n",
    "    print(f\"  [{i}] {name:12s}: {value:.3f} |{bar}|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Espacio de Acciones (9 acciones discretas)\n",
    "\n",
    "El coche puede realizar **9 acciones diferentes**, combinando aceleraci√≥n y direcci√≥n:\n",
    "\n",
    "| Acci√≥n | Aceleraci√≥n | Direcci√≥n | Descripci√≥n |\n",
    "|--------|-------------|-----------|-------------|\n",
    "| 0 | - | - | Nada (inercia) |\n",
    "| 1 | ‚úì Acelerar | - | Solo acelerar |\n",
    "| 2 | ‚úì Frenar | - | Solo frenar |\n",
    "| 3 | - | ‚Üê Izquierda | Solo girar izquierda |\n",
    "| 4 | - | ‚Üí Derecha | Solo girar derecha |\n",
    "| 5 | ‚úì Acelerar | ‚Üê Izquierda | Acelerar + Izquierda |\n",
    "| 6 | ‚úì Acelerar | ‚Üí Derecha | Acelerar + Derecha |\n",
    "| 7 | ‚úì Frenar | ‚Üê Izquierda | Frenar + Izquierda |\n",
    "| 8 | ‚úì Frenar | ‚Üí Derecha | Frenar + Derecha |\n",
    "\n",
    "### F√≠sica del Giro\n",
    "\n",
    "**Importante**: El giro solo funciona si el coche se est√° moviendo.\n",
    "\n",
    "```python\n",
    "if self.speed > 0.1:  # Necesita velocidad m√≠nima para girar\n",
    "    turn_factor = min(1.0, self.speed / 3.0)  # Giro m√°s efectivo a velocidad media\n",
    "    actual_turn = self.turn_speed * turn_factor\n",
    "```\n",
    "\n",
    "Esto simula **f√≠sica realista**: no puedes girar el volante si est√°s parado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostrar el efecto de cada acci√≥n\n",
    "print(\"=\"*60)\n",
    "print(\"ESPACIO DE ACCIONES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "acciones = [\n",
    "    (0, \"Nada (inercia)\", \"-\", \"-\"),\n",
    "    (1, \"Acelerar\", \"‚Üë\", \"-\"),\n",
    "    (2, \"Frenar\", \"‚Üì\", \"-\"),\n",
    "    (3, \"Girar izquierda\", \"-\", \"‚Üê\"),\n",
    "    (4, \"Girar derecha\", \"-\", \"‚Üí\"),\n",
    "    (5, \"Acelerar + Izq\", \"‚Üë\", \"‚Üê\"),\n",
    "    (6, \"Acelerar + Der\", \"‚Üë\", \"‚Üí\"),\n",
    "    (7, \"Frenar + Izq\", \"‚Üì\", \"‚Üê\"),\n",
    "    (8, \"Frenar + Der\", \"‚Üì\", \"‚Üí\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Acci√≥n':<8} {'Descripci√≥n':<20} {'Acel':<6} {'Dir':<6}\")\n",
    "print(\"-\" * 45)\n",
    "for a, desc, acel, dir in acciones:\n",
    "    print(f\"{a:<8} {desc:<20} {acel:<6} {dir:<6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Funci√≥n de Recompensa\n",
    "\n",
    "La recompensa incentiva al coche a **avanzar** y **evitar colisiones**:\n",
    "\n",
    "| Evento | Recompensa | Prop√≥sito |\n",
    "|--------|------------|----------|\n",
    "| Avanzar | `+0.1 √ó Œîdistancia` | Incentivar movimiento |\n",
    "| Moverse (v > 0) | `+0.1` | Evitar quedarse parado |\n",
    "| Colisi√≥n | `-10.0` | Penalizar choques |\n",
    "\n",
    "### C√≥digo de la recompensa (racing_game.py:372-382)\n",
    "\n",
    "```python\n",
    "if not car.alive:\n",
    "    reward = -10  # Penalizaci√≥n por colisi√≥n\n",
    "else:\n",
    "    reward = (car.distance - old_distance) * 0.1  # Recompensa por avanzar\n",
    "    if car.speed > 0:\n",
    "        reward += 0.1  # Bonus por moverse\n",
    "```\n",
    "\n",
    "### ¬øPor qu√© este dise√±o?\n",
    "\n",
    "1. **Recompensa densa**: El agente recibe feedback continuo, no solo al final\n",
    "2. **Bonus por velocidad**: Evita que el agente aprenda a quedarse quieto\n",
    "3. **Penalizaci√≥n fuerte**: -10 es significativo comparado con +0.1 por paso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Arquitectura del Agente\n",
    "\n",
    "### ‚ùì PREGUNTA CLAVE: ¬øLos 4 coches comparten la misma red neuronal?\n",
    "\n",
    "## **NO.** Cada coche tiene su propio agente completamente independiente.\n",
    "\n",
    "En `racing_game.py:544-554`:\n",
    "```python\n",
    "agents = [CarAgent(i) for i in range(n_cars)]  # 4 agentes separados\n",
    "```\n",
    "\n",
    "### Cada `CarAgent` contiene:\n",
    "\n",
    "| Componente | Descripci√≥n | Par√°metros |\n",
    "|------------|-------------|------------|\n",
    "| `q_network` | Red DQN principal | 18,569 |\n",
    "| `target_network` | Copia para estabilidad | 18,569 |\n",
    "| `optimizer` | Adam (lr=0.001) | - |\n",
    "| `memory` | Replay buffer | 50,000 experiencias |\n",
    "\n",
    "### Arquitectura de la Red (CarDQN)\n",
    "\n",
    "```\n",
    "Estado [6] ‚Üí Linear(128) ‚Üí ReLU ‚Üí Linear(128) ‚Üí ReLU ‚Üí Q-valores [9]\n",
    "\n",
    "Par√°metros:\n",
    "  Capa 1: 6√ó128 + 128 = 896\n",
    "  Capa 2: 128√ó128 + 128 = 16,512\n",
    "  Capa 3: 128√ó9 + 9 = 1,161\n",
    "  TOTAL: 18,569 par√°metros por agente\n",
    "  \n",
    "Con 4 coches: 18,569 √ó 4 = 74,276 par√°metros totales\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar la arquitectura original\n",
    "from racing_game import CarDQN, CarAgent\n",
    "\n",
    "# Crear red y contar par√°metros\n",
    "red = CarDQN(state_size=6, n_actions=9)\n",
    "n_params = sum(p.numel() for p in red.parameters())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ARQUITECTURA CarDQN (Original)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{red}\")\n",
    "print(f\"\\nPar√°metros totales: {n_params:,}\")\n",
    "print(f\"Con 4 coches independientes: {n_params * 4:,} par√°metros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¬øPor qu√© agentes independientes?\n",
    "\n",
    "| Enfoque | Ventajas | Desventajas |\n",
    "|---------|----------|-------------|\n",
    "| **Independiente (actual)** | Simple, paralelo, sin interferencia | M√°s par√°metros, sin transfer |\n",
    "| **Red compartida** | Menos par√°metros, transfer learning | Competencia por representaci√≥n |\n",
    "| **Evolutivo** | Mejor exploraci√≥n global | Convergencia m√°s lenta |\n",
    "\n",
    "En las siguientes secciones implementaremos **todas las variantes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. C√≥digo Original Explicado\n",
    "\n",
    "Importamos las clases principales del archivo `racing_game.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar todo el m√≥dulo\n",
    "from racing_game import (\n",
    "    Car, Track, RacingGame,\n",
    "    CarDQN, CarAgent,\n",
    "    WINDOW_WIDTH, WINDOW_HEIGHT,\n",
    "    CAR_COLORS\n",
    ")\n",
    "\n",
    "print(\"Clases importadas:\")\n",
    "print(\"  - Car: Representa un coche individual\")\n",
    "print(\"  - Track: El circuito\")\n",
    "print(\"  - RacingGame: Gestiona el juego completo\")\n",
    "print(\"  - CarDQN: Red neuronal DQN\")\n",
    "print(\"  - CarAgent: Agente completo con entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Entrenamiento R√°pido (Demo)\n",
    "\n",
    "Entrenamos por pocos episodios para ver el proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_demo(n_cars=4, episodes=20, max_steps=500, render_cada=5):\n",
    "    \"\"\"\n",
    "    Entrena agentes DQN independientes (arquitectura original).\n",
    "    \n",
    "    Args:\n",
    "        n_cars: N√∫mero de coches\n",
    "        episodes: Episodios de entrenamiento\n",
    "        max_steps: Pasos m√°ximos por episodio\n",
    "        render_cada: Renderizar cada N episodios (0 = nunca)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ENTRENAMIENTO DEMO - {n_cars} Coches Independientes\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Crear juego (con renderizado si queremos visualizar)\n",
    "    render = render_cada > 0\n",
    "    game = RacingGame(n_cars=n_cars, render=render)\n",
    "    \n",
    "    # Crear agentes independientes\n",
    "    agents = [CarAgent(i) for i in range(n_cars)]\n",
    "    \n",
    "    # M√©tricas\n",
    "    fitness_history = [[] for _ in range(n_cars)]\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        states = game.reset()\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Cada agente decide su acci√≥n\n",
    "            actions = [agent.act(state) for agent, state in zip(agents, states)]\n",
    "            \n",
    "            # Ejecutar paso\n",
    "            next_states, rewards, dones, all_done = game.step(actions)\n",
    "            \n",
    "            # Almacenar experiencias y entrenar\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.remember(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "                agent.replay()\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            # Renderizar\n",
    "            if render and (ep % render_cada == 0):\n",
    "                game.render()\n",
    "                game.clock.tick(60)\n",
    "                \n",
    "                # Procesar eventos pygame\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        game.close()\n",
    "                        return agents, fitness_history\n",
    "            \n",
    "            if all_done:\n",
    "                break\n",
    "        \n",
    "        # Actualizar agentes\n",
    "        for i, agent in enumerate(agents):\n",
    "            agent.decay_epsilon()\n",
    "            if ep % 10 == 0:\n",
    "                agent.update_target()\n",
    "            fitness_history[i].append(game.cars[i].fitness)\n",
    "        \n",
    "        # Log\n",
    "        if (ep + 1) % 5 == 0:\n",
    "            avg_fitness = np.mean([car.fitness for car in game.cars])\n",
    "            print(f\"Ep {ep+1:3d} | Fitness: {avg_fitness:.0f} | Œµ: {agents[0].epsilon:.3f}\")\n",
    "    \n",
    "    if render:\n",
    "        game.close()\n",
    "    \n",
    "    return agents, fitness_history\n",
    "\n",
    "# Entrenar demo (sin visualizaci√≥n para ir m√°s r√°pido)\n",
    "agents_original, history_original = entrenar_demo(n_cars=4, episodes=30, render_cada=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curvas de aprendizaje\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, hist in enumerate(history_original):\n",
    "    plt.plot(hist, label=f'Coche {i}', alpha=0.7)\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Fitness')\n",
    "plt.title('Fitness por Coche (Agentes Independientes)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "avg_fitness = np.mean(history_original, axis=0)\n",
    "plt.plot(avg_fitness, 'b-', linewidth=2)\n",
    "plt.fill_between(range(len(avg_fitness)), \n",
    "                 np.min(history_original, axis=0),\n",
    "                 np.max(history_original, axis=0),\n",
    "                 alpha=0.3)\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Fitness Promedio')\n",
    "plt.title('Fitness Promedio (con rango)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. VARIANTES DE ARQUITECTURA\n",
    "\n",
    "A continuaci√≥n implementamos **5 variantes** de la arquitectura multi-agente:\n",
    "\n",
    "| Variante | Descripci√≥n | Diferencia clave |\n",
    "|----------|-------------|------------------|\n",
    "| A | SharedDQN | Red compartida con cabezas independientes |\n",
    "| B | CompetitiveDQN | Recompensa basada en ranking |\n",
    "| C | CooperativeDQN | Recompensa por rendimiento grupal |\n",
    "| D | A2C Multi-Agente | Policy Gradient en vez de DQN |\n",
    "| E | EvolutionaryDQN | Selecci√≥n de mejores agentes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variante A: SharedDQN (Red Compartida)\n",
    "\n",
    "### Idea\n",
    "En lugar de 4 redes independientes, usamos **una sola red** con:\n",
    "- **Encoder compartido**: Aprende representaciones comunes\n",
    "- **Cabezas independientes**: Una por coche para especializaci√≥n\n",
    "\n",
    "### Ventajas\n",
    "- Menos par√°metros (18K vs 72K)\n",
    "- Transfer de conocimiento entre coches\n",
    "- Entrenamiento m√°s eficiente\n",
    "\n",
    "### Desventajas\n",
    "- Los coches compiten por la representaci√≥n\n",
    "- Puede ser menos flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Red DQN con encoder compartido y cabezas independientes por coche.\n",
    "    \n",
    "    Arquitectura:\n",
    "        Estado [6] ‚Üí Shared[128] ‚Üí ReLU ‚Üí Shared[128] ‚Üí ReLU\n",
    "                                                    ‚Üì\n",
    "                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                              ‚Üì          ‚Üì          ‚Üì          ‚Üì          \n",
    "                          Head_0[9]  Head_1[9]  Head_2[9]  Head_3[9]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=6, n_actions=9, n_cars=4):\n",
    "        super().__init__()\n",
    "        self.n_cars = n_cars\n",
    "        \n",
    "        # Encoder compartido\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Cabezas independientes (una por coche)\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(128, n_actions) for _ in range(n_cars)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, car_id=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Estado (batch, 6) o (6,)\n",
    "            car_id: ID del coche (0-3). Si None, devuelve para todos.\n",
    "        \"\"\"\n",
    "        shared_features = self.shared(x)\n",
    "        \n",
    "        if car_id is not None:\n",
    "            return self.heads[car_id](shared_features)\n",
    "        else:\n",
    "            # Devolver Q-valores para todos los coches\n",
    "            return [head(shared_features) for head in self.heads]\n",
    "\n",
    "\n",
    "class SharedCarAgent:\n",
    "    \"\"\"\n",
    "    Agente multi-coche con red compartida.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_cars=4, state_size=6, n_actions=9):\n",
    "        self.n_cars = n_cars\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # UNA sola red para todos\n",
    "        self.q_network = SharedDQN(state_size, n_actions, n_cars).to(self.device)\n",
    "        self.target_network = SharedDQN(state_size, n_actions, n_cars).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # UN solo optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        \n",
    "        # Memorias separadas por coche (para diversidad)\n",
    "        self.memories = [deque(maxlen=50000) for _ in range(n_cars)]\n",
    "        self.batch_size = 32\n",
    "    \n",
    "    def act(self, state, car_id):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_network(state_t, car_id)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def remember(self, car_id, state, action, reward, next_state, done):\n",
    "        self.memories[car_id].append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Entrena con experiencias de TODOS los coches.\"\"\"\n",
    "        total_loss = 0\n",
    "        \n",
    "        for car_id in range(self.n_cars):\n",
    "            if len(self.memories[car_id]) < self.batch_size:\n",
    "                continue\n",
    "            \n",
    "            batch = random.sample(self.memories[car_id], self.batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            \n",
    "            states_t = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "            actions_t = torch.LongTensor(actions).to(self.device)\n",
    "            rewards_t = torch.FloatTensor(rewards).to(self.device)\n",
    "            next_states_t = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "            dones_t = torch.FloatTensor(dones).to(self.device)\n",
    "            \n",
    "            # Q-valores para este coche espec√≠fico\n",
    "            q_values = self.q_network(states_t, car_id).gather(1, actions_t.unsqueeze(1)).squeeze()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                next_q = self.target_network(next_states_t, car_id).max(1)[0]\n",
    "                target = rewards_t + (1 - dones_t) * self.gamma * next_q\n",
    "            \n",
    "            loss = nn.MSELoss()(q_values, target)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return total_loss / self.n_cars\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "# Verificar arquitectura\n",
    "shared_net = SharedDQN(state_size=6, n_actions=9, n_cars=4)\n",
    "n_params_shared = sum(p.numel() for p in shared_net.parameters())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VARIANTE A: SharedDQN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPar√°metros totales: {n_params_shared:,}\")\n",
    "print(f\"Comparado con independientes: {n_params * 4:,}\")\n",
    "print(f\"Reducci√≥n: {(1 - n_params_shared / (n_params * 4)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variante B: CompetitiveDQN (Recompensa por Ranking)\n",
    "\n",
    "### Idea\n",
    "Modificamos la **funci√≥n de recompensa** para incentivar competencia:\n",
    "- Bonus por **adelantar** otros coches\n",
    "- Penalizaci√≥n por **ser adelantado**\n",
    "\n",
    "### Ventajas\n",
    "- Genera comportamientos m√°s agresivos\n",
    "- Mayor diversidad de estrategias\n",
    "\n",
    "### Desventajas\n",
    "- Puede generar colisiones intencionales\n",
    "- M√°s dif√≠cil de converger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def competitive_reward(car, all_cars, old_distance):\n",
    "    \"\"\"\n",
    "    Calcula recompensa competitiva basada en ranking.\n",
    "    \n",
    "    Args:\n",
    "        car: Coche actual\n",
    "        all_cars: Lista de todos los coches\n",
    "        old_distance: Distancia del coche antes del paso\n",
    "    \n",
    "    Returns:\n",
    "        Recompensa modificada\n",
    "    \"\"\"\n",
    "    if not car.alive:\n",
    "        return -10.0\n",
    "    \n",
    "    # Recompensa base (igual que original)\n",
    "    base_reward = (car.distance - old_distance) * 0.1\n",
    "    if car.speed > 0:\n",
    "        base_reward += 0.1\n",
    "    \n",
    "    # Calcular ranking (1 = primero, n = √∫ltimo)\n",
    "    alive_cars = [c for c in all_cars if c.alive]\n",
    "    if len(alive_cars) <= 1:\n",
    "        return base_reward + 1.0  # Bonus por ser el √∫ltimo superviviente\n",
    "    \n",
    "    rank = sum(1 for c in alive_cars if c.distance > car.distance) + 1\n",
    "    n_alive = len(alive_cars)\n",
    "    \n",
    "    # Bonus/penalizaci√≥n por ranking (0.5 por cada posici√≥n)\n",
    "    ranking_reward = (n_alive - rank) * 0.2  # Mejor ranking = m√°s recompensa\n",
    "    \n",
    "    return base_reward + ranking_reward\n",
    "\n",
    "\n",
    "class CompetitiveRacingGame(RacingGame):\n",
    "    \"\"\"\n",
    "    Variante del juego con recompensa competitiva.\n",
    "    \"\"\"\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Paso con recompensa competitiva.\"\"\"\n",
    "        states = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        \n",
    "        # Guardar distancias anteriores\n",
    "        old_distances = [car.distance for car in self.cars]\n",
    "        \n",
    "        # Actualizar coches\n",
    "        for car, action in zip(self.cars, actions):\n",
    "            car.update(action, self.track)\n",
    "        \n",
    "        # Calcular recompensas competitivas\n",
    "        for i, car in enumerate(self.cars):\n",
    "            reward = competitive_reward(car, self.cars, old_distances[i])\n",
    "            states.append(car.get_state())\n",
    "            rewards.append(reward)\n",
    "            dones.append(not car.alive)\n",
    "        \n",
    "        all_done = all(not car.alive for car in self.cars)\n",
    "        return states, rewards, dones, all_done\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VARIANTE B: CompetitiveDQN\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nEstructura de recompensa:\")\n",
    "print(\"  Base: +0.1 √ó Œîdistancia + 0.1 (si v > 0)\")\n",
    "print(\"  Ranking: +0.2 √ó (n_alive - rank)\")\n",
    "print(\"  Colisi√≥n: -10\")\n",
    "print(\"  √öltimo superviviente: +1.0 bonus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variante C: CooperativeDQN (Recompensa Grupal)\n",
    "\n",
    "### Idea\n",
    "En lugar de competir, los coches **cooperan**:\n",
    "- Bonus si **todos avanzan**\n",
    "- Penalizaci√≥n si chocan **entre ellos**\n",
    "\n",
    "### Ventajas\n",
    "- Comportamientos m√°s seguros\n",
    "- Los coches aprenden a evitarse\n",
    "\n",
    "### Desventajas\n",
    "- Puede ser menos \"emocionante\"\n",
    "- M√°s dif√≠cil definir la recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooperative_reward(car, all_cars, old_distance):\n",
    "    \"\"\"\n",
    "    Calcula recompensa cooperativa basada en rendimiento grupal.\n",
    "    \n",
    "    Args:\n",
    "        car: Coche actual\n",
    "        all_cars: Lista de todos los coches\n",
    "        old_distance: Distancia del coche antes del paso\n",
    "    \n",
    "    Returns:\n",
    "        Recompensa modificada\n",
    "    \"\"\"\n",
    "    if not car.alive:\n",
    "        return -10.0\n",
    "    \n",
    "    # Recompensa base individual\n",
    "    individual_reward = (car.distance - old_distance) * 0.1\n",
    "    if car.speed > 0:\n",
    "        individual_reward += 0.05  # Reducido para dar m√°s peso al grupo\n",
    "    \n",
    "    # Recompensa grupal\n",
    "    alive_cars = [c for c in all_cars if c.alive]\n",
    "    n_alive = len(alive_cars)\n",
    "    \n",
    "    # Bonus por supervivencia grupal\n",
    "    survival_bonus = n_alive * 0.05  # M√°s coches vivos = mejor\n",
    "    \n",
    "    # Bonus por distancia promedio del grupo\n",
    "    if n_alive > 1:\n",
    "        avg_distance = np.mean([c.distance for c in alive_cars])\n",
    "        group_progress = avg_distance * 0.001  # Peque√±o bonus por progreso grupal\n",
    "    else:\n",
    "        group_progress = 0\n",
    "    \n",
    "    return individual_reward + survival_bonus + group_progress\n",
    "\n",
    "\n",
    "class CooperativeRacingGame(RacingGame):\n",
    "    \"\"\"\n",
    "    Variante del juego con recompensa cooperativa.\n",
    "    \"\"\"\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Paso con recompensa cooperativa.\"\"\"\n",
    "        states = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        \n",
    "        old_distances = [car.distance for car in self.cars]\n",
    "        \n",
    "        for car, action in zip(self.cars, actions):\n",
    "            car.update(action, self.track)\n",
    "        \n",
    "        for i, car in enumerate(self.cars):\n",
    "            reward = cooperative_reward(car, self.cars, old_distances[i])\n",
    "            states.append(car.get_state())\n",
    "            rewards.append(reward)\n",
    "            dones.append(not car.alive)\n",
    "        \n",
    "        all_done = all(not car.alive for car in self.cars)\n",
    "        return states, rewards, dones, all_done\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VARIANTE C: CooperativeDQN\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nEstructura de recompensa:\")\n",
    "print(\"  Individual: +0.1 √ó Œîdistancia + 0.05 (si v > 0)\")\n",
    "print(\"  Supervivencia: +0.05 √ó n_alive\")\n",
    "print(\"  Progreso grupal: +0.001 √ó avg_distance\")\n",
    "print(\"  Colisi√≥n: -10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variante D: A2C Multi-Agente (Policy Gradient)\n",
    "\n",
    "### Idea\n",
    "Usar **Advantage Actor-Critic (A2C)** en lugar de DQN:\n",
    "- **Actor**: Pol√≠tica que elige acciones directamente\n",
    "- **Critic**: Estima el valor del estado\n",
    "\n",
    "### Ventajas\n",
    "- M√°s estable para algunos problemas\n",
    "- Funciona bien con acciones continuas\n",
    "\n",
    "### Desventajas\n",
    "- Menos eficiente en datos (on-policy)\n",
    "- M√°s hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Red Actor-Critic para un coche.\n",
    "    \n",
    "    Actor: Produce distribuci√≥n de probabilidad sobre acciones\n",
    "    Critic: Estima V(s)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=6, n_actions=9):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Backbone compartido\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor head (pol√≠tica)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Critic head (valor)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        policy = self.actor(shared)  # Probabilidades de acciones\n",
    "        value = self.critic(shared)   # V(s)\n",
    "        return policy, value\n",
    "\n",
    "\n",
    "class A2CAgent:\n",
    "    \"\"\"\n",
    "    Agente A2C para un coche individual.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id, state_size=6, n_actions=9):\n",
    "        self.id = agent_id\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = 0.95\n",
    "        self.entropy_coef = 0.01  # Para exploraci√≥n\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.network = ActorCritic(state_size, n_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=0.001)\n",
    "        \n",
    "        # Almacenar trayectoria del episodio\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "    \n",
    "    def act(self, state):\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            policy, value = self.network(state_t)\n",
    "        \n",
    "        # Muestrear acci√≥n de la distribuci√≥n\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Guardar para entrenamiento\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action.item())\n",
    "        self.values.append(value.item())\n",
    "        self.log_probs.append(dist.log_prob(action).item())\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def remember_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Entrena al final del episodio.\"\"\"\n",
    "        if len(self.rewards) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Calcular returns y advantages\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        values = torch.FloatTensor(self.values).to(self.device)\n",
    "        log_probs = torch.FloatTensor(self.log_probs).to(self.device)\n",
    "        \n",
    "        # Normalizar returns\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Advantage = Returns - Values\n",
    "        advantages = returns - values\n",
    "        \n",
    "        # Actor loss (policy gradient)\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        \n",
    "        # Critic loss (value function)\n",
    "        critic_loss = nn.MSELoss()(values, returns)\n",
    "        \n",
    "        # Entropy para exploraci√≥n\n",
    "        states_t = torch.FloatTensor(np.array(self.states)).to(self.device)\n",
    "        policy, _ = self.network(states_t)\n",
    "        entropy = -(policy * torch.log(policy + 1e-8)).sum(dim=-1).mean()\n",
    "        \n",
    "        # Loss total\n",
    "        loss = actor_loss + 0.5 * critic_loss - self.entropy_coef * entropy\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Limpiar buffers\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Verificar arquitectura\n",
    "ac_net = ActorCritic(state_size=6, n_actions=9)\n",
    "n_params_ac = sum(p.numel() for p in ac_net.parameters())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VARIANTE D: A2C Multi-Agente\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{ac_net}\")\n",
    "print(f\"\\nPar√°metros por agente: {n_params_ac:,}\")\n",
    "print(f\"Con 4 coches: {n_params_ac * 4:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variante E: EvolutionaryDQN (Algoritmo Evolutivo)\n",
    "\n",
    "### Idea\n",
    "En lugar de backpropagation, usamos **evoluci√≥n**:\n",
    "- Torneo entre agentes\n",
    "- Los mejores \"sobreviven\"\n",
    "- Mutaci√≥n de pesos\n",
    "\n",
    "### Ventajas\n",
    "- Explora mejor el espacio de soluciones\n",
    "- No requiere gradientes\n",
    "\n",
    "### Desventajas\n",
    "- Convergencia m√°s lenta\n",
    "- Menos eficiente en datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolutionaryAgent:\n",
    "    \"\"\"\n",
    "    Agente que evoluciona mediante selecci√≥n y mutaci√≥n.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id, state_size=6, n_actions=9):\n",
    "        self.id = agent_id\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Red simple (sin target network ni replay buffer)\n",
    "        self.network = CarDQN(state_size, n_actions).to(self.device)\n",
    "        self.fitness = 0\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Acci√≥n greedy (sin exploraci√≥n Œµ).\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.network(state_t)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def mutate(self, mutation_rate=0.1, mutation_strength=0.1):\n",
    "        \"\"\"\n",
    "        Muta los pesos de la red.\n",
    "        \n",
    "        Args:\n",
    "            mutation_rate: Probabilidad de mutar cada peso\n",
    "            mutation_strength: Magnitud de la mutaci√≥n\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param in self.network.parameters():\n",
    "                mask = torch.rand_like(param) < mutation_rate\n",
    "                noise = torch.randn_like(param) * mutation_strength\n",
    "                param.add_(mask.float() * noise)\n",
    "    \n",
    "    def copy_from(self, other):\n",
    "        \"\"\"Copia los pesos de otro agente.\"\"\"\n",
    "        self.network.load_state_dict(other.network.state_dict())\n",
    "\n",
    "\n",
    "def evolve_population(agents, fitness_scores, elite_ratio=0.25, mutation_rate=0.1):\n",
    "    \"\"\"\n",
    "    Evoluciona la poblaci√≥n de agentes.\n",
    "    \n",
    "    Args:\n",
    "        agents: Lista de agentes\n",
    "        fitness_scores: Lista de fitness de cada agente\n",
    "        elite_ratio: Porcentaje de √©lite que sobrevive\n",
    "        mutation_rate: Tasa de mutaci√≥n\n",
    "    \"\"\"\n",
    "    n = len(agents)\n",
    "    n_elite = max(1, int(n * elite_ratio))\n",
    "    \n",
    "    # Ordenar por fitness (mejores primero)\n",
    "    sorted_indices = np.argsort(fitness_scores)[::-1]\n",
    "    \n",
    "    # √âlite: los mejores sobreviven sin cambios\n",
    "    elite_indices = sorted_indices[:n_elite]\n",
    "    \n",
    "    # El resto: copiar de √©lite y mutar\n",
    "    for i in sorted_indices[n_elite:]:\n",
    "        # Elegir padre de la √©lite\n",
    "        parent_idx = elite_indices[random.randint(0, n_elite - 1)]\n",
    "        agents[i].copy_from(agents[parent_idx])\n",
    "        agents[i].mutate(mutation_rate)\n",
    "    \n",
    "    return agents\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VARIANTE E: EvolutionaryDQN\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nProceso evolutivo:\")\n",
    "print(\"  1. Evaluar fitness de cada agente\")\n",
    "print(\"  2. Seleccionar √©lite (top 25%)\")\n",
    "print(\"  3. Clonar √©lite para reemplazar peores\")\n",
    "print(\"  4. Mutar clones (10% de pesos, œÉ=0.1)\")\n",
    "print(\"  5. Repetir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Comparaci√≥n de Variantes\n",
    "\n",
    "Entrenamos todas las variantes y comparamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_variante_shared(n_cars=4, episodes=30, max_steps=500):\n",
    "    \"\"\"Entrena con SharedDQN.\"\"\"\n",
    "    print(\"\\nEntrenando SharedDQN...\")\n",
    "    \n",
    "    game = RacingGame(n_cars=n_cars, render=False)\n",
    "    agent = SharedCarAgent(n_cars=n_cars)\n",
    "    \n",
    "    fitness_history = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        states = game.reset()\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = [agent.act(states[i], i) for i in range(n_cars)]\n",
    "            next_states, rewards, dones, all_done = game.step(actions)\n",
    "            \n",
    "            for i in range(n_cars):\n",
    "                agent.remember(i, states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "            \n",
    "            agent.replay()\n",
    "            states = next_states\n",
    "            \n",
    "            if all_done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        if ep % 10 == 0:\n",
    "            agent.update_target()\n",
    "        \n",
    "        avg_fitness = np.mean([car.fitness for car in game.cars])\n",
    "        fitness_history.append(avg_fitness)\n",
    "        \n",
    "        if (ep + 1) % 10 == 0:\n",
    "            print(f\"  Ep {ep+1}: Fitness = {avg_fitness:.0f}\")\n",
    "    \n",
    "    return fitness_history\n",
    "\n",
    "\n",
    "def entrenar_variante_competitive(n_cars=4, episodes=30, max_steps=500):\n",
    "    \"\"\"Entrena con recompensa competitiva.\"\"\"\n",
    "    print(\"\\nEntrenando CompetitiveDQN...\")\n",
    "    \n",
    "    game = CompetitiveRacingGame(n_cars=n_cars, render=False)\n",
    "    agents = [CarAgent(i) for i in range(n_cars)]\n",
    "    \n",
    "    fitness_history = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        states = game.reset()\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = [agent.act(state) for agent, state in zip(agents, states)]\n",
    "            next_states, rewards, dones, all_done = game.step(actions)\n",
    "            \n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.remember(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "                agent.replay()\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            if all_done:\n",
    "                break\n",
    "        \n",
    "        for agent in agents:\n",
    "            agent.decay_epsilon()\n",
    "            if ep % 10 == 0:\n",
    "                agent.update_target()\n",
    "        \n",
    "        avg_fitness = np.mean([car.fitness for car in game.cars])\n",
    "        fitness_history.append(avg_fitness)\n",
    "        \n",
    "        if (ep + 1) % 10 == 0:\n",
    "            print(f\"  Ep {ep+1}: Fitness = {avg_fitness:.0f}\")\n",
    "    \n",
    "    return fitness_history\n",
    "\n",
    "\n",
    "def entrenar_variante_cooperative(n_cars=4, episodes=30, max_steps=500):\n",
    "    \"\"\"Entrena con recompensa cooperativa.\"\"\"\n",
    "    print(\"\\nEntrenando CooperativeDQN...\")\n",
    "    \n",
    "    game = CooperativeRacingGame(n_cars=n_cars, render=False)\n",
    "    agents = [CarAgent(i) for i in range(n_cars)]\n",
    "    \n",
    "    fitness_history = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        states = game.reset()\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = [agent.act(state) for agent, state in zip(agents, states)]\n",
    "            next_states, rewards, dones, all_done = game.step(actions)\n",
    "            \n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.remember(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "                agent.replay()\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            if all_done:\n",
    "                break\n",
    "        \n",
    "        for agent in agents:\n",
    "            agent.decay_epsilon()\n",
    "            if ep % 10 == 0:\n",
    "                agent.update_target()\n",
    "        \n",
    "        avg_fitness = np.mean([car.fitness for car in game.cars])\n",
    "        fitness_history.append(avg_fitness)\n",
    "        \n",
    "        if (ep + 1) % 10 == 0:\n",
    "            print(f\"  Ep {ep+1}: Fitness = {avg_fitness:.0f}\")\n",
    "    \n",
    "    return fitness_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar todas las variantes\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARACI√ìN DE VARIANTES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Original (agentes independientes)\n",
    "print(\"\\nEntrenando Original (Independientes)...\")\n",
    "_, history_original = entrenar_demo(n_cars=4, episodes=30, render_cada=0)\n",
    "avg_original = np.mean(history_original, axis=0)\n",
    "\n",
    "# Shared\n",
    "history_shared = entrenar_variante_shared(episodes=30)\n",
    "\n",
    "# Competitive\n",
    "history_competitive = entrenar_variante_competitive(episodes=30)\n",
    "\n",
    "# Cooperative\n",
    "history_cooperative = entrenar_variante_cooperative(episodes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparaci√≥n\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Curvas de aprendizaje\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(avg_original, label='Original (Independientes)', linewidth=2)\n",
    "plt.plot(history_shared, label='SharedDQN', linewidth=2)\n",
    "plt.plot(history_competitive, label='CompetitiveDQN', linewidth=2)\n",
    "plt.plot(history_cooperative, label='CooperativeDQN', linewidth=2)\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Fitness Promedio')\n",
    "plt.title('Comparaci√≥n de Variantes')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Fitness final (boxplot)\n",
    "plt.subplot(1, 2, 2)\n",
    "data = [\n",
    "    avg_original[-10:],\n",
    "    history_shared[-10:],\n",
    "    history_competitive[-10:],\n",
    "    history_cooperative[-10:]\n",
    "]\n",
    "labels = ['Original', 'Shared', 'Competitive', 'Cooperative']\n",
    "plt.bar(labels, [np.mean(d) for d in data], yerr=[np.std(d) for d in data], capsize=5)\n",
    "plt.ylabel('Fitness Final (√∫ltimos 10 eps)')\n",
    "plt.title('Rendimiento Final por Variante')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabla resumen\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN DE RESULTADOS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Variante':<20} {'Fitness Final':<15} {'Std':<10}\")\n",
    "print(\"-\" * 45)\n",
    "for name, hist in zip(labels, data):\n",
    "    print(f\"{name:<20} {np.mean(hist):<15.1f} {np.std(hist):<10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Conclusiones\n",
    "\n",
    "## ¬øQu√© aprendimos?\n",
    "\n",
    "1. **Arquitectura Multi-Agente**: \n",
    "   - La implementaci√≥n original usa **agentes independientes** (cada coche tiene su propia red)\n",
    "   - Esto es simple pero usa m√°s par√°metros\n",
    "\n",
    "2. **Variantes**:\n",
    "   - **SharedDQN**: M√°s eficiente en par√°metros, transfer learning\n",
    "   - **Competitive**: Genera comportamientos m√°s agresivos\n",
    "   - **Cooperative**: Comportamientos m√°s seguros\n",
    "   - **A2C**: Policy gradient alternativo\n",
    "   - **Evolutionary**: Exploraci√≥n sin gradientes\n",
    "\n",
    "3. **Dise√±o del Estado**:\n",
    "   - Los sensores raycast proporcionan informaci√≥n local suficiente\n",
    "   - 6 dimensiones es compacto pero efectivo\n",
    "\n",
    "## Siguientes Pasos\n",
    "\n",
    "- Entrenar por m√°s episodios (200+) para ver convergencia real\n",
    "- Experimentar con diferentes hiperpar√°metros\n",
    "- Probar arquitecturas m√°s complejas (CNNs con visualizaci√≥n)\n",
    "- Implementar comunicaci√≥n entre agentes\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [Multi-Agent Reinforcement Learning: A Selective Overview](https://arxiv.org/abs/1911.10635)\n",
    "- [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)\n",
    "- [Asynchronous Methods for Deep Reinforcement Learning (A3C)](https://arxiv.org/abs/1602.01783)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## üèéÔ∏è Variantes de Entrenamiento Multi-Agente\n\nEl proyecto Racing es especialmente rico en variantes porque tiene **4 agentes simult√°neos**. La forma en que estos agentes comparten (o no) conocimiento define filosof√≠as muy distintas de aprendizaje multi-agente.\n\n| Variante | Nombre | Redes | Buffer | Concepto clave |\n|----------|--------|-------|--------|----------------|\n| A | Independientes *(actual)* | 4 redes separadas | 4 buffers | Cada agente aprende solo |\n| B | Red Compartida | 1 red compartida | 1 buffer | Centralizado, 4√ó m√°s datos |\n| C | Maestro-Alumno | 1+N redes | independientes | Transfer learning |\n| D | Competitivo | 4 redes separadas | 4 buffers | Recompensa relativa |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Variante A ‚Äî Agentes Independientes *(implementaci√≥n actual)*\n\n```python\npython racing_game.py --train --variant independent\n```\n\nCada coche tiene su **propia red neuronal** [6‚Üí128‚Üí128‚Üí9] y su propio replay buffer (50K experiencias). Aprenden en paralelo pero sin comunicarse.\n\n**Cu√°ndo funciona bien**: cuando los agentes tienen objetivos distintos o cuando queremos que cada uno desarrolle un estilo propio.\n\n**Limitaci√≥n**: cada agente ve solo 1/4 de la experiencia total disponible. Aprende m√°s lento que si pudiera aprender de todos los coches.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante A: Agentes Independientes\n# Cada coche tiene su propia red DQN y su propio buffer\nimport subprocess\n# subprocess.run([\"python\", \"racing_game.py\", \"--train\", \"--variant\", \"independent\", \"--episodes\", \"50\"])\n\n# Para ejecutar directamente en este notebook:\nimport sys, os\nos.chdir(os.path.dirname(os.path.abspath(\"racing_game.py\")))\n\n# Importar funciones del archivo principal\n# from racing_game import train_agents\n# train_agents(n_cars=4, episodes=50)\n\nprint(\"Variante A: 4 agentes con redes independientes\")\nprint(\"  - 4 redes DQN: [6 ‚Üí 128 ‚Üí 128 ‚Üí 9]\")\nprint(\"  - 4 replay buffers (50K exp. cada uno)\")\nprint(\"  - Cada agente entrena con sus propias experiencias\")\nprint(\"  - Modelos guardados: car_agent_0.pth, car_agent_1.pth, ...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Variante B ‚Äî Red Compartida (Centralized Training)\n\n```python\npython racing_game.py --train --variant shared\n```\n\nTodos los coches usan **la misma red neuronal** y sus experiencias van al **mismo replay buffer**. Es como tener un √∫nico agente que se ejecuta en 4 instancias simult√°neas.\n\n**Ventaja**: el buffer acumula 4√ó m√°s experiencias por episodio ‚Üí aprende m√°s r√°pido.\n\n**Limitaci√≥n**: todos aprenden la misma pol√≠tica. No pueden especializarse.\n\n**Concepto**: *Centralized Training, Decentralized Execution* (CTDE) ‚Äî uno de los paradigmas fundamentales en Multi-Agent RL.\n\n```\nEstado Coche 0 ‚îÄ‚îÄ‚îê\nEstado Coche 1 ‚îÄ‚îÄ‚î§‚îÄ‚îÄ‚ñ∫ Red Compartida ‚îÄ‚îÄ‚ñ∫ Acci√≥n para cada coche\nEstado Coche 2 ‚îÄ‚îÄ‚î§         ‚ñ≤\nEstado Coche 3 ‚îÄ‚îÄ‚îò         ‚îÇ\n                    Todas las exp. ‚Üí mismo buffer\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante B: Red Compartida\n# from racing_game import train_shared, TORCH_AVAILABLE\n\n# if TORCH_AVAILABLE:\n#     train_shared(n_cars=4, episodes=50)\n\nprint(\"Variante B: Red neuronal compartida\")\nprint(\"  - 1 red DQN compartida: [6 ‚Üí 128 ‚Üí 128 ‚Üí 9]\")\nprint(\"  - 1 replay buffer compartido (50K exp.)\")\nprint(\"  - Todas las experiencias de los 4 coches van al mismo buffer\")\nprint(\"  - 4√ó m√°s datos por episodio ‚Üí convergencia m√°s r√°pida\")\nprint(\"  - Modelo guardado: car_shared.pth\")\n\n# Diferencia clave en el bucle de entrenamiento:\ncodigo_compartido = \"\"\"\n# Variante B: un solo agente para todos\nshared_agent = SharedCarAgent(n_agents=4)\n\n# Todas las experiencias al mismo buffer\nfor i in range(n_cars):\n    shared_agent.remember(states[i], actions[i], rewards[i], next_states[i], dones[i])\n\n# Una sola actualizaci√≥n de red\nshared_agent.replay()  # Entrena con batch del buffer compartido\n\"\"\"\nprint(\"\\nC√≥digo clave (Variante B vs A):\")\nprint(codigo_compartido)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Variante C ‚Äî Maestro-Alumno (Transfer Learning)\n\n```python\npython racing_game.py --train --variant master_student\n```\n\n**Fase 1**: Un agente *maestro* entrena solo durante N episodios hasta adquirir una pol√≠tica b√°sica.\n\n**Fase 2**: Los dem√°s agentes *copian* exactamente los pesos del maestro y refinan a partir de ese punto, con epsilon reducido (ya saben algo).\n\n**Por qu√© funciona**: el conocimiento de \"c√≥mo conducir\" es transferible. Los alumnos no empiezan desde exploraci√≥n aleatoria sino desde una pol√≠tica ya competente.\n\n**Analog√≠a**: como aprender a conducir viendo primero c√≥mo lo hace un instructor y luego practicando por tu cuenta.\n\n| | Sin transfer (Var. A) | Con transfer (Var. C) |\n|--|--|--|\n| Inicio alumnos | Œµ = 1.0 (aleatorio) | Œµ = 0.3 (ya saben algo) |\n| Episodios √∫tiles desde | ~50 ep. | ~5 ep. |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante C: Maestro-Alumno\n# from racing_game import train_master_student\n\n# train_master_student(n_cars=4, master_episodes=50, student_episodes=50)\n\nprint(\"Variante C: Maestro-Alumno\")\nprint(\"\\nFase 1 ‚Äî El maestro aprende solo:\")\nprint(\"  - 1 coche, entrena master_episodes episodios\")\nprint(\"  - Guarda pesos en: car_master.pth\")\nprint(\"\\nFase 2 ‚Äî Los alumnos copian al maestro:\")\nprint(\"  - 4 coches cargan car_master.pth\")\nprint(\"  - epsilon = 0.3 (exploraci√≥n reducida, ya saben conducir)\")\nprint(\"  - Refinan durante student_episodes episodios\")\nprint(\"  - Modelos guardados: car_student_0.pth, car_student_1.pth, ...\")\n\ntransferencia = \"\"\"\n# Copia de pesos del maestro a los alumnos\nfor i in range(n_cars):\n    student = CarAgent(i)\n    student.q_network.load_state_dict(master.q_network.state_dict())\n    student.target_network.load_state_dict(master.q_network.state_dict())\n    student.epsilon = 0.3  # Reducido: ya saben algo\n\"\"\"\nprint(\"\\nC√≥digo de transferencia de pesos:\")\nprint(transferencia)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Variante D ‚Äî Competitivo con Ranking\n\n```python\npython racing_game.py --train --variant competitive\n```\n\nLa recompensa ya no es solo por **sobrevivir y avanzar**, sino tambi√©n por la **posici√≥n relativa** respecto al resto de coches vivos.\n\n**C√°lculo del ranking bonus**:\n```\nrank_bonus ‚àà [-0.5, +0.5]\n  L√≠der (rank 0):    +0.5\n  √öltimo (rank n-1): -0.5\n```\n\n**Recompensa total**: `base_reward + rank_bonus`\n\nEsto introduce **competencia expl√≠cita**: no basta con mantenerse en pista, hay que hacerlo *mejor que los dem√°s*.\n\n**Comportamiento emergente esperado**: los agentes aprenden a ser m√°s agresivos, priorizando adelantar en lugar de simplemente sobrevivir.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Variante D: Competitivo con ranking\n# from racing_game import train_competitive\n\n# train_competitive(n_cars=4, episodes=50)\n\nprint(\"Variante D: Competitivo con Ranking\")\nprint(\"  - 4 redes DQN independientes (igual que Var. A)\")\nprint(\"  - Recompensa base: avanzar + sobrevivir\")\nprint(\"  - Bonus adicional por posici√≥n relativa: [-0.5, +0.5]\")\n\nranking_code = \"\"\"\n# Calcular ranking por fitness (mayor fitness = mejor posici√≥n)\nalive_indices = [i for i, car in enumerate(game.cars) if car.alive]\nsorted_alive = sorted(alive_indices, key=lambda i: game.cars[i].fitness, reverse=True)\n\nfor rank, idx in enumerate(sorted_alive):\n    rank_bonus = 0.5 * (1 - 2 * rank / max(n_alive - 1, 1))\n    competitive_rewards[idx] += rank_bonus\n    # rank 0 (l√≠der) ‚Üí +0.5\n    # rank 3 (√∫ltimo) ‚Üí -0.5\n\"\"\"\nprint(\"\\nC√≥digo de recompensa competitiva:\")\nprint(ranking_code)\n\nprint(\"\\nComparaci√≥n de filosof√≠as:\")\nprint(\"  Variante A/B: 'Sobrevive y avanza' (cooperativo impl√≠cito)\")\nprint(\"  Variante D:   'S√© mejor que los dem√°s' (competitivo expl√≠cito)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Comparativa de Variantes\n\n| Aspecto | A: Independ. | B: Compartida | C: Maestro | D: Competitivo |\n|---------|-------------|---------------|------------|----------------|\n| Convergencia | Media | M√°s r√°pida | R√°pida inicial | Media |\n| Especializaci√≥n | S√≠ | No | Parcial | S√≠ |\n| Datos por ep. | 1√ó | 4√ó | 1√ó (fase 2) | 1√ó |\n| Complejidad impl. | Baja | Baja | Media | Media |\n| Comportamiento | Neutral | Homog√©neo | Variado | Agresivo |\n\n**¬øCu√°ndo usar cada una?**\n- **A** ‚Äî Baseline, cuando quieres comparar con otras variantes\n- **B** ‚Äî Cuando el tiempo de convergencia es prioritario\n- **C** ‚Äî Cuando tienes un agente pre-entrenado o quieres demostrar transfer learning\n- **D** ‚Äî Cuando quieres comportamiento emergente competitivo",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}